{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业3：深度学习框架实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业将练习深度学习框架的使用，大部分内容用 PyTorch 实现。第1题利用卷积层和全连接层实现手写数字的识别，第2题利用 RNN 来实现英文名的自动生成，第3题是算法题，利用卷积运算实现任意大整数的乘法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 目标：通过对 MNIST 数据进行训练，构建一个简单的图像分类模型，对图片中的数字进行识别。你将利用该模型对自己真实手写出的数字进行预测，观察模型效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 主要步骤：获取数据，创建模型结构，定义损失函数，编写训练循环，实施预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取数据。我们使用知名的 MNIST 数据集，它可以从 PyTorch 中利用工具函数下载得到。原始的 MNIST 数据训练集大小为60000，我们随机抽取其中的10000个观测进行简单的训练。以下函数会在当前目录建立一个名为 data 的文件夹，其中会包含下载得到的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：请在任何程序的最开始加上随机数种子的设置。请保持这一习惯。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27b7066ea30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "loader = DataLoader(mnist, batch_size=10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一次性取出随机抽取到的10000个观测，其中 x 是图片数据，y 是图片对应的数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个习惯性动作是查看数据的大小和维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以利用下面的函数展示图片的内容。如选择第一张图片，先将其转换成 Numpy 数组，再绘制图形："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANA0lEQVR4nO3db6xU9Z3H8c9HF57wJ4KuLFKUbqNm131gN4Rs0mbjpoG4PuH2QZeSuGETsrcxuKEJiXvjn9RHRnHbZmNi5TZiL1ogTVojD+puCWni9knjxaBiCdVFFm4h3FYSofEBCt99cA/NBWfODDPnzBnu9/1KbmbmfGfO7+vED+ecOTPn54gQgLnvhqYbADAYhB1IgrADSRB2IAnCDiTxZ4MczDYf/QM1iwi3Wt7Xlt32/baP2v7A9lg/6wJQL/d6nt32jZJ+K2mtpClJb0raGBG/KXkNW3agZnVs2ddI+iAijkXEBUl7Ja3vY30AatRP2FdIOjnr8VSx7Aq2R21P2p7sYywAfernA7pWuwqf202PiHFJ4xK78UCT+tmyT0laOevxFySd6q8dAHXpJ+xvSrrT9hdtz5f0TUn7qmkLQNV63o2PiM9sPyzpvyXdKGlnRLxXWWcAKtXzqbeeBuOYHahdLV+qAXD9IOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0fP87JJk+7ik85IuSvosIlZX0RSA6vUV9sI/RMQfKlgPgBqxGw8k0W/YQ9IvbB+0PdrqCbZHbU/anuxzLAB9cET0/mL7tog4ZftWSfsl/VtEvFHy/N4HA9CViHCr5X1t2SPiVHE7LelVSWv6WR+A+vQcdtsLbC+6fF/SOkmHq2oMQLX6+TR+maRXbV9ez+6I+K9KugJQub6O2a95MI7ZgdrVcswO4PpB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAElVccHIo7N69u7S+YcOG0vpLL71UWj948GDb2ssvv1z62pGRkdL6xMREab1ON9xQ/u/9pUuXBtTJcNmyZUtp/YUXXhhQJ9Vhyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADScyZq8tevHixtF7nf+fRo0dL67fddltpfdGiRVW2c02KS4G3Ncj/P4ZJp/PsO3bsGFAn146rywLJEXYgCcIOJEHYgSQIO5AEYQeSIOxAEnPm9+wff/xxab3T77YXLlzY89h33313ab3Tb8I79V6nYT7P3qm3xYsXD6iTuaHjlt32TtvTtg/PWrbU9n7b7xe3S+ptE0C/utmN/5Gk+69aNibpQETcKelA8RjAEOsY9oh4Q9LZqxavl3T5WkoTkkaqbQtA1Xo9Zl8WEaclKSJO27613RNtj0oa7XEcABWp/QO6iBiXNC7V+0MYAOV6PfV2xvZySSpup6trCUAdeg37PkmbivubJL1WTTsA6tJxN972Hkn3SbrF9pSk70h6WtJPbG+WdELSN+psshtLly4tra9ataq0vnnz5gq7udL58+dL69u3b69t7OvZ7bffXlo/duzYgDqZGzqGPSI2til9reJeANSIr8sCSRB2IAnCDiRB2IEkCDuQxJz5iWsnx48fL60/8cQTg2kEf7Js2bLSep2Xaz537lxp/cSJE7WN3RS27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJrz7Bg+69atK62vXbu2trHHxsqvkfr666/XNnZT2LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIe5JS8zAiD2T788MPS+sqVK/ta/6FDh9rWRkZGSl87NTXV19hNioiWc12zZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJPg9O/oyf/780vr4+Hjb2ooVK6pu5wq7du1qW7uez6P3quOW3fZO29O2D89a9qTt39k+VPw9UG+bAPrVzW78jyTd32L59yPi3uLv59W2BaBqHcMeEW9IOjuAXgDUqJ8P6B62/U6xm7+k3ZNsj9qetD3Zx1gA+tRr2H8g6UuS7pV0WtJ32z0xIsYjYnVErO5xLAAV6CnsEXEmIi5GxCVJP5S0ptq2AFStp7DbXj7r4dclHW73XADDoeN5dtt7JN0n6RbbU5K+I+k+2/dKCknHJX2rvhbRpAULFpTWn3rqqdL6gw8+2PPYFy5cKK1v3769tP7888/3PPZc1DHsEbGxxeIXa+gFQI34uiyQBGEHkiDsQBKEHUiCsANJcClplHruuedK6w899FBtYz/zzDOl9ccee6y2sa9nXEoaSI6wA0kQdiAJwg4kQdiBJAg7kARhB5LgUtLJ7d69u7S+YcOG2sYeGxsrrT/77LO1jZ0RW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSILz7HPc448/XlrvdB693+sdnDx5sm1tYmKir3Xj2rBlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOM8+B9x1111ta/1MmdyNjz76qLQ+MjLStjY9PV1xNyjTcctue6XtX9o+Yvs921uL5Utt77f9fnG7pP52AfSqm934zyRti4i/kvR3krbY/mtJY5IORMSdkg4UjwEMqY5hj4jTEfFWcf+8pCOSVkhaL+ny9x0nJI3U1COAClzTMbvtVZK+LOnXkpZFxGlp5h8E27e2ec2opNE++wTQp67DbnuhpJ9K+nZEnLNbzh33ORExLmm8WAcTOwIN6erUm+15mgn6jyPiZ8XiM7aXF/XlkvhoFRhiHbfsntmEvyjpSER8b1Zpn6RNkp4ubl+rpUPonnvuKa3v27evbe2OO+6oup0rbN26tbT+9ttv1zo+utfNbvxXJP2zpHdtHyqWPaqZkP/E9mZJJyR9o5YOAVSiY9gj4leS2h2gf63adgDUha/LAkkQdiAJwg4kQdiBJAg7kAQ/cb0OlJ1Hl+o9l75z587SeqfeMDzYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEpxnHwI33XRTaf3mm2+ubexXXnmltL5t27bS+ieffFJlO6gRW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSILz7EPgkUceKa0vXLiwtrE3bdpU27oxXNiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAS3czPvlLSLkl/IemSpPGI+E/bT0r6V0m/L576aET8vK5G57JOc5h/+umnpfV58+a1re3Zs6f0tatXry6tT05OltZx/ejmSzWfSdoWEW/ZXiTpoO39Re37EfEf9bUHoCrdzM9+WtLp4v5520ckrai7MQDVuqZjdturJH1Z0q+LRQ/bfsf2TttL2rxm1PakbfYHgQZ1HXbbCyX9VNK3I+KcpB9I+pKkezWz5f9uq9dFxHhErI6I8oNDALXqKuy252km6D+OiJ9JUkSciYiLEXFJ0g8lramvTQD96hh225b0oqQjEfG9WcuXz3ra1yUdrr49AFVxRJQ/wf6qpP+R9K5mTr1J0qOSNmpmFz4kHZf0reLDvLJ1lQ+Gls6ePVtaX7x4cc/r3rFjR2l9y5YtPa8bzYgIt1rezafxv5LU6sWcUweuI3yDDkiCsANJEHYgCcIOJEHYgSQIO5AEl5K+Duzdu7e0Pjo62vO6O/0EFnMHW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLj79krHcz+vaT/m7XoFkl/GFgD12ZYexvWviR661WVvd0REX/eqjDQsH9ucHtyWK9NN6y9DWtfEr31alC9sRsPJEHYgSSaDvt4w+OXGdbehrUvid56NZDeGj1mBzA4TW/ZAQwIYQeSaCTstu+3fdT2B7bHmuihHdvHbb9r+1DT89MVc+hN2z48a9lS2/ttv1/ctpxjr6HenrT9u+K9O2T7gYZ6W2n7l7aP2H7P9tZieaPvXUlfA3nfBn7MbvtGSb+VtFbSlKQ3JW2MiN8MtJE2bB+XtDoiGv8Chu2/l/RHSbsi4m+KZdslnY2Ip4t/KJdExL8PSW9PSvpj09N4F7MVLZ89zbikEUn/ogbfu5K+/kkDeN+a2LKvkfRBRByLiAuS9kpa30AfQy8i3pB09XQw6yVNFPcnNPM/y8C16W0oRMTpiHiruH9e0uVpxht970r6Gogmwr5C0slZj6c0XPO9h6Rf2D5ou/frPdVn2eVptorbWxvu52odp/EepKumGR+a966X6c/71UTYW00lNUzn/74SEX8r6R8lbSl2V9GdrqbxHpQW04wPhV6nP+9XE2GfkrRy1uMvSDrVQB8tRcSp4nZa0qsavqmoz1yeQbe4nW64nz8Zpmm8W00zriF475qc/ryJsL8p6U7bX7Q9X9I3Je1roI/Psb2g+OBEthdIWqfhm4p6n6RNxf1Nkl5rsJcrDMs03u2mGVfD713j059HxMD/JD2gmU/k/1fSY0300Kavv5T0dvH3XtO9Sdqjmd26TzWzR7RZ0s2SDkh6v7hdOkS9vayZqb3f0UywljfU21c1c2j4jqRDxd8DTb93JX0N5H3j67JAEnyDDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+H8eN/u8ByIKeQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = x[0].squeeze().cpu().numpy()\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来请你选择5个你喜欢的数字（10000以下），然后取出对应位置的图片，并画出它们的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for num in [111,222,333,444,555]:\n",
    "    img = x[num].squeeze().cpu().numpy()\n",
    "    print(img.shape)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 搭建模型。我们搭建一个类似于 LeNet-5 的网络，结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-82eabb4c17e90d467197d013f7629f3c_720w.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要创建2个卷积层、2个汇聚层（池化层）和2个全连接层，**暂时忽略所有的激活函数**。所有隐藏层的函数细节都可以在[官方文档](https://pytorch.org/docs/stable/nn.html)中按分类找到。每一个隐藏层本质上都是将一个数组变换成另一个数组的函数，因此为了确认编写的模型是正确的，可以先用一个小数据进行测试，观察输入和输出的维度。例如，我们先取出前10个观测，此时输入的维度是 `[10, 1, 28, 28]`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "smallx = x[0:10]\n",
    "smally = y[0:10]\n",
    "print(smallx.shape)\n",
    "print(smally.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建第1个卷积层，并测试输出的维度。注意到我们可以直接将隐藏层当成一个函数来调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "conv1 = torch.nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "res = conv1(smallx)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，输出的维度为 `[20, 24, 24]`（不包括第1位的数据批次维度），与之前图中的结果吻合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，请按照图中提示编写层对象 `pool1`、`conv2`、`pool2`、`fc1` 和 `fc2`，并顺次测试输入与输出的维度，使其与上图匹配。注意，在将一个大小为 `[10, 50, 4, 4]` 的数组（假设叫 `somearray`）传递给 `fc1` 之前，需要先将其变形为只有两个维度的数组，做法是 `somearray.view(-1, 50*4*4)`，其中 -1 表示该位置的大小不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 12, 12])\n",
      "torch.Size([10, 50, 8, 8])\n",
      "torch.Size([10, 50, 4, 4])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "res = pool1(res)\n",
    "print(res.shape)\n",
    "\n",
    "conv2 = torch.nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "res = conv2(res)\n",
    "print(res.shape)\n",
    "\n",
    "pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "res = pool2(res)\n",
    "print(res.shape)\n",
    "\n",
    "fc1 = torch.nn.Linear(50 * 4 * 4, 500)\n",
    "res = fc1(res.view(-1, 50 * 4 * 4))\n",
    "print(res.shape)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "res = relu(res)\n",
    "print(res.shape)\n",
    "\n",
    "fc2 = torch.nn.Linear(500, 10)\n",
    "res = fc2(res)\n",
    "print(res.shape)\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "res = softmax(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建模型类。在确保隐藏层维度都正确后，将所有的隐藏层封装到一个模型类中，其中模型结构在 `__init__()` 中定义，具体的计算过程在 `forward()` 中实现。此时需要加入激活函数。在本模型中，**请在 `conv1`、`conv2` 和 `fc1` 后加入 ReLU 激活函数，并在 `fc2` 后加入 Softmax 激活函数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(50 * 4 * 4, 500)\n",
    "        self.fc2 = torch.nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.fc1(x.view(-1, 50 * 4 * 4))\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次测试输入输出的维度是否正确。如果模型编写正确，输出的维度应该是 `[10, 10]`，且输出结果为0到1之间的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "tensor([[0.0992, 0.0989, 0.0982, 0.1089, 0.0943, 0.0912, 0.0996, 0.1020, 0.1021,\n",
      "         0.1056],\n",
      "        [0.0997, 0.1004, 0.0978, 0.1085, 0.0938, 0.0907, 0.0982, 0.1045, 0.1020,\n",
      "         0.1045],\n",
      "        [0.0990, 0.0993, 0.0979, 0.1081, 0.0913, 0.0921, 0.0976, 0.1088, 0.1030,\n",
      "         0.1031],\n",
      "        [0.0955, 0.0996, 0.1010, 0.1081, 0.0931, 0.0913, 0.0988, 0.1044, 0.1031,\n",
      "         0.1051],\n",
      "        [0.0995, 0.0988, 0.0985, 0.1091, 0.0919, 0.0918, 0.0980, 0.1059, 0.1017,\n",
      "         0.1049],\n",
      "        [0.0977, 0.0994, 0.0996, 0.1073, 0.0924, 0.0922, 0.1018, 0.1019, 0.1040,\n",
      "         0.1038],\n",
      "        [0.0972, 0.1011, 0.1010, 0.1062, 0.0931, 0.0908, 0.1006, 0.1048, 0.1037,\n",
      "         0.1016],\n",
      "        [0.0996, 0.0984, 0.0987, 0.1068, 0.0904, 0.0900, 0.0992, 0.1076, 0.1020,\n",
      "         0.1075],\n",
      "        [0.0972, 0.0998, 0.1012, 0.1084, 0.0925, 0.0905, 0.1022, 0.1014, 0.1023,\n",
      "         0.1045],\n",
      "        [0.0979, 0.0998, 0.0985, 0.1080, 0.0959, 0.0910, 0.0979, 0.1049, 0.1027,\n",
      "         0.1033]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([7, 4, 5, 4, 0, 7, 1, 0, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = MyModel()\n",
    "pred = model(smallx)\n",
    "print(pred.shape)\n",
    "print(pred)\n",
    "print(smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred` 的每一行加总为1，其中每一个元素代表对应类别的预测概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以直接打印模型对象，观察隐藏层的结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 损失函数。对于分类问题，损失函数通常选取为负对数似然函数。在 PyTorch 中，可以使用 `torch.nn.NLLLoss` 来完成计算。其用法是先定义一个损失函数对象，然后在预测值和真实标签上调用该函数对象。注意：损失函数对象的第一个参数是预测概率的**对数值**，第二个参数是真实的标签。[文档说明](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3231, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn = torch.nn.NLLLoss()\n",
    "lossfn(torch.log(pred), smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 利用课上介绍的循环模板和代码示例，对模型进行迭代训练。对于本数据，选取 mini-batch 大小为200，共遍历数据10遍，优化器选为 Adam，学习率为0.001。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 2, loss = 2.3050460815429688\n",
      "epoch 1, batch 3, loss = 2.2473204135894775\n",
      "epoch 1, batch 4, loss = 2.2203662395477295\n",
      "epoch 1, batch 5, loss = 2.166632890701294\n",
      "epoch 1, batch 6, loss = 2.0988450050354004\n",
      "epoch 1, batch 7, loss = 2.04423451423645\n",
      "epoch 1, batch 8, loss = 1.8825541734695435\n",
      "epoch 1, batch 9, loss = 1.7789987325668335\n",
      "epoch 1, batch 10, loss = 1.7005940675735474\n",
      "epoch 1, batch 11, loss = 1.4537014961242676\n",
      "epoch 1, batch 12, loss = 1.315425992012024\n",
      "epoch 1, batch 13, loss = 1.2458666563034058\n",
      "epoch 1, batch 14, loss = 1.1077849864959717\n",
      "epoch 1, batch 15, loss = 1.007583498954773\n",
      "epoch 1, batch 16, loss = 0.8979427218437195\n",
      "epoch 1, batch 17, loss = 0.7679138779640198\n",
      "epoch 1, batch 18, loss = 0.7869899272918701\n",
      "epoch 1, batch 19, loss = 0.7071364521980286\n",
      "epoch 1, batch 20, loss = 0.6270896792411804\n",
      "epoch 1, batch 21, loss = 0.6990595459938049\n",
      "epoch 1, batch 22, loss = 0.6995390057563782\n",
      "epoch 1, batch 23, loss = 0.6035932898521423\n",
      "epoch 1, batch 24, loss = 0.5231016874313354\n",
      "epoch 1, batch 25, loss = 0.5987288951873779\n",
      "epoch 1, batch 26, loss = 0.6824304461479187\n",
      "epoch 1, batch 27, loss = 0.43086349964141846\n",
      "epoch 1, batch 28, loss = 0.53407883644104\n",
      "epoch 1, batch 29, loss = 0.43908870220184326\n",
      "epoch 1, batch 30, loss = 0.5396312475204468\n",
      "epoch 1, batch 31, loss = 0.5318405032157898\n",
      "epoch 1, batch 32, loss = 0.3657747209072113\n",
      "epoch 1, batch 33, loss = 0.3161954879760742\n",
      "epoch 1, batch 34, loss = 0.5519315600395203\n",
      "epoch 1, batch 35, loss = 0.3699825406074524\n",
      "epoch 1, batch 36, loss = 0.4243515133857727\n",
      "epoch 1, batch 37, loss = 0.5417483448982239\n",
      "epoch 1, batch 38, loss = 0.3971700966358185\n",
      "epoch 1, batch 39, loss = 0.2896517217159271\n",
      "epoch 1, batch 40, loss = 0.3803277611732483\n",
      "epoch 1, batch 41, loss = 0.39683929085731506\n",
      "epoch 1, batch 42, loss = 0.35654982924461365\n",
      "epoch 1, batch 43, loss = 0.4173039197921753\n",
      "epoch 1, batch 44, loss = 0.2804347276687622\n",
      "epoch 1, batch 45, loss = 0.372831791639328\n",
      "epoch 1, batch 46, loss = 0.3350135087966919\n",
      "epoch 1, batch 47, loss = 0.3469702899456024\n",
      "epoch 1, batch 48, loss = 0.3237672448158264\n",
      "epoch 1, batch 49, loss = 0.3715530037879944\n",
      "epoch 1, batch 50, loss = 0.31588757038116455\n",
      "epoch 1, batch 51, loss = 0.3671167492866516\n",
      "epoch 1, batch 52, loss = 0.3051544427871704\n",
      "epoch 1, batch 53, loss = 0.27889981865882874\n",
      "epoch 1, batch 54, loss = 0.38411468267440796\n",
      "epoch 1, batch 55, loss = 0.27171245217323303\n",
      "epoch 1, batch 56, loss = 0.39594581723213196\n",
      "epoch 1, batch 57, loss = 0.2891245186328888\n",
      "epoch 1, batch 58, loss = 0.3611195385456085\n",
      "epoch 1, batch 59, loss = 0.2491045892238617\n",
      "epoch 1, batch 60, loss = 0.253251850605011\n",
      "epoch 1, batch 61, loss = 0.22162935137748718\n",
      "epoch 1, batch 62, loss = 0.27378445863723755\n",
      "epoch 1, batch 63, loss = 0.2770627737045288\n",
      "epoch 1, batch 64, loss = 0.35928231477737427\n",
      "epoch 1, batch 65, loss = 0.27349844574928284\n",
      "epoch 1, batch 66, loss = 0.2936438024044037\n",
      "epoch 1, batch 67, loss = 0.23627881705760956\n",
      "epoch 1, batch 68, loss = 0.19077032804489136\n",
      "epoch 1, batch 69, loss = 0.23309701681137085\n",
      "epoch 1, batch 70, loss = 0.15675635635852814\n",
      "epoch 1, batch 71, loss = 0.22766748070716858\n",
      "epoch 1, batch 72, loss = 0.2775649428367615\n",
      "epoch 1, batch 73, loss = 0.2904166281223297\n",
      "epoch 1, batch 74, loss = 0.22536998987197876\n",
      "epoch 1, batch 75, loss = 0.31358256936073303\n",
      "epoch 1, batch 76, loss = 0.13220421969890594\n",
      "epoch 1, batch 77, loss = 0.3564088046550751\n",
      "epoch 1, batch 78, loss = 0.19275783002376556\n",
      "epoch 1, batch 79, loss = 0.21210262179374695\n",
      "epoch 1, batch 80, loss = 0.21468715369701385\n",
      "epoch 1, batch 81, loss = 0.2920866310596466\n",
      "epoch 1, batch 82, loss = 0.19358789920806885\n",
      "epoch 1, batch 83, loss = 0.270902544260025\n",
      "epoch 1, batch 84, loss = 0.3004324734210968\n",
      "epoch 1, batch 85, loss = 0.2756841778755188\n",
      "epoch 1, batch 86, loss = 0.2703440189361572\n",
      "epoch 1, batch 87, loss = 0.2262594848871231\n",
      "epoch 1, batch 88, loss = 0.2474280148744583\n",
      "epoch 1, batch 89, loss = 0.1955384463071823\n",
      "epoch 1, batch 90, loss = 0.23462457954883575\n",
      "epoch 1, batch 91, loss = 0.25876176357269287\n",
      "epoch 1, batch 92, loss = 0.2595047652721405\n",
      "epoch 1, batch 93, loss = 0.22688089311122894\n",
      "epoch 1, batch 94, loss = 0.28084874153137207\n",
      "epoch 1, batch 95, loss = 0.20849458873271942\n",
      "epoch 1, batch 96, loss = 0.18817263841629028\n",
      "epoch 1, batch 97, loss = 0.20829927921295166\n",
      "epoch 1, batch 98, loss = 0.16816048324108124\n",
      "epoch 1, batch 99, loss = 0.18601354956626892\n",
      "epoch 1, batch 100, loss = 0.20876047015190125\n",
      "epoch 1, batch 101, loss = 0.21610605716705322\n",
      "epoch 1, batch 102, loss = 0.1806045025587082\n",
      "epoch 1, batch 103, loss = 0.15845118463039398\n",
      "epoch 1, batch 104, loss = 0.10730987787246704\n",
      "epoch 1, batch 105, loss = 0.1287989467382431\n",
      "epoch 1, batch 106, loss = 0.1929778903722763\n",
      "epoch 1, batch 107, loss = 0.140678271651268\n",
      "epoch 1, batch 108, loss = 0.15787096321582794\n",
      "epoch 1, batch 109, loss = 0.14505040645599365\n",
      "epoch 1, batch 110, loss = 0.14991901814937592\n",
      "epoch 1, batch 111, loss = 0.18387503921985626\n",
      "epoch 1, batch 112, loss = 0.16055244207382202\n",
      "epoch 1, batch 113, loss = 0.16701193153858185\n",
      "epoch 1, batch 114, loss = 0.16617296636104584\n",
      "epoch 1, batch 115, loss = 0.21428680419921875\n",
      "epoch 1, batch 116, loss = 0.13656659424304962\n",
      "epoch 1, batch 117, loss = 0.1851854771375656\n",
      "epoch 1, batch 118, loss = 0.16089875996112823\n",
      "epoch 1, batch 119, loss = 0.20176275074481964\n",
      "epoch 1, batch 120, loss = 0.15906326472759247\n",
      "epoch 1, batch 121, loss = 0.2104947865009308\n",
      "epoch 1, batch 122, loss = 0.15175136923789978\n",
      "epoch 1, batch 123, loss = 0.25036707520484924\n",
      "epoch 1, batch 124, loss = 0.1618194431066513\n",
      "epoch 1, batch 125, loss = 0.11004996299743652\n",
      "epoch 1, batch 126, loss = 0.14295120537281036\n",
      "epoch 1, batch 127, loss = 0.1324039101600647\n",
      "epoch 1, batch 128, loss = 0.21932828426361084\n",
      "epoch 1, batch 129, loss = 0.08135053515434265\n",
      "epoch 1, batch 130, loss = 0.14343591034412384\n",
      "epoch 1, batch 131, loss = 0.12773853540420532\n",
      "epoch 1, batch 132, loss = 0.12398474663496017\n",
      "epoch 1, batch 133, loss = 0.13559165596961975\n",
      "epoch 1, batch 134, loss = 0.11832763999700546\n",
      "epoch 1, batch 135, loss = 0.12977394461631775\n",
      "epoch 1, batch 136, loss = 0.16115084290504456\n",
      "epoch 1, batch 137, loss = 0.17799177765846252\n",
      "epoch 1, batch 138, loss = 0.17357982695102692\n",
      "epoch 1, batch 139, loss = 0.14082852005958557\n",
      "epoch 1, batch 140, loss = 0.13068623840808868\n",
      "epoch 1, batch 141, loss = 0.12406038492918015\n",
      "epoch 1, batch 142, loss = 0.14328715205192566\n",
      "epoch 1, batch 143, loss = 0.09151365607976913\n",
      "epoch 1, batch 144, loss = 0.11039184778928757\n",
      "epoch 1, batch 145, loss = 0.13205905258655548\n",
      "epoch 1, batch 146, loss = 0.0799936056137085\n",
      "epoch 1, batch 147, loss = 0.15206573903560638\n",
      "epoch 1, batch 148, loss = 0.14033269882202148\n",
      "epoch 1, batch 149, loss = 0.0742005705833435\n",
      "epoch 1, batch 150, loss = 0.21841664612293243\n",
      "epoch 1, batch 151, loss = 0.13047321140766144\n",
      "epoch 1, batch 152, loss = 0.1203637421131134\n",
      "epoch 1, batch 153, loss = 0.09787815809249878\n",
      "epoch 1, batch 154, loss = 0.06796994805335999\n",
      "epoch 1, batch 155, loss = 0.08862105011940002\n",
      "epoch 1, batch 156, loss = 0.20304811000823975\n",
      "epoch 1, batch 157, loss = 0.1992020606994629\n",
      "epoch 1, batch 158, loss = 0.12583978474140167\n",
      "epoch 1, batch 159, loss = 0.11546444147825241\n",
      "epoch 1, batch 160, loss = 0.10274174064397812\n",
      "epoch 1, batch 161, loss = 0.12925878167152405\n",
      "epoch 1, batch 162, loss = 0.1629076451063156\n",
      "epoch 1, batch 163, loss = 0.0842432975769043\n",
      "epoch 1, batch 164, loss = 0.10200861841440201\n",
      "epoch 1, batch 165, loss = 0.08520665019750595\n",
      "epoch 1, batch 166, loss = 0.13676324486732483\n",
      "epoch 1, batch 167, loss = 0.0688517615199089\n",
      "epoch 1, batch 168, loss = 0.10036750137805939\n",
      "epoch 1, batch 169, loss = 0.07735472917556763\n",
      "epoch 1, batch 170, loss = 0.039086781442165375\n",
      "epoch 1, batch 171, loss = 0.14502406120300293\n",
      "epoch 1, batch 172, loss = 0.10025813430547714\n",
      "epoch 1, batch 173, loss = 0.07039845734834671\n",
      "epoch 1, batch 174, loss = 0.07021574676036835\n",
      "epoch 1, batch 175, loss = 0.11171415448188782\n",
      "epoch 1, batch 176, loss = 0.07737719267606735\n",
      "epoch 1, batch 177, loss = 0.13371114432811737\n",
      "epoch 1, batch 178, loss = 0.12962503731250763\n",
      "epoch 1, batch 179, loss = 0.16959331929683685\n",
      "epoch 1, batch 180, loss = 0.07587788254022598\n",
      "epoch 1, batch 181, loss = 0.14981739223003387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 182, loss = 0.14431951940059662\n",
      "epoch 1, batch 183, loss = 0.07141241431236267\n",
      "epoch 1, batch 184, loss = 0.16411590576171875\n",
      "epoch 1, batch 185, loss = 0.0713929682970047\n",
      "epoch 1, batch 186, loss = 0.11358346790075302\n",
      "epoch 1, batch 187, loss = 0.17957812547683716\n",
      "epoch 1, batch 188, loss = 0.11624517291784286\n",
      "epoch 1, batch 189, loss = 0.0713072195649147\n",
      "epoch 1, batch 190, loss = 0.1054958701133728\n",
      "epoch 1, batch 191, loss = 0.07586383819580078\n",
      "epoch 1, batch 192, loss = 0.07723506540060043\n",
      "epoch 1, batch 193, loss = 0.08352962136268616\n",
      "epoch 1, batch 194, loss = 0.04781626537442207\n",
      "epoch 1, batch 195, loss = 0.09956946223974228\n",
      "epoch 1, batch 196, loss = 0.13867880403995514\n",
      "epoch 1, batch 197, loss = 0.1403398960828781\n",
      "epoch 1, batch 198, loss = 0.11380694061517715\n",
      "epoch 1, batch 199, loss = 0.1302448809146881\n",
      "epoch 1, batch 200, loss = 0.07064805924892426\n",
      "epoch 2, batch 2, loss = 0.08442145586013794\n",
      "epoch 2, batch 3, loss = 0.0304433424025774\n",
      "epoch 2, batch 4, loss = 0.07031092792749405\n",
      "epoch 2, batch 5, loss = 0.09269670397043228\n",
      "epoch 2, batch 6, loss = 0.10681210458278656\n",
      "epoch 2, batch 7, loss = 0.03011816181242466\n",
      "epoch 2, batch 8, loss = 0.05281573906540871\n",
      "epoch 2, batch 9, loss = 0.03479330986738205\n",
      "epoch 2, batch 10, loss = 0.01955495774745941\n",
      "epoch 2, batch 11, loss = 0.09096731245517731\n",
      "epoch 2, batch 12, loss = 0.06164364889264107\n",
      "epoch 2, batch 13, loss = 0.08129812031984329\n",
      "epoch 2, batch 14, loss = 0.02773979678750038\n",
      "epoch 2, batch 15, loss = 0.03074667416512966\n",
      "epoch 2, batch 16, loss = 0.05552549660205841\n",
      "epoch 2, batch 17, loss = 0.05217880383133888\n",
      "epoch 2, batch 18, loss = 0.09678391367197037\n",
      "epoch 2, batch 19, loss = 0.14347395300865173\n",
      "epoch 2, batch 20, loss = 0.13922974467277527\n",
      "epoch 2, batch 21, loss = 0.03638762608170509\n",
      "epoch 2, batch 22, loss = 0.0655069500207901\n",
      "epoch 2, batch 23, loss = 0.10457254201173782\n",
      "epoch 2, batch 24, loss = 0.05099916830658913\n",
      "epoch 2, batch 25, loss = 0.06337343901395798\n",
      "epoch 2, batch 26, loss = 0.07961338758468628\n",
      "epoch 2, batch 27, loss = 0.021885380148887634\n",
      "epoch 2, batch 28, loss = 0.05138538405299187\n",
      "epoch 2, batch 29, loss = 0.06594348698854446\n",
      "epoch 2, batch 30, loss = 0.03809976577758789\n",
      "epoch 2, batch 31, loss = 0.1048329547047615\n",
      "epoch 2, batch 32, loss = 0.048217274248600006\n",
      "epoch 2, batch 33, loss = 0.07230444997549057\n",
      "epoch 2, batch 34, loss = 0.051436565816402435\n",
      "epoch 2, batch 35, loss = 0.12052562832832336\n",
      "epoch 2, batch 36, loss = 0.14719337224960327\n",
      "epoch 2, batch 37, loss = 0.05802327021956444\n",
      "epoch 2, batch 38, loss = 0.08596338331699371\n",
      "epoch 2, batch 39, loss = 0.05498046800494194\n",
      "epoch 2, batch 40, loss = 0.06561803817749023\n",
      "epoch 2, batch 41, loss = 0.043972715735435486\n",
      "epoch 2, batch 42, loss = 0.09464352577924728\n",
      "epoch 2, batch 43, loss = 0.021591555327177048\n",
      "epoch 2, batch 44, loss = 0.043795667588710785\n",
      "epoch 2, batch 45, loss = 0.1115039512515068\n",
      "epoch 2, batch 46, loss = 0.06319103389978409\n",
      "epoch 2, batch 47, loss = 0.03393819183111191\n",
      "epoch 2, batch 48, loss = 0.06119934469461441\n",
      "epoch 2, batch 49, loss = 0.036487407982349396\n",
      "epoch 2, batch 50, loss = 0.03375846520066261\n",
      "epoch 2, batch 51, loss = 0.06763304024934769\n",
      "epoch 2, batch 52, loss = 0.06528826057910919\n",
      "epoch 2, batch 53, loss = 0.07528028637170792\n",
      "epoch 2, batch 54, loss = 0.04854607954621315\n",
      "epoch 2, batch 55, loss = 0.05545615777373314\n",
      "epoch 2, batch 56, loss = 0.053443554788827896\n",
      "epoch 2, batch 57, loss = 0.06863509118556976\n",
      "epoch 2, batch 58, loss = 0.13550415635108948\n",
      "epoch 2, batch 59, loss = 0.08614988625049591\n",
      "epoch 2, batch 60, loss = 0.06474877148866653\n",
      "epoch 2, batch 61, loss = 0.08089206367731094\n",
      "epoch 2, batch 62, loss = 0.08414305001497269\n",
      "epoch 2, batch 63, loss = 0.0395343191921711\n",
      "epoch 2, batch 64, loss = 0.08554956316947937\n",
      "epoch 2, batch 65, loss = 0.07314234972000122\n",
      "epoch 2, batch 66, loss = 0.07777445018291473\n",
      "epoch 2, batch 67, loss = 0.10101146996021271\n",
      "epoch 2, batch 68, loss = 0.03089091368019581\n",
      "epoch 2, batch 69, loss = 0.08358651399612427\n",
      "epoch 2, batch 70, loss = 0.1491614580154419\n",
      "epoch 2, batch 71, loss = 0.09699807316064835\n",
      "epoch 2, batch 72, loss = 0.10836920142173767\n",
      "epoch 2, batch 73, loss = 0.06467275321483612\n",
      "epoch 2, batch 74, loss = 0.08567960560321808\n",
      "epoch 2, batch 75, loss = 0.0806349366903305\n",
      "epoch 2, batch 76, loss = 0.06769981235265732\n",
      "epoch 2, batch 77, loss = 0.040754690766334534\n",
      "epoch 2, batch 78, loss = 0.06487949192523956\n",
      "epoch 2, batch 79, loss = 0.04614187777042389\n",
      "epoch 2, batch 80, loss = 0.07067307084798813\n",
      "epoch 2, batch 81, loss = 0.02792578749358654\n",
      "epoch 2, batch 82, loss = 0.06354887783527374\n",
      "epoch 2, batch 83, loss = 0.03489118441939354\n",
      "epoch 2, batch 84, loss = 0.03896785154938698\n",
      "epoch 2, batch 85, loss = 0.054371871054172516\n",
      "epoch 2, batch 86, loss = 0.07192075252532959\n",
      "epoch 2, batch 87, loss = 0.08309555053710938\n",
      "epoch 2, batch 88, loss = 0.11882459372282028\n",
      "epoch 2, batch 89, loss = 0.06386791169643402\n",
      "epoch 2, batch 90, loss = 0.06629306077957153\n",
      "epoch 2, batch 91, loss = 0.06608067452907562\n",
      "epoch 2, batch 92, loss = 0.06809894740581512\n",
      "epoch 2, batch 93, loss = 0.040675267577171326\n",
      "epoch 2, batch 94, loss = 0.06355459988117218\n",
      "epoch 2, batch 95, loss = 0.09160685539245605\n",
      "epoch 2, batch 96, loss = 0.030422378331422806\n",
      "epoch 2, batch 97, loss = 0.06754600256681442\n",
      "epoch 2, batch 98, loss = 0.09853198379278183\n",
      "epoch 2, batch 99, loss = 0.027924953028559685\n",
      "epoch 2, batch 100, loss = 0.034528326243162155\n",
      "epoch 2, batch 101, loss = 0.04285488277673721\n",
      "epoch 2, batch 102, loss = 0.0666225403547287\n",
      "epoch 2, batch 103, loss = 0.07269161939620972\n",
      "epoch 2, batch 104, loss = 0.07297888398170471\n",
      "epoch 2, batch 105, loss = 0.02647024393081665\n",
      "epoch 2, batch 106, loss = 0.08988118171691895\n",
      "epoch 2, batch 107, loss = 0.0928611159324646\n",
      "epoch 2, batch 108, loss = 0.03856125846505165\n",
      "epoch 2, batch 109, loss = 0.07265881448984146\n",
      "epoch 2, batch 110, loss = 0.09769006818532944\n",
      "epoch 2, batch 111, loss = 0.05822638049721718\n",
      "epoch 2, batch 112, loss = 0.08034763485193253\n",
      "epoch 2, batch 113, loss = 0.05088092014193535\n",
      "epoch 2, batch 114, loss = 0.053606558591127396\n",
      "epoch 2, batch 115, loss = 0.02337637171149254\n",
      "epoch 2, batch 116, loss = 0.04954088106751442\n",
      "epoch 2, batch 117, loss = 0.05861958488821983\n",
      "epoch 2, batch 118, loss = 0.04243272915482521\n",
      "epoch 2, batch 119, loss = 0.05004441738128662\n",
      "epoch 2, batch 120, loss = 0.052022404968738556\n",
      "epoch 2, batch 121, loss = 0.08839676529169083\n",
      "epoch 2, batch 122, loss = 0.030236871913075447\n",
      "epoch 2, batch 123, loss = 0.041338082402944565\n",
      "epoch 2, batch 124, loss = 0.07689957320690155\n",
      "epoch 2, batch 125, loss = 0.06456288695335388\n",
      "epoch 2, batch 126, loss = 0.03671717643737793\n",
      "epoch 2, batch 127, loss = 0.042422469705343246\n",
      "epoch 2, batch 128, loss = 0.04019832983613014\n",
      "epoch 2, batch 129, loss = 0.0482449010014534\n",
      "epoch 2, batch 130, loss = 0.015811359509825706\n",
      "epoch 2, batch 131, loss = 0.11255382746458054\n",
      "epoch 2, batch 132, loss = 0.05295298993587494\n",
      "epoch 2, batch 133, loss = 0.021718237549066544\n",
      "epoch 2, batch 134, loss = 0.04071202874183655\n",
      "epoch 2, batch 135, loss = 0.06111707538366318\n",
      "epoch 2, batch 136, loss = 0.04333527386188507\n",
      "epoch 2, batch 137, loss = 0.03612920269370079\n",
      "epoch 2, batch 138, loss = 0.03676969185471535\n",
      "epoch 2, batch 139, loss = 0.03924394026398659\n",
      "epoch 2, batch 140, loss = 0.08055093884468079\n",
      "epoch 2, batch 141, loss = 0.06448183208703995\n",
      "epoch 2, batch 142, loss = 0.06749237328767776\n",
      "epoch 2, batch 143, loss = 0.060378316789865494\n",
      "epoch 2, batch 144, loss = 0.09739988297224045\n",
      "epoch 2, batch 145, loss = 0.08609762042760849\n",
      "epoch 2, batch 146, loss = 0.09124477207660675\n",
      "epoch 2, batch 147, loss = 0.05624796822667122\n",
      "epoch 2, batch 148, loss = 0.024422120302915573\n",
      "epoch 2, batch 149, loss = 0.019402429461479187\n",
      "epoch 2, batch 150, loss = 0.08281078189611435\n",
      "epoch 2, batch 151, loss = 0.05125660076737404\n",
      "epoch 2, batch 152, loss = 0.06989046931266785\n",
      "epoch 2, batch 153, loss = 0.04709036648273468\n",
      "epoch 2, batch 154, loss = 0.05094366893172264\n",
      "epoch 2, batch 155, loss = 0.05994299799203873\n",
      "epoch 2, batch 156, loss = 0.09768529236316681\n",
      "epoch 2, batch 157, loss = 0.059357862919569016\n",
      "epoch 2, batch 158, loss = 0.02916710078716278\n",
      "epoch 2, batch 159, loss = 0.05301009118556976\n",
      "epoch 2, batch 160, loss = 0.06807806342840195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 161, loss = 0.04972610995173454\n",
      "epoch 2, batch 162, loss = 0.07994469255208969\n",
      "epoch 2, batch 163, loss = 0.10722971707582474\n",
      "epoch 2, batch 164, loss = 0.0961739644408226\n",
      "epoch 2, batch 165, loss = 0.037535469979047775\n",
      "epoch 2, batch 166, loss = 0.07717525213956833\n",
      "epoch 2, batch 167, loss = 0.02252839133143425\n",
      "epoch 2, batch 168, loss = 0.03943777084350586\n",
      "epoch 2, batch 169, loss = 0.07181132584810257\n",
      "epoch 2, batch 170, loss = 0.04885997623205185\n",
      "epoch 2, batch 171, loss = 0.044203244149684906\n",
      "epoch 2, batch 172, loss = 0.03710553050041199\n",
      "epoch 2, batch 173, loss = 0.16433531045913696\n",
      "epoch 2, batch 174, loss = 0.042727041989564896\n",
      "epoch 2, batch 175, loss = 0.08356107771396637\n",
      "epoch 2, batch 176, loss = 0.05505504459142685\n",
      "epoch 2, batch 177, loss = 0.05331771820783615\n",
      "epoch 2, batch 178, loss = 0.029325848445296288\n",
      "epoch 2, batch 179, loss = 0.039843589067459106\n",
      "epoch 2, batch 180, loss = 0.05274667590856552\n",
      "epoch 2, batch 181, loss = 0.059539083391427994\n",
      "epoch 2, batch 182, loss = 0.04813922569155693\n",
      "epoch 2, batch 183, loss = 0.03813047707080841\n",
      "epoch 2, batch 184, loss = 0.06476103514432907\n",
      "epoch 2, batch 185, loss = 0.03836629539728165\n",
      "epoch 2, batch 186, loss = 0.03300293907523155\n",
      "epoch 2, batch 187, loss = 0.05781073495745659\n",
      "epoch 2, batch 188, loss = 0.019975710660219193\n",
      "epoch 2, batch 189, loss = 0.05140094459056854\n",
      "epoch 2, batch 190, loss = 0.028132205829024315\n",
      "epoch 2, batch 191, loss = 0.07347026467323303\n",
      "epoch 2, batch 192, loss = 0.0496707446873188\n",
      "epoch 2, batch 193, loss = 0.020254872739315033\n",
      "epoch 2, batch 194, loss = 0.05062035471200943\n",
      "epoch 2, batch 195, loss = 0.044351302087306976\n",
      "epoch 2, batch 196, loss = 0.07714122533798218\n",
      "epoch 2, batch 197, loss = 0.09285721927881241\n",
      "epoch 2, batch 198, loss = 0.07932295650243759\n",
      "epoch 2, batch 199, loss = 0.07585453242063522\n",
      "epoch 2, batch 200, loss = 0.07450059056282043\n",
      "epoch 3, batch 2, loss = 0.017958449199795723\n",
      "epoch 3, batch 3, loss = 0.07012302428483963\n",
      "epoch 3, batch 4, loss = 0.049008697271347046\n",
      "epoch 3, batch 5, loss = 0.06875946372747421\n",
      "epoch 3, batch 6, loss = 0.06597987562417984\n",
      "epoch 3, batch 7, loss = 0.05389033257961273\n",
      "epoch 3, batch 8, loss = 0.030472472310066223\n",
      "epoch 3, batch 9, loss = 0.03417770564556122\n",
      "epoch 3, batch 10, loss = 0.08775490522384644\n",
      "epoch 3, batch 11, loss = 0.03303540125489235\n",
      "epoch 3, batch 12, loss = 0.011821060441434383\n",
      "epoch 3, batch 13, loss = 0.030565040186047554\n",
      "epoch 3, batch 14, loss = 0.04536834731698036\n",
      "epoch 3, batch 15, loss = 0.025395017117261887\n",
      "epoch 3, batch 16, loss = 0.052932534366846085\n",
      "epoch 3, batch 17, loss = 0.08363509178161621\n",
      "epoch 3, batch 18, loss = 0.02680326998233795\n",
      "epoch 3, batch 19, loss = 0.019369328394532204\n",
      "epoch 3, batch 20, loss = 0.03752544894814491\n",
      "epoch 3, batch 21, loss = 0.03715080767869949\n",
      "epoch 3, batch 22, loss = 0.028911635279655457\n",
      "epoch 3, batch 23, loss = 0.04123431071639061\n",
      "epoch 3, batch 24, loss = 0.022642934694886208\n",
      "epoch 3, batch 25, loss = 0.013467520475387573\n",
      "epoch 3, batch 26, loss = 0.03753800690174103\n",
      "epoch 3, batch 27, loss = 0.04689136520028114\n",
      "epoch 3, batch 28, loss = 0.02823282778263092\n",
      "epoch 3, batch 29, loss = 0.03813553228974342\n",
      "epoch 3, batch 30, loss = 0.010019803419709206\n",
      "epoch 3, batch 31, loss = 0.025219857692718506\n",
      "epoch 3, batch 32, loss = 0.05672980844974518\n",
      "epoch 3, batch 33, loss = 0.013157716020941734\n",
      "epoch 3, batch 34, loss = 0.018580490723252296\n",
      "epoch 3, batch 35, loss = 0.0236987117677927\n",
      "epoch 3, batch 36, loss = 0.015167426317930222\n",
      "epoch 3, batch 37, loss = 0.07316549867391586\n",
      "epoch 3, batch 38, loss = 0.06589625030755997\n",
      "epoch 3, batch 39, loss = 0.014052893966436386\n",
      "epoch 3, batch 40, loss = 0.066836416721344\n",
      "epoch 3, batch 41, loss = 0.05477961525321007\n",
      "epoch 3, batch 42, loss = 0.030003640800714493\n",
      "epoch 3, batch 43, loss = 0.01813693158328533\n",
      "epoch 3, batch 44, loss = 0.026328859850764275\n",
      "epoch 3, batch 45, loss = 0.01612365059554577\n",
      "epoch 3, batch 46, loss = 0.01998736336827278\n",
      "epoch 3, batch 47, loss = 0.02965054102241993\n",
      "epoch 3, batch 48, loss = 0.017245208844542503\n",
      "epoch 3, batch 49, loss = 0.05787045508623123\n",
      "epoch 3, batch 50, loss = 0.050179410725831985\n",
      "epoch 3, batch 51, loss = 0.02899865433573723\n",
      "epoch 3, batch 52, loss = 0.04561929777264595\n",
      "epoch 3, batch 53, loss = 0.025852637365460396\n",
      "epoch 3, batch 54, loss = 0.06264142692089081\n",
      "epoch 3, batch 55, loss = 0.008640964515507221\n",
      "epoch 3, batch 56, loss = 0.01888200454413891\n",
      "epoch 3, batch 57, loss = 0.04806175082921982\n",
      "epoch 3, batch 58, loss = 0.05791986361145973\n",
      "epoch 3, batch 59, loss = 0.03530830517411232\n",
      "epoch 3, batch 60, loss = 0.08267905563116074\n",
      "epoch 3, batch 61, loss = 0.08660652488470078\n",
      "epoch 3, batch 62, loss = 0.08481188863515854\n",
      "epoch 3, batch 63, loss = 0.026961829513311386\n",
      "epoch 3, batch 64, loss = 0.028558168560266495\n",
      "epoch 3, batch 65, loss = 0.05989111587405205\n",
      "epoch 3, batch 66, loss = 0.04694575443863869\n",
      "epoch 3, batch 67, loss = 0.03382708132266998\n",
      "epoch 3, batch 68, loss = 0.06318765878677368\n",
      "epoch 3, batch 69, loss = 0.01945347711443901\n",
      "epoch 3, batch 70, loss = 0.025237128138542175\n",
      "epoch 3, batch 71, loss = 0.011362557299435139\n",
      "epoch 3, batch 72, loss = 0.029629284515976906\n",
      "epoch 3, batch 73, loss = 0.06640979647636414\n",
      "epoch 3, batch 74, loss = 0.0719761773943901\n",
      "epoch 3, batch 75, loss = 0.0430329404771328\n",
      "epoch 3, batch 76, loss = 0.06797465682029724\n",
      "epoch 3, batch 77, loss = 0.014390354976058006\n",
      "epoch 3, batch 78, loss = 0.06294693797826767\n",
      "epoch 3, batch 79, loss = 0.03388482332229614\n",
      "epoch 3, batch 80, loss = 0.053871601819992065\n",
      "epoch 3, batch 81, loss = 0.06308066099882126\n",
      "epoch 3, batch 82, loss = 0.038941677659749985\n",
      "epoch 3, batch 83, loss = 0.07025811076164246\n",
      "epoch 3, batch 84, loss = 0.07515799254179001\n",
      "epoch 3, batch 85, loss = 0.03042064979672432\n",
      "epoch 3, batch 86, loss = 0.04293867200613022\n",
      "epoch 3, batch 87, loss = 0.03259722888469696\n",
      "epoch 3, batch 88, loss = 0.030529899522662163\n",
      "epoch 3, batch 89, loss = 0.0813080295920372\n",
      "epoch 3, batch 90, loss = 0.01614588312804699\n",
      "epoch 3, batch 91, loss = 0.05387672781944275\n",
      "epoch 3, batch 92, loss = 0.07525742053985596\n",
      "epoch 3, batch 93, loss = 0.0384473092854023\n",
      "epoch 3, batch 94, loss = 0.045264095067977905\n",
      "epoch 3, batch 95, loss = 0.031163854524493217\n",
      "epoch 3, batch 96, loss = 0.048087432980537415\n",
      "epoch 3, batch 97, loss = 0.041153136640787125\n",
      "epoch 3, batch 98, loss = 0.06322579085826874\n",
      "epoch 3, batch 99, loss = 0.058581285178661346\n",
      "epoch 3, batch 100, loss = 0.05053962767124176\n",
      "epoch 3, batch 101, loss = 0.03167353570461273\n",
      "epoch 3, batch 102, loss = 0.10108158737421036\n",
      "epoch 3, batch 103, loss = 0.07599352300167084\n",
      "epoch 3, batch 104, loss = 0.08888556808233261\n",
      "epoch 3, batch 105, loss = 0.020612966269254684\n",
      "epoch 3, batch 106, loss = 0.02794063650071621\n",
      "epoch 3, batch 107, loss = 0.06010523810982704\n",
      "epoch 3, batch 108, loss = 0.03933952748775482\n",
      "epoch 3, batch 109, loss = 0.0568055659532547\n",
      "epoch 3, batch 110, loss = 0.048723138868808746\n",
      "epoch 3, batch 111, loss = 0.05558798462152481\n",
      "epoch 3, batch 112, loss = 0.058734312653541565\n",
      "epoch 3, batch 113, loss = 0.037222377955913544\n",
      "epoch 3, batch 114, loss = 0.09214544296264648\n",
      "epoch 3, batch 115, loss = 0.028969381004571915\n",
      "epoch 3, batch 116, loss = 0.02046753652393818\n",
      "epoch 3, batch 117, loss = 0.04451736435294151\n",
      "epoch 3, batch 118, loss = 0.024354713037610054\n",
      "epoch 3, batch 119, loss = 0.0702713131904602\n",
      "epoch 3, batch 120, loss = 0.03795244172215462\n",
      "epoch 3, batch 121, loss = 0.0611814484000206\n",
      "epoch 3, batch 122, loss = 0.057869378477334976\n",
      "epoch 3, batch 123, loss = 0.013031005859375\n",
      "epoch 3, batch 124, loss = 0.04992371425032616\n",
      "epoch 3, batch 125, loss = 0.03907714784145355\n",
      "epoch 3, batch 126, loss = 0.03861599788069725\n",
      "epoch 3, batch 127, loss = 0.027269650250673294\n",
      "epoch 3, batch 128, loss = 0.03287207707762718\n",
      "epoch 3, batch 129, loss = 0.04921107739210129\n",
      "epoch 3, batch 130, loss = 0.015127834863960743\n",
      "epoch 3, batch 131, loss = 0.08228956162929535\n",
      "epoch 3, batch 132, loss = 0.08595365285873413\n",
      "epoch 3, batch 133, loss = 0.02560296468436718\n",
      "epoch 3, batch 134, loss = 0.04820575192570686\n",
      "epoch 3, batch 135, loss = 0.017102299258112907\n",
      "epoch 3, batch 136, loss = 0.019118105992674828\n",
      "epoch 3, batch 137, loss = 0.020085828378796577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 138, loss = 0.04299290105700493\n",
      "epoch 3, batch 139, loss = 0.05976196378469467\n",
      "epoch 3, batch 140, loss = 0.03685297816991806\n",
      "epoch 3, batch 141, loss = 0.028913049027323723\n",
      "epoch 3, batch 142, loss = 0.020775064826011658\n",
      "epoch 3, batch 143, loss = 0.045138705521821976\n",
      "epoch 3, batch 144, loss = 0.04534835368394852\n",
      "epoch 3, batch 145, loss = 0.024429945275187492\n",
      "epoch 3, batch 146, loss = 0.018607869744300842\n",
      "epoch 3, batch 147, loss = 0.01885182596743107\n",
      "epoch 3, batch 148, loss = 0.023571422323584557\n",
      "epoch 3, batch 149, loss = 0.03673050180077553\n",
      "epoch 3, batch 150, loss = 0.03155186027288437\n",
      "epoch 3, batch 151, loss = 0.040457554161548615\n",
      "epoch 3, batch 152, loss = 0.05911850556731224\n",
      "epoch 3, batch 153, loss = 0.010503483936190605\n",
      "epoch 3, batch 154, loss = 0.03692091628909111\n",
      "epoch 3, batch 155, loss = 0.04629363492131233\n",
      "epoch 3, batch 156, loss = 0.055661436170339584\n",
      "epoch 3, batch 157, loss = 0.01036430150270462\n",
      "epoch 3, batch 158, loss = 0.07032711803913116\n",
      "epoch 3, batch 159, loss = 0.04034329950809479\n",
      "epoch 3, batch 160, loss = 0.0300062894821167\n",
      "epoch 3, batch 161, loss = 0.057900719344615936\n",
      "epoch 3, batch 162, loss = 0.022582318633794785\n",
      "epoch 3, batch 163, loss = 0.038135092705488205\n",
      "epoch 3, batch 164, loss = 0.06701671332120895\n",
      "epoch 3, batch 165, loss = 0.04038873314857483\n",
      "epoch 3, batch 166, loss = 0.0736028403043747\n",
      "epoch 3, batch 167, loss = 0.04781778156757355\n",
      "epoch 3, batch 168, loss = 0.03303486853837967\n",
      "epoch 3, batch 169, loss = 0.031004726886749268\n",
      "epoch 3, batch 170, loss = 0.04703817516565323\n",
      "epoch 3, batch 171, loss = 0.03640667349100113\n",
      "epoch 3, batch 172, loss = 0.07864537090063095\n",
      "epoch 3, batch 173, loss = 0.05313733592629433\n",
      "epoch 3, batch 174, loss = 0.023558421060442924\n",
      "epoch 3, batch 175, loss = 0.054871488362550735\n",
      "epoch 3, batch 176, loss = 0.017348311841487885\n",
      "epoch 3, batch 177, loss = 0.05198708549141884\n",
      "epoch 3, batch 178, loss = 0.01766234263777733\n",
      "epoch 3, batch 179, loss = 0.006569517310708761\n",
      "epoch 3, batch 180, loss = 0.040491774678230286\n",
      "epoch 3, batch 181, loss = 0.022456813603639603\n",
      "epoch 3, batch 182, loss = 0.04114174842834473\n",
      "epoch 3, batch 183, loss = 0.03469257056713104\n",
      "epoch 3, batch 184, loss = 0.04908803477883339\n",
      "epoch 3, batch 185, loss = 0.03354503586888313\n",
      "epoch 3, batch 186, loss = 0.015053963288664818\n",
      "epoch 3, batch 187, loss = 0.03682413697242737\n",
      "epoch 3, batch 188, loss = 0.03615394979715347\n",
      "epoch 3, batch 189, loss = 0.04028552025556564\n",
      "epoch 3, batch 190, loss = 0.06157856434583664\n",
      "epoch 3, batch 191, loss = 0.02148909494280815\n",
      "epoch 3, batch 192, loss = 0.03000401332974434\n",
      "epoch 3, batch 193, loss = 0.015023049898445606\n",
      "epoch 3, batch 194, loss = 0.04661046341061592\n",
      "epoch 3, batch 195, loss = 0.04022219032049179\n",
      "epoch 3, batch 196, loss = 0.03774205595254898\n",
      "epoch 3, batch 197, loss = 0.06323409825563431\n",
      "epoch 3, batch 198, loss = 0.008132395334541798\n",
      "epoch 3, batch 199, loss = 0.046873122453689575\n",
      "epoch 3, batch 200, loss = 0.054384566843509674\n",
      "epoch 4, batch 2, loss = 0.058566950261592865\n",
      "epoch 4, batch 3, loss = 0.031097549945116043\n",
      "epoch 4, batch 4, loss = 0.01353504043072462\n",
      "epoch 4, batch 5, loss = 0.02722417563199997\n",
      "epoch 4, batch 6, loss = 0.029954280704259872\n",
      "epoch 4, batch 7, loss = 0.017598452046513557\n",
      "epoch 4, batch 8, loss = 0.017770960927009583\n",
      "epoch 4, batch 9, loss = 0.033438537269830704\n",
      "epoch 4, batch 10, loss = 0.10260259360074997\n",
      "epoch 4, batch 11, loss = 0.06365998834371567\n",
      "epoch 4, batch 12, loss = 0.043705735355615616\n",
      "epoch 4, batch 13, loss = 0.017901485785841942\n",
      "epoch 4, batch 14, loss = 0.024662943556904793\n",
      "epoch 4, batch 15, loss = 0.020493190735578537\n",
      "epoch 4, batch 16, loss = 0.033396292477846146\n",
      "epoch 4, batch 17, loss = 0.019159827381372452\n",
      "epoch 4, batch 18, loss = 0.02075737714767456\n",
      "epoch 4, batch 19, loss = 0.011315878480672836\n",
      "epoch 4, batch 20, loss = 0.043727412819862366\n",
      "epoch 4, batch 21, loss = 0.054187875241041183\n",
      "epoch 4, batch 22, loss = 0.029352931305766106\n",
      "epoch 4, batch 23, loss = 0.010382269509136677\n",
      "epoch 4, batch 24, loss = 0.04300583899021149\n",
      "epoch 4, batch 25, loss = 0.018127942457795143\n",
      "epoch 4, batch 26, loss = 0.03169216215610504\n",
      "epoch 4, batch 27, loss = 0.08874614536762238\n",
      "epoch 4, batch 28, loss = 0.032630521804094315\n",
      "epoch 4, batch 29, loss = 0.028999684378504753\n",
      "epoch 4, batch 30, loss = 0.02870568074285984\n",
      "epoch 4, batch 31, loss = 0.010935943573713303\n",
      "epoch 4, batch 32, loss = 0.02548556588590145\n",
      "epoch 4, batch 33, loss = 0.01164290402084589\n",
      "epoch 4, batch 34, loss = 0.055088747292757034\n",
      "epoch 4, batch 35, loss = 0.028784895315766335\n",
      "epoch 4, batch 36, loss = 0.01885790377855301\n",
      "epoch 4, batch 37, loss = 0.028492964804172516\n",
      "epoch 4, batch 38, loss = 0.014633102342486382\n",
      "epoch 4, batch 39, loss = 0.025494705885648727\n",
      "epoch 4, batch 40, loss = 0.019088825210928917\n",
      "epoch 4, batch 41, loss = 0.010592016391456127\n",
      "epoch 4, batch 42, loss = 0.007063066586852074\n",
      "epoch 4, batch 43, loss = 0.0254074577242136\n",
      "epoch 4, batch 44, loss = 0.034685131162405014\n",
      "epoch 4, batch 45, loss = 0.03527618199586868\n",
      "epoch 4, batch 46, loss = 0.007719445042312145\n",
      "epoch 4, batch 47, loss = 0.02384241111576557\n",
      "epoch 4, batch 48, loss = 0.04657593369483948\n",
      "epoch 4, batch 49, loss = 0.017298100516200066\n",
      "epoch 4, batch 50, loss = 0.01833439990878105\n",
      "epoch 4, batch 51, loss = 0.02738545648753643\n",
      "epoch 4, batch 52, loss = 0.026716697961091995\n",
      "epoch 4, batch 53, loss = 0.0469336211681366\n",
      "epoch 4, batch 54, loss = 0.013327258639037609\n",
      "epoch 4, batch 55, loss = 0.013676178641617298\n",
      "epoch 4, batch 56, loss = 0.025190651416778564\n",
      "epoch 4, batch 57, loss = 0.016157066449522972\n",
      "epoch 4, batch 58, loss = 0.019290853291749954\n",
      "epoch 4, batch 59, loss = 0.04057919979095459\n",
      "epoch 4, batch 60, loss = 0.03096790984272957\n",
      "epoch 4, batch 61, loss = 0.009799325838685036\n",
      "epoch 4, batch 62, loss = 0.018032053485512733\n",
      "epoch 4, batch 63, loss = 0.03715841844677925\n",
      "epoch 4, batch 64, loss = 0.04131228104233742\n",
      "epoch 4, batch 65, loss = 0.027346068993210793\n",
      "epoch 4, batch 66, loss = 0.048418160527944565\n",
      "epoch 4, batch 67, loss = 0.010044809430837631\n",
      "epoch 4, batch 68, loss = 0.03176434710621834\n",
      "epoch 4, batch 69, loss = 0.012935692444443703\n",
      "epoch 4, batch 70, loss = 0.029879705980420113\n",
      "epoch 4, batch 71, loss = 0.019792912527918816\n",
      "epoch 4, batch 72, loss = 0.016317350789904594\n",
      "epoch 4, batch 73, loss = 0.038656409829854965\n",
      "epoch 4, batch 74, loss = 0.027482323348522186\n",
      "epoch 4, batch 75, loss = 0.012755997478961945\n",
      "epoch 4, batch 76, loss = 0.04368780180811882\n",
      "epoch 4, batch 77, loss = 0.03320528566837311\n",
      "epoch 4, batch 78, loss = 0.030292538926005363\n",
      "epoch 4, batch 79, loss = 0.06609790772199631\n",
      "epoch 4, batch 80, loss = 0.017834868282079697\n",
      "epoch 4, batch 81, loss = 0.02152118645608425\n",
      "epoch 4, batch 82, loss = 0.09161083400249481\n",
      "epoch 4, batch 83, loss = 0.03678055852651596\n",
      "epoch 4, batch 84, loss = 0.022905917838215828\n",
      "epoch 4, batch 85, loss = 0.007085259072482586\n",
      "epoch 4, batch 86, loss = 0.04110535606741905\n",
      "epoch 4, batch 87, loss = 0.006954421289265156\n",
      "epoch 4, batch 88, loss = 0.02979319356381893\n",
      "epoch 4, batch 89, loss = 0.034108974039554596\n",
      "epoch 4, batch 90, loss = 0.06260211765766144\n",
      "epoch 4, batch 91, loss = 0.006097250618040562\n",
      "epoch 4, batch 92, loss = 0.03155452385544777\n",
      "epoch 4, batch 93, loss = 0.009050125256180763\n",
      "epoch 4, batch 94, loss = 0.012181694619357586\n",
      "epoch 4, batch 95, loss = 0.049425460398197174\n",
      "epoch 4, batch 96, loss = 0.014719485305249691\n",
      "epoch 4, batch 97, loss = 0.0078103370033204556\n",
      "epoch 4, batch 98, loss = 0.028925161808729172\n",
      "epoch 4, batch 99, loss = 0.004098105244338512\n",
      "epoch 4, batch 100, loss = 0.04622522369027138\n",
      "epoch 4, batch 101, loss = 0.008879252709448338\n",
      "epoch 4, batch 102, loss = 0.043398451060056686\n",
      "epoch 4, batch 103, loss = 0.015946518629789352\n",
      "epoch 4, batch 104, loss = 0.019033417105674744\n",
      "epoch 4, batch 105, loss = 0.028213482350111008\n",
      "epoch 4, batch 106, loss = 0.008195780217647552\n",
      "epoch 4, batch 107, loss = 0.05786149203777313\n",
      "epoch 4, batch 108, loss = 0.0258281659334898\n",
      "epoch 4, batch 109, loss = 0.05112365633249283\n",
      "epoch 4, batch 110, loss = 0.00439264252781868\n",
      "epoch 4, batch 111, loss = 0.03569065034389496\n",
      "epoch 4, batch 112, loss = 0.022623715922236443\n",
      "epoch 4, batch 113, loss = 0.029345836490392685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 114, loss = 0.023956023156642914\n",
      "epoch 4, batch 115, loss = 0.018056683242321014\n",
      "epoch 4, batch 116, loss = 0.02671082504093647\n",
      "epoch 4, batch 117, loss = 0.012370525859296322\n",
      "epoch 4, batch 118, loss = 0.03244800493121147\n",
      "epoch 4, batch 119, loss = 0.023915447294712067\n",
      "epoch 4, batch 120, loss = 0.042474281042814255\n",
      "epoch 4, batch 121, loss = 0.04853648692369461\n",
      "epoch 4, batch 122, loss = 0.014810805208981037\n",
      "epoch 4, batch 123, loss = 0.030283043161034584\n",
      "epoch 4, batch 124, loss = 0.02412671037018299\n",
      "epoch 4, batch 125, loss = 0.05062371864914894\n",
      "epoch 4, batch 126, loss = 0.007174113765358925\n",
      "epoch 4, batch 127, loss = 0.06499283015727997\n",
      "epoch 4, batch 128, loss = 0.025952525436878204\n",
      "epoch 4, batch 129, loss = 0.05460438132286072\n",
      "epoch 4, batch 130, loss = 0.0585109107196331\n",
      "epoch 4, batch 131, loss = 0.029693912714719772\n",
      "epoch 4, batch 132, loss = 0.02048627845942974\n",
      "epoch 4, batch 133, loss = 0.03957844153046608\n",
      "epoch 4, batch 134, loss = 0.016599297523498535\n",
      "epoch 4, batch 135, loss = 0.03236939385533333\n",
      "epoch 4, batch 136, loss = 0.012778853066265583\n",
      "epoch 4, batch 137, loss = 0.035142138600349426\n",
      "epoch 4, batch 138, loss = 0.027191076427698135\n",
      "epoch 4, batch 139, loss = 0.02842402644455433\n",
      "epoch 4, batch 140, loss = 0.05174694210290909\n",
      "epoch 4, batch 141, loss = 0.047760456800460815\n",
      "epoch 4, batch 142, loss = 0.03269609063863754\n",
      "epoch 4, batch 143, loss = 0.031635772436857224\n",
      "epoch 4, batch 144, loss = 0.028961600735783577\n",
      "epoch 4, batch 145, loss = 0.022981492802500725\n",
      "epoch 4, batch 146, loss = 0.03758027404546738\n",
      "epoch 4, batch 147, loss = 0.024332940578460693\n",
      "epoch 4, batch 148, loss = 0.04984699562191963\n",
      "epoch 4, batch 149, loss = 0.010054162703454494\n",
      "epoch 4, batch 150, loss = 0.01521791610866785\n",
      "epoch 4, batch 151, loss = 0.024824710562825203\n",
      "epoch 4, batch 152, loss = 0.007791073527187109\n",
      "epoch 4, batch 153, loss = 0.007819153368473053\n",
      "epoch 4, batch 154, loss = 0.06276867538690567\n",
      "epoch 4, batch 155, loss = 0.03775136545300484\n",
      "epoch 4, batch 156, loss = 0.005628323182463646\n",
      "epoch 4, batch 157, loss = 0.06315209716558456\n",
      "epoch 4, batch 158, loss = 0.008894754573702812\n",
      "epoch 4, batch 159, loss = 0.04485334828495979\n",
      "epoch 4, batch 160, loss = 0.01836322993040085\n",
      "epoch 4, batch 161, loss = 0.03469718247652054\n",
      "epoch 4, batch 162, loss = 0.06334687024354935\n",
      "epoch 4, batch 163, loss = 0.053573571145534515\n",
      "epoch 4, batch 164, loss = 0.018016977235674858\n",
      "epoch 4, batch 165, loss = 0.012628140859305859\n",
      "epoch 4, batch 166, loss = 0.04285053908824921\n",
      "epoch 4, batch 167, loss = 0.017770566046237946\n",
      "epoch 4, batch 168, loss = 0.009039240889251232\n",
      "epoch 4, batch 169, loss = 0.031002333387732506\n",
      "epoch 4, batch 170, loss = 0.017015276476740837\n",
      "epoch 4, batch 171, loss = 0.01682310551404953\n",
      "epoch 4, batch 172, loss = 0.04358726367354393\n",
      "epoch 4, batch 173, loss = 0.009366217069327831\n",
      "epoch 4, batch 174, loss = 0.032657694071531296\n",
      "epoch 4, batch 175, loss = 0.006441956385970116\n",
      "epoch 4, batch 176, loss = 0.02283382974565029\n",
      "epoch 4, batch 177, loss = 0.016130594536662102\n",
      "epoch 4, batch 178, loss = 0.009547268971800804\n",
      "epoch 4, batch 179, loss = 0.011983292177319527\n",
      "epoch 4, batch 180, loss = 0.016321463510394096\n",
      "epoch 4, batch 181, loss = 0.04816397652029991\n",
      "epoch 4, batch 182, loss = 0.04650602862238884\n",
      "epoch 4, batch 183, loss = 0.021233974024653435\n",
      "epoch 4, batch 184, loss = 0.006979655008763075\n",
      "epoch 4, batch 185, loss = 0.01943572238087654\n",
      "epoch 4, batch 186, loss = 0.049564070999622345\n",
      "epoch 4, batch 187, loss = 0.014384787529706955\n",
      "epoch 4, batch 188, loss = 0.06738824397325516\n",
      "epoch 4, batch 189, loss = 0.1314343810081482\n",
      "epoch 4, batch 190, loss = 0.07527291029691696\n",
      "epoch 4, batch 191, loss = 0.058644410222768784\n",
      "epoch 4, batch 192, loss = 0.02167299948632717\n",
      "epoch 4, batch 193, loss = 0.056142810732126236\n",
      "epoch 4, batch 194, loss = 0.02971915900707245\n",
      "epoch 4, batch 195, loss = 0.027136575430631638\n",
      "epoch 4, batch 196, loss = 0.07110308855772018\n",
      "epoch 4, batch 197, loss = 0.0319964736700058\n",
      "epoch 4, batch 198, loss = 0.014626576565206051\n",
      "epoch 4, batch 199, loss = 0.16409902274608612\n",
      "epoch 4, batch 200, loss = 0.12100621312856674\n",
      "epoch 5, batch 2, loss = 0.011512036435306072\n",
      "epoch 5, batch 3, loss = 0.06066667661070824\n",
      "epoch 5, batch 4, loss = 0.021391015499830246\n",
      "epoch 5, batch 5, loss = 0.027258463203907013\n",
      "epoch 5, batch 6, loss = 0.022401077672839165\n",
      "epoch 5, batch 7, loss = 0.04123009741306305\n",
      "epoch 5, batch 8, loss = 0.01393724326044321\n",
      "epoch 5, batch 9, loss = 0.005997973494231701\n",
      "epoch 5, batch 10, loss = 0.0036286995746195316\n",
      "epoch 5, batch 11, loss = 0.028529293835163116\n",
      "epoch 5, batch 12, loss = 0.011354440823197365\n",
      "epoch 5, batch 13, loss = 0.007014123257249594\n",
      "epoch 5, batch 14, loss = 0.005116207990795374\n",
      "epoch 5, batch 15, loss = 0.013755532912909985\n",
      "epoch 5, batch 16, loss = 0.03184161335229874\n",
      "epoch 5, batch 17, loss = 0.015494185499846935\n",
      "epoch 5, batch 18, loss = 0.023491596803069115\n",
      "epoch 5, batch 19, loss = 0.008267105557024479\n",
      "epoch 5, batch 20, loss = 0.043192289769649506\n",
      "epoch 5, batch 21, loss = 0.008208696730434895\n",
      "epoch 5, batch 22, loss = 0.004060209263116121\n",
      "epoch 5, batch 23, loss = 0.00915498472750187\n",
      "epoch 5, batch 24, loss = 0.03363102674484253\n",
      "epoch 5, batch 25, loss = 0.03259708732366562\n",
      "epoch 5, batch 26, loss = 0.03067767806351185\n",
      "epoch 5, batch 27, loss = 0.04168782755732536\n",
      "epoch 5, batch 28, loss = 0.03563931956887245\n",
      "epoch 5, batch 29, loss = 0.03189904987812042\n",
      "epoch 5, batch 30, loss = 0.023241212591528893\n",
      "epoch 5, batch 31, loss = 0.027675503864884377\n",
      "epoch 5, batch 32, loss = 0.010554484091699123\n",
      "epoch 5, batch 33, loss = 0.005849041976034641\n",
      "epoch 5, batch 34, loss = 0.041314635425806046\n",
      "epoch 5, batch 35, loss = 0.013950088992714882\n",
      "epoch 5, batch 36, loss = 0.015727851539850235\n",
      "epoch 5, batch 37, loss = 0.011546721681952477\n",
      "epoch 5, batch 38, loss = 0.03605818375945091\n",
      "epoch 5, batch 39, loss = 0.04519808664917946\n",
      "epoch 5, batch 40, loss = 0.025190815329551697\n",
      "epoch 5, batch 41, loss = 0.027680538594722748\n",
      "epoch 5, batch 42, loss = 0.027627751231193542\n",
      "epoch 5, batch 43, loss = 0.027818864211440086\n",
      "epoch 5, batch 44, loss = 0.01280478946864605\n",
      "epoch 5, batch 45, loss = 0.010508076287806034\n",
      "epoch 5, batch 46, loss = 0.0300857312977314\n",
      "epoch 5, batch 47, loss = 0.031207527965307236\n",
      "epoch 5, batch 48, loss = 0.0050248317420482635\n",
      "epoch 5, batch 49, loss = 0.024703100323677063\n",
      "epoch 5, batch 50, loss = 0.04249979183077812\n",
      "epoch 5, batch 51, loss = 0.024789460003376007\n",
      "epoch 5, batch 52, loss = 0.05861659348011017\n",
      "epoch 5, batch 53, loss = 0.025875484570860863\n",
      "epoch 5, batch 54, loss = 0.021642012521624565\n",
      "epoch 5, batch 55, loss = 0.026847444474697113\n",
      "epoch 5, batch 56, loss = 0.020440075546503067\n",
      "epoch 5, batch 57, loss = 0.013515453785657883\n",
      "epoch 5, batch 58, loss = 0.059996284544467926\n",
      "epoch 5, batch 59, loss = 0.01803816854953766\n",
      "epoch 5, batch 60, loss = 0.008690245449543\n",
      "epoch 5, batch 61, loss = 0.017812959849834442\n",
      "epoch 5, batch 62, loss = 0.022378146648406982\n",
      "epoch 5, batch 63, loss = 0.015363208949565887\n",
      "epoch 5, batch 64, loss = 0.014079762622714043\n",
      "epoch 5, batch 65, loss = 0.014293158426880836\n",
      "epoch 5, batch 66, loss = 0.012728387489914894\n",
      "epoch 5, batch 67, loss = 0.002175703179091215\n",
      "epoch 5, batch 68, loss = 0.010087501257658005\n",
      "epoch 5, batch 69, loss = 0.00922727957367897\n",
      "epoch 5, batch 70, loss = 0.009085620753467083\n",
      "epoch 5, batch 71, loss = 0.012848848477005959\n",
      "epoch 5, batch 72, loss = 0.06069544330239296\n",
      "epoch 5, batch 73, loss = 0.02394949644804001\n",
      "epoch 5, batch 74, loss = 0.04346796125173569\n",
      "epoch 5, batch 75, loss = 0.031620364636182785\n",
      "epoch 5, batch 76, loss = 0.029510455206036568\n",
      "epoch 5, batch 77, loss = 0.02270800806581974\n",
      "epoch 5, batch 78, loss = 0.04403981566429138\n",
      "epoch 5, batch 79, loss = 0.004052676726132631\n",
      "epoch 5, batch 80, loss = 0.013676854781806469\n",
      "epoch 5, batch 81, loss = 0.020794516429305077\n",
      "epoch 5, batch 82, loss = 0.02686925418674946\n",
      "epoch 5, batch 83, loss = 0.028542065992951393\n",
      "epoch 5, batch 84, loss = 0.005362921394407749\n",
      "epoch 5, batch 85, loss = 0.018656931817531586\n",
      "epoch 5, batch 86, loss = 0.04286876320838928\n",
      "epoch 5, batch 87, loss = 0.018291380256414413\n",
      "epoch 5, batch 88, loss = 0.018114546313881874\n",
      "epoch 5, batch 89, loss = 0.015471936203539371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, batch 90, loss = 0.005294380243867636\n",
      "epoch 5, batch 91, loss = 0.02721977047622204\n",
      "epoch 5, batch 92, loss = 0.03220772370696068\n",
      "epoch 5, batch 93, loss = 0.0033134091645479202\n",
      "epoch 5, batch 94, loss = 0.040043048560619354\n",
      "epoch 5, batch 95, loss = 0.013929561711847782\n",
      "epoch 5, batch 96, loss = 0.009135907515883446\n",
      "epoch 5, batch 97, loss = 0.06540325284004211\n",
      "epoch 5, batch 98, loss = 0.054975010454654694\n",
      "epoch 5, batch 99, loss = 0.003929158207029104\n",
      "epoch 5, batch 100, loss = 0.020725222304463387\n",
      "epoch 5, batch 101, loss = 0.022556306794285774\n",
      "epoch 5, batch 102, loss = 0.008286049589514732\n",
      "epoch 5, batch 103, loss = 0.003461638931185007\n",
      "epoch 5, batch 104, loss = 0.026164818555116653\n",
      "epoch 5, batch 105, loss = 0.022056037560105324\n",
      "epoch 5, batch 106, loss = 0.024850839748978615\n",
      "epoch 5, batch 107, loss = 0.007990666665136814\n",
      "epoch 5, batch 108, loss = 0.0132426293566823\n",
      "epoch 5, batch 109, loss = 0.03356434404850006\n",
      "epoch 5, batch 110, loss = 0.03227751702070236\n",
      "epoch 5, batch 111, loss = 0.007484896574169397\n",
      "epoch 5, batch 112, loss = 0.018489520996809006\n",
      "epoch 5, batch 113, loss = 0.02580457180738449\n",
      "epoch 5, batch 114, loss = 0.01548904087394476\n",
      "epoch 5, batch 115, loss = 0.009090582840144634\n",
      "epoch 5, batch 116, loss = 0.015402666293084621\n",
      "epoch 5, batch 117, loss = 0.012991968542337418\n",
      "epoch 5, batch 118, loss = 0.01977626048028469\n",
      "epoch 5, batch 119, loss = 0.009432954713702202\n",
      "epoch 5, batch 120, loss = 0.013020511716604233\n",
      "epoch 5, batch 121, loss = 0.023430924862623215\n",
      "epoch 5, batch 122, loss = 0.017246900126338005\n",
      "epoch 5, batch 123, loss = 0.053232498466968536\n",
      "epoch 5, batch 124, loss = 0.01868283748626709\n",
      "epoch 5, batch 125, loss = 0.020802496001124382\n",
      "epoch 5, batch 126, loss = 0.021851642057299614\n",
      "epoch 5, batch 127, loss = 0.01848096027970314\n",
      "epoch 5, batch 128, loss = 0.012720911763608456\n",
      "epoch 5, batch 129, loss = 0.012872005812823772\n",
      "epoch 5, batch 130, loss = 0.01800215244293213\n",
      "epoch 5, batch 131, loss = 0.04442216828465462\n",
      "epoch 5, batch 132, loss = 0.013958088122308254\n",
      "epoch 5, batch 133, loss = 0.00994326826184988\n",
      "epoch 5, batch 134, loss = 0.005332190543413162\n",
      "epoch 5, batch 135, loss = 0.014348759315907955\n",
      "epoch 5, batch 136, loss = 0.009218425489962101\n",
      "epoch 5, batch 137, loss = 0.00911159347742796\n",
      "epoch 5, batch 138, loss = 0.013305816799402237\n",
      "epoch 5, batch 139, loss = 0.007940827868878841\n",
      "epoch 5, batch 140, loss = 0.03299608826637268\n",
      "epoch 5, batch 141, loss = 0.03517571836709976\n",
      "epoch 5, batch 142, loss = 0.009404231794178486\n",
      "epoch 5, batch 143, loss = 0.027536144480109215\n",
      "epoch 5, batch 144, loss = 0.026723818853497505\n",
      "epoch 5, batch 145, loss = 0.03009507805109024\n",
      "epoch 5, batch 146, loss = 0.013059351593255997\n",
      "epoch 5, batch 147, loss = 0.06264522671699524\n",
      "epoch 5, batch 148, loss = 0.03405305743217468\n",
      "epoch 5, batch 149, loss = 0.006580065470188856\n",
      "epoch 5, batch 150, loss = 0.033123549073934555\n",
      "epoch 5, batch 151, loss = 0.0031573548913002014\n",
      "epoch 5, batch 152, loss = 0.007763233967125416\n",
      "epoch 5, batch 153, loss = 0.02410154603421688\n",
      "epoch 5, batch 154, loss = 0.0074367402121424675\n",
      "epoch 5, batch 155, loss = 0.023385513573884964\n",
      "epoch 5, batch 156, loss = 0.009358036331832409\n",
      "epoch 5, batch 157, loss = 0.018220754340291023\n",
      "epoch 5, batch 158, loss = 0.017103232443332672\n",
      "epoch 5, batch 159, loss = 0.039199814200401306\n",
      "epoch 5, batch 160, loss = 0.01663183979690075\n",
      "epoch 5, batch 161, loss = 0.011195901781320572\n",
      "epoch 5, batch 162, loss = 0.015568231232464314\n",
      "epoch 5, batch 163, loss = 0.0046286266297101974\n",
      "epoch 5, batch 164, loss = 0.0074539328925311565\n",
      "epoch 5, batch 165, loss = 0.06227658689022064\n",
      "epoch 5, batch 166, loss = 0.021529830992221832\n",
      "epoch 5, batch 167, loss = 0.039916519075632095\n",
      "epoch 5, batch 168, loss = 0.013527129776775837\n",
      "epoch 5, batch 169, loss = 0.015702342614531517\n",
      "epoch 5, batch 170, loss = 0.012791048735380173\n",
      "epoch 5, batch 171, loss = 0.02529156394302845\n",
      "epoch 5, batch 172, loss = 0.015321982093155384\n",
      "epoch 5, batch 173, loss = 0.06027555093169212\n",
      "epoch 5, batch 174, loss = 0.05018123239278793\n",
      "epoch 5, batch 175, loss = 0.07112959027290344\n",
      "epoch 5, batch 176, loss = 0.031407710164785385\n",
      "epoch 5, batch 177, loss = 0.022756798192858696\n",
      "epoch 5, batch 178, loss = 0.06084571033716202\n",
      "epoch 5, batch 179, loss = 0.10183825343847275\n",
      "epoch 5, batch 180, loss = 0.01824522204697132\n",
      "epoch 5, batch 181, loss = 0.04295171797275543\n",
      "epoch 5, batch 182, loss = 0.03628234937787056\n",
      "epoch 5, batch 183, loss = 0.04117997735738754\n",
      "epoch 5, batch 184, loss = 0.02542593516409397\n",
      "epoch 5, batch 185, loss = 0.05200687795877457\n",
      "epoch 5, batch 186, loss = 0.04628017917275429\n",
      "epoch 5, batch 187, loss = 0.013087079860270023\n",
      "epoch 5, batch 188, loss = 0.007136802654713392\n",
      "epoch 5, batch 189, loss = 0.06879325956106186\n",
      "epoch 5, batch 190, loss = 0.021038472652435303\n",
      "epoch 5, batch 191, loss = 0.039027027785778046\n",
      "epoch 5, batch 192, loss = 0.048521626740694046\n",
      "epoch 5, batch 193, loss = 0.042660001665353775\n",
      "epoch 5, batch 194, loss = 0.035772498697042465\n",
      "epoch 5, batch 195, loss = 0.018210964277386665\n",
      "epoch 5, batch 196, loss = 0.025163982063531876\n",
      "epoch 5, batch 197, loss = 0.03448515757918358\n",
      "epoch 5, batch 198, loss = 0.022224171087145805\n",
      "epoch 5, batch 199, loss = 0.009586971253156662\n",
      "epoch 5, batch 200, loss = 0.004088875371962786\n",
      "epoch 6, batch 2, loss = 0.0023091507609933615\n",
      "epoch 6, batch 3, loss = 0.008974685333669186\n",
      "epoch 6, batch 4, loss = 0.024222392588853836\n",
      "epoch 6, batch 5, loss = 0.015429651364684105\n",
      "epoch 6, batch 6, loss = 0.03909725323319435\n",
      "epoch 6, batch 7, loss = 0.01077097188681364\n",
      "epoch 6, batch 8, loss = 0.015780864283442497\n",
      "epoch 6, batch 9, loss = 0.02574882283806801\n",
      "epoch 6, batch 10, loss = 0.0038482341915369034\n",
      "epoch 6, batch 11, loss = 0.012023227289319038\n",
      "epoch 6, batch 12, loss = 0.01851242035627365\n",
      "epoch 6, batch 13, loss = 0.03322387486696243\n",
      "epoch 6, batch 14, loss = 0.034531932324171066\n",
      "epoch 6, batch 15, loss = 0.014535379596054554\n",
      "epoch 6, batch 16, loss = 0.012536890804767609\n",
      "epoch 6, batch 17, loss = 0.021552929654717445\n",
      "epoch 6, batch 18, loss = 0.01688186638057232\n",
      "epoch 6, batch 19, loss = 0.013954673893749714\n",
      "epoch 6, batch 20, loss = 0.024359574541449547\n",
      "epoch 6, batch 21, loss = 0.024796554818749428\n",
      "epoch 6, batch 22, loss = 0.01033052895218134\n",
      "epoch 6, batch 23, loss = 0.008875343948602676\n",
      "epoch 6, batch 24, loss = 0.014589949510991573\n",
      "epoch 6, batch 25, loss = 0.02431607060134411\n",
      "epoch 6, batch 26, loss = 0.019495896995067596\n",
      "epoch 6, batch 27, loss = 0.011758554726839066\n",
      "epoch 6, batch 28, loss = 0.009349862113595009\n",
      "epoch 6, batch 29, loss = 0.024638338014483452\n",
      "epoch 6, batch 30, loss = 0.02249246835708618\n",
      "epoch 6, batch 31, loss = 0.014338778331875801\n",
      "epoch 6, batch 32, loss = 0.02320941537618637\n",
      "epoch 6, batch 33, loss = 0.020107218995690346\n",
      "epoch 6, batch 34, loss = 0.0204125065356493\n",
      "epoch 6, batch 35, loss = 0.010774862952530384\n",
      "epoch 6, batch 36, loss = 0.019343003630638123\n",
      "epoch 6, batch 37, loss = 0.0020586159080266953\n",
      "epoch 6, batch 38, loss = 0.010115060955286026\n",
      "epoch 6, batch 39, loss = 0.01125493086874485\n",
      "epoch 6, batch 40, loss = 0.014177965931594372\n",
      "epoch 6, batch 41, loss = 0.001800853875465691\n",
      "epoch 6, batch 42, loss = 0.011515065096318722\n",
      "epoch 6, batch 43, loss = 0.01617303304374218\n",
      "epoch 6, batch 44, loss = 0.009610173292458057\n",
      "epoch 6, batch 45, loss = 0.013100184500217438\n",
      "epoch 6, batch 46, loss = 0.008561906404793262\n",
      "epoch 6, batch 47, loss = 0.019502969458699226\n",
      "epoch 6, batch 48, loss = 0.007418016903102398\n",
      "epoch 6, batch 49, loss = 0.02926829271018505\n",
      "epoch 6, batch 50, loss = 0.008238915354013443\n",
      "epoch 6, batch 51, loss = 0.02192491479218006\n",
      "epoch 6, batch 52, loss = 0.026834232732653618\n",
      "epoch 6, batch 53, loss = 0.019492773339152336\n",
      "epoch 6, batch 54, loss = 0.04617561027407646\n",
      "epoch 6, batch 55, loss = 0.012589513324201107\n",
      "epoch 6, batch 56, loss = 0.04121941700577736\n",
      "epoch 6, batch 57, loss = 0.048250362277030945\n",
      "epoch 6, batch 58, loss = 0.02241506800055504\n",
      "epoch 6, batch 59, loss = 0.006604132708162069\n",
      "epoch 6, batch 60, loss = 0.0027783988043665886\n",
      "epoch 6, batch 61, loss = 0.005694859195500612\n",
      "epoch 6, batch 62, loss = 0.02003798633813858\n",
      "epoch 6, batch 63, loss = 0.05551273003220558\n",
      "epoch 6, batch 64, loss = 0.03273717314004898\n",
      "epoch 6, batch 65, loss = 0.00297393836081028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, batch 66, loss = 0.011649204418063164\n",
      "epoch 6, batch 67, loss = 0.041961293667554855\n",
      "epoch 6, batch 68, loss = 0.0376485213637352\n",
      "epoch 6, batch 69, loss = 0.020590433850884438\n",
      "epoch 6, batch 70, loss = 0.05040924996137619\n",
      "epoch 6, batch 71, loss = 0.010448683053255081\n",
      "epoch 6, batch 72, loss = 0.025269119068980217\n",
      "epoch 6, batch 73, loss = 0.042569708079099655\n",
      "epoch 6, batch 74, loss = 0.0026827622205018997\n",
      "epoch 6, batch 75, loss = 0.034452084451913834\n",
      "epoch 6, batch 76, loss = 0.004939587786793709\n",
      "epoch 6, batch 77, loss = 0.00693867914378643\n",
      "epoch 6, batch 78, loss = 0.04360097274184227\n",
      "epoch 6, batch 79, loss = 0.04636388272047043\n",
      "epoch 6, batch 80, loss = 0.028523758053779602\n",
      "epoch 6, batch 81, loss = 0.02140725776553154\n",
      "epoch 6, batch 82, loss = 0.013731665909290314\n",
      "epoch 6, batch 83, loss = 0.02852596342563629\n",
      "epoch 6, batch 84, loss = 0.018288293853402138\n",
      "epoch 6, batch 85, loss = 0.014035599306225777\n",
      "epoch 6, batch 86, loss = 0.029652705416083336\n",
      "epoch 6, batch 87, loss = 0.015094581060111523\n",
      "epoch 6, batch 88, loss = 0.009966808371245861\n",
      "epoch 6, batch 89, loss = 0.060370270162820816\n",
      "epoch 6, batch 90, loss = 0.008950453251600266\n",
      "epoch 6, batch 91, loss = 0.013255328871309757\n",
      "epoch 6, batch 92, loss = 0.008025160059332848\n",
      "epoch 6, batch 93, loss = 0.022914431989192963\n",
      "epoch 6, batch 94, loss = 0.009298964403569698\n",
      "epoch 6, batch 95, loss = 0.05379072576761246\n",
      "epoch 6, batch 96, loss = 0.010461678728461266\n",
      "epoch 6, batch 97, loss = 0.0219796784222126\n",
      "epoch 6, batch 98, loss = 0.017479652538895607\n",
      "epoch 6, batch 99, loss = 0.009638728573918343\n",
      "epoch 6, batch 100, loss = 0.0028266615699976683\n",
      "epoch 6, batch 101, loss = 0.01460782065987587\n",
      "epoch 6, batch 102, loss = 0.06988660246133804\n",
      "epoch 6, batch 103, loss = 0.009477981366217136\n",
      "epoch 6, batch 104, loss = 0.002810709411278367\n",
      "epoch 6, batch 105, loss = 0.01644408144056797\n",
      "epoch 6, batch 106, loss = 0.02849733643233776\n",
      "epoch 6, batch 107, loss = 0.02918998897075653\n",
      "epoch 6, batch 108, loss = 0.004477871581912041\n",
      "epoch 6, batch 109, loss = 0.005593977402895689\n",
      "epoch 6, batch 110, loss = 0.043713439255952835\n",
      "epoch 6, batch 111, loss = 0.00794028490781784\n",
      "epoch 6, batch 112, loss = 0.034052226692438126\n",
      "epoch 6, batch 113, loss = 0.009717674925923347\n",
      "epoch 6, batch 114, loss = 0.01982877403497696\n",
      "epoch 6, batch 115, loss = 0.013108748942613602\n",
      "epoch 6, batch 116, loss = 0.006038594990968704\n",
      "epoch 6, batch 117, loss = 0.03741535544395447\n",
      "epoch 6, batch 118, loss = 0.004319960717111826\n",
      "epoch 6, batch 119, loss = 0.007489235606044531\n",
      "epoch 6, batch 120, loss = 0.005646910052746534\n",
      "epoch 6, batch 121, loss = 0.010108127258718014\n",
      "epoch 6, batch 122, loss = 0.026976004242897034\n",
      "epoch 6, batch 123, loss = 0.044901590794324875\n",
      "epoch 6, batch 124, loss = 0.02035566046833992\n",
      "epoch 6, batch 125, loss = 0.00541633740067482\n",
      "epoch 6, batch 126, loss = 0.020685024559497833\n",
      "epoch 6, batch 127, loss = 0.011949457228183746\n",
      "epoch 6, batch 128, loss = 0.01006799004971981\n",
      "epoch 6, batch 129, loss = 0.023123849183321\n",
      "epoch 6, batch 130, loss = 0.0024406583979725838\n",
      "epoch 6, batch 131, loss = 0.02046120911836624\n",
      "epoch 6, batch 132, loss = 0.00663806963711977\n",
      "epoch 6, batch 133, loss = 0.003935550339519978\n",
      "epoch 6, batch 134, loss = 0.006939734797924757\n",
      "epoch 6, batch 135, loss = 0.0020905020646750927\n",
      "epoch 6, batch 136, loss = 0.00287872483022511\n",
      "epoch 6, batch 137, loss = 0.0034838232677429914\n",
      "epoch 6, batch 138, loss = 0.03170621395111084\n",
      "epoch 6, batch 139, loss = 0.01561681553721428\n",
      "epoch 6, batch 140, loss = 0.007356927264481783\n",
      "epoch 6, batch 141, loss = 0.02678764797747135\n",
      "epoch 6, batch 142, loss = 0.012979507446289062\n",
      "epoch 6, batch 143, loss = 0.03800220042467117\n",
      "epoch 6, batch 144, loss = 0.012611866928637028\n",
      "epoch 6, batch 145, loss = 0.015450774691998959\n",
      "epoch 6, batch 146, loss = 0.014466497115790844\n",
      "epoch 6, batch 147, loss = 0.0037367790937423706\n",
      "epoch 6, batch 148, loss = 0.01871469058096409\n",
      "epoch 6, batch 149, loss = 0.026504475623369217\n",
      "epoch 6, batch 150, loss = 0.007182651199400425\n",
      "epoch 6, batch 151, loss = 0.031558603048324585\n",
      "epoch 6, batch 152, loss = 0.02343169040977955\n",
      "epoch 6, batch 153, loss = 0.005590026266872883\n",
      "epoch 6, batch 154, loss = 0.054732292890548706\n",
      "epoch 6, batch 155, loss = 0.015052430331707\n",
      "epoch 6, batch 156, loss = 0.08632883429527283\n",
      "epoch 6, batch 157, loss = 0.03545774891972542\n",
      "epoch 6, batch 158, loss = 0.016904160380363464\n",
      "epoch 6, batch 159, loss = 0.009814940392971039\n",
      "epoch 6, batch 160, loss = 0.014876971021294594\n",
      "epoch 6, batch 161, loss = 0.005557705648243427\n",
      "epoch 6, batch 162, loss = 0.027109535411000252\n",
      "epoch 6, batch 163, loss = 0.012246421538293362\n",
      "epoch 6, batch 164, loss = 0.018783481791615486\n",
      "epoch 6, batch 165, loss = 0.02461564727127552\n",
      "epoch 6, batch 166, loss = 0.011694200336933136\n",
      "epoch 6, batch 167, loss = 0.010443150997161865\n",
      "epoch 6, batch 168, loss = 0.025608204305171967\n",
      "epoch 6, batch 169, loss = 0.02157013677060604\n",
      "epoch 6, batch 170, loss = 0.012851939536631107\n",
      "epoch 6, batch 171, loss = 0.011693041771650314\n",
      "epoch 6, batch 172, loss = 0.011960194446146488\n",
      "epoch 6, batch 173, loss = 0.006714012008160353\n",
      "epoch 6, batch 174, loss = 0.014139202423393726\n",
      "epoch 6, batch 175, loss = 0.00694319000467658\n",
      "epoch 6, batch 176, loss = 0.004004338290542364\n",
      "epoch 6, batch 177, loss = 0.026683427393436432\n",
      "epoch 6, batch 178, loss = 0.010422674007713795\n",
      "epoch 6, batch 179, loss = 0.014516396448016167\n",
      "epoch 6, batch 180, loss = 0.0438929945230484\n",
      "epoch 6, batch 181, loss = 0.0015687461709603667\n",
      "epoch 6, batch 182, loss = 0.004038263577967882\n",
      "epoch 6, batch 183, loss = 0.02531806379556656\n",
      "epoch 6, batch 184, loss = 0.02395503595471382\n",
      "epoch 6, batch 185, loss = 0.005788543727248907\n",
      "epoch 6, batch 186, loss = 0.01372986938804388\n",
      "epoch 6, batch 187, loss = 0.008082987740635872\n",
      "epoch 6, batch 188, loss = 0.009658933617174625\n",
      "epoch 6, batch 189, loss = 0.02554360404610634\n",
      "epoch 6, batch 190, loss = 0.016689568758010864\n",
      "epoch 6, batch 191, loss = 0.030241597443819046\n",
      "epoch 6, batch 192, loss = 0.05052239075303078\n",
      "epoch 6, batch 193, loss = 0.005848855245858431\n",
      "epoch 6, batch 194, loss = 0.010136858560144901\n",
      "epoch 6, batch 195, loss = 0.005189101677387953\n",
      "epoch 6, batch 196, loss = 0.011822926811873913\n",
      "epoch 6, batch 197, loss = 0.03402049094438553\n",
      "epoch 6, batch 198, loss = 0.007918898947536945\n",
      "epoch 6, batch 199, loss = 0.04978720843791962\n",
      "epoch 6, batch 200, loss = 0.00887157954275608\n",
      "epoch 7, batch 2, loss = 0.021810678765177727\n",
      "epoch 7, batch 3, loss = 0.0039160242304205894\n",
      "epoch 7, batch 4, loss = 0.007670735940337181\n",
      "epoch 7, batch 5, loss = 0.031928934156894684\n",
      "epoch 7, batch 6, loss = 0.021516546607017517\n",
      "epoch 7, batch 7, loss = 0.00912860780954361\n",
      "epoch 7, batch 8, loss = 0.0022640512324869633\n",
      "epoch 7, batch 9, loss = 0.031801171600818634\n",
      "epoch 7, batch 10, loss = 0.02539517916738987\n",
      "epoch 7, batch 11, loss = 0.0067825280129909515\n",
      "epoch 7, batch 12, loss = 0.015527401119470596\n",
      "epoch 7, batch 13, loss = 0.005515469703823328\n",
      "epoch 7, batch 14, loss = 0.03615407645702362\n",
      "epoch 7, batch 15, loss = 0.02035294473171234\n",
      "epoch 7, batch 16, loss = 0.02284151315689087\n",
      "epoch 7, batch 17, loss = 0.0018293260363861918\n",
      "epoch 7, batch 18, loss = 0.020099811255931854\n",
      "epoch 7, batch 19, loss = 0.005814471282064915\n",
      "epoch 7, batch 20, loss = 0.012959131971001625\n",
      "epoch 7, batch 21, loss = 0.004324179142713547\n",
      "epoch 7, batch 22, loss = 0.011804040521383286\n",
      "epoch 7, batch 23, loss = 0.00543901976197958\n",
      "epoch 7, batch 24, loss = 0.011653360910713673\n",
      "epoch 7, batch 25, loss = 0.008249275386333466\n",
      "epoch 7, batch 26, loss = 0.012914158403873444\n",
      "epoch 7, batch 27, loss = 0.005685735959559679\n",
      "epoch 7, batch 28, loss = 0.005133164115250111\n",
      "epoch 7, batch 29, loss = 0.0174797885119915\n",
      "epoch 7, batch 30, loss = 0.006468328181654215\n",
      "epoch 7, batch 31, loss = 0.004118971526622772\n",
      "epoch 7, batch 32, loss = 0.01108277402818203\n",
      "epoch 7, batch 33, loss = 0.010892561636865139\n",
      "epoch 7, batch 34, loss = 0.013187405653297901\n",
      "epoch 7, batch 35, loss = 0.010203789919614792\n",
      "epoch 7, batch 36, loss = 0.018385279923677444\n",
      "epoch 7, batch 37, loss = 0.0023362988140434027\n",
      "epoch 7, batch 38, loss = 0.01894119754433632\n",
      "epoch 7, batch 39, loss = 0.030618702992796898\n",
      "epoch 7, batch 40, loss = 0.011892287991940975\n",
      "epoch 7, batch 41, loss = 0.033367596566677094\n",
      "epoch 7, batch 42, loss = 0.016712123528122902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, batch 43, loss = 0.0265553817152977\n",
      "epoch 7, batch 44, loss = 0.01059024129062891\n",
      "epoch 7, batch 45, loss = 0.0017206113552674651\n",
      "epoch 7, batch 46, loss = 0.02546425350010395\n",
      "epoch 7, batch 47, loss = 0.002597993006929755\n",
      "epoch 7, batch 48, loss = 0.013192256912589073\n",
      "epoch 7, batch 49, loss = 0.010339338332414627\n",
      "epoch 7, batch 50, loss = 0.03258318081498146\n",
      "epoch 7, batch 51, loss = 0.04421889781951904\n",
      "epoch 7, batch 52, loss = 0.010191362351179123\n",
      "epoch 7, batch 53, loss = 0.0016787232598289847\n",
      "epoch 7, batch 54, loss = 0.009004469029605389\n",
      "epoch 7, batch 55, loss = 0.015418881550431252\n",
      "epoch 7, batch 56, loss = 0.018049711361527443\n",
      "epoch 7, batch 57, loss = 0.004246457479894161\n",
      "epoch 7, batch 58, loss = 0.0032088204752653837\n",
      "epoch 7, batch 59, loss = 0.015376989729702473\n",
      "epoch 7, batch 60, loss = 0.00872595515102148\n",
      "epoch 7, batch 61, loss = 0.042110294103622437\n",
      "epoch 7, batch 62, loss = 0.02235977165400982\n",
      "epoch 7, batch 63, loss = 0.004302794113755226\n",
      "epoch 7, batch 64, loss = 0.020100079476833344\n",
      "epoch 7, batch 65, loss = 0.03159845992922783\n",
      "epoch 7, batch 66, loss = 0.00947464071214199\n",
      "epoch 7, batch 67, loss = 0.01610228791832924\n",
      "epoch 7, batch 68, loss = 0.02902272716164589\n",
      "epoch 7, batch 69, loss = 0.005496252793818712\n",
      "epoch 7, batch 70, loss = 0.017633788287639618\n",
      "epoch 7, batch 71, loss = 0.04556841403245926\n",
      "epoch 7, batch 72, loss = 0.06615602970123291\n",
      "epoch 7, batch 73, loss = 0.0048703704960644245\n",
      "epoch 7, batch 74, loss = 0.014838711358606815\n",
      "epoch 7, batch 75, loss = 0.006723742466419935\n",
      "epoch 7, batch 76, loss = 0.004063768312335014\n",
      "epoch 7, batch 77, loss = 0.016083957627415657\n",
      "epoch 7, batch 78, loss = 0.0030552607495337725\n",
      "epoch 7, batch 79, loss = 0.007851755246520042\n",
      "epoch 7, batch 80, loss = 0.018972869962453842\n",
      "epoch 7, batch 81, loss = 0.015891872346401215\n",
      "epoch 7, batch 82, loss = 0.014318755827844143\n",
      "epoch 7, batch 83, loss = 0.020201561972498894\n",
      "epoch 7, batch 84, loss = 0.034700583666563034\n",
      "epoch 7, batch 85, loss = 0.007032160181552172\n",
      "epoch 7, batch 86, loss = 0.014581091701984406\n",
      "epoch 7, batch 87, loss = 0.0038916566409170628\n",
      "epoch 7, batch 88, loss = 0.008238442242145538\n",
      "epoch 7, batch 89, loss = 0.06426747143268585\n",
      "epoch 7, batch 90, loss = 0.012113699689507484\n",
      "epoch 7, batch 91, loss = 0.054802216589450836\n",
      "epoch 7, batch 92, loss = 0.03930347040295601\n",
      "epoch 7, batch 93, loss = 0.015954218804836273\n",
      "epoch 7, batch 94, loss = 0.01284614484757185\n",
      "epoch 7, batch 95, loss = 0.04134586825966835\n",
      "epoch 7, batch 96, loss = 0.034334696829319\n",
      "epoch 7, batch 97, loss = 0.00907046440988779\n",
      "epoch 7, batch 98, loss = 0.011639841832220554\n",
      "epoch 7, batch 99, loss = 0.009107115678489208\n",
      "epoch 7, batch 100, loss = 0.004685983993113041\n",
      "epoch 7, batch 101, loss = 0.007397532928735018\n",
      "epoch 7, batch 102, loss = 0.06730400770902634\n",
      "epoch 7, batch 103, loss = 0.004222129471600056\n",
      "epoch 7, batch 104, loss = 0.021775567904114723\n",
      "epoch 7, batch 105, loss = 0.006514759734272957\n",
      "epoch 7, batch 106, loss = 0.006655258126556873\n",
      "epoch 7, batch 107, loss = 0.006442760117352009\n",
      "epoch 7, batch 108, loss = 0.015061050653457642\n",
      "epoch 7, batch 109, loss = 0.0420934297144413\n",
      "epoch 7, batch 110, loss = 0.04878810793161392\n",
      "epoch 7, batch 111, loss = 0.06338416039943695\n",
      "epoch 7, batch 112, loss = 0.04395802319049835\n",
      "epoch 7, batch 113, loss = 0.006637598853558302\n",
      "epoch 7, batch 114, loss = 0.049878835678100586\n",
      "epoch 7, batch 115, loss = 0.01106630451977253\n",
      "epoch 7, batch 116, loss = 0.0134708471596241\n",
      "epoch 7, batch 117, loss = 0.04174014553427696\n",
      "epoch 7, batch 118, loss = 0.01597466506063938\n",
      "epoch 7, batch 119, loss = 0.008679390884935856\n",
      "epoch 7, batch 120, loss = 0.010762488469481468\n",
      "epoch 7, batch 121, loss = 0.03273088485002518\n",
      "epoch 7, batch 122, loss = 0.03250905126333237\n",
      "epoch 7, batch 123, loss = 0.010027631185948849\n",
      "epoch 7, batch 124, loss = 0.03154939040541649\n",
      "epoch 7, batch 125, loss = 0.01501535065472126\n",
      "epoch 7, batch 126, loss = 0.01105308998376131\n",
      "epoch 7, batch 127, loss = 0.021629225462675095\n",
      "epoch 7, batch 128, loss = 0.045704178512096405\n",
      "epoch 7, batch 129, loss = 0.008061024360358715\n",
      "epoch 7, batch 130, loss = 0.014494973234832287\n",
      "epoch 7, batch 131, loss = 0.023712236434221268\n",
      "epoch 7, batch 132, loss = 0.01844055950641632\n",
      "epoch 7, batch 133, loss = 0.023228369653224945\n",
      "epoch 7, batch 134, loss = 0.007118587382137775\n",
      "epoch 7, batch 135, loss = 0.011191833764314651\n",
      "epoch 7, batch 136, loss = 0.02972746640443802\n",
      "epoch 7, batch 137, loss = 0.0035217038821429014\n",
      "epoch 7, batch 138, loss = 0.005456332117319107\n",
      "epoch 7, batch 139, loss = 0.018015997484326363\n",
      "epoch 7, batch 140, loss = 0.037780944257974625\n",
      "epoch 7, batch 141, loss = 0.007336332928389311\n",
      "epoch 7, batch 142, loss = 0.002565589500591159\n",
      "epoch 7, batch 143, loss = 0.009417984634637833\n",
      "epoch 7, batch 144, loss = 0.03998885676264763\n",
      "epoch 7, batch 145, loss = 0.006783097516745329\n",
      "epoch 7, batch 146, loss = 0.012305841781198978\n",
      "epoch 7, batch 147, loss = 0.013639770448207855\n",
      "epoch 7, batch 148, loss = 0.020802469924092293\n",
      "epoch 7, batch 149, loss = 0.005135216750204563\n",
      "epoch 7, batch 150, loss = 0.005210154689848423\n",
      "epoch 7, batch 151, loss = 0.027167828753590584\n",
      "epoch 7, batch 152, loss = 0.07326995581388474\n",
      "epoch 7, batch 153, loss = 0.003968689125031233\n",
      "epoch 7, batch 154, loss = 0.02005089819431305\n",
      "epoch 7, batch 155, loss = 0.010057375766336918\n",
      "epoch 7, batch 156, loss = 0.0045898170210421085\n",
      "epoch 7, batch 157, loss = 0.042402252554893494\n",
      "epoch 7, batch 158, loss = 0.005907539743930101\n",
      "epoch 7, batch 159, loss = 0.003498937003314495\n",
      "epoch 7, batch 160, loss = 0.03701092302799225\n",
      "epoch 7, batch 161, loss = 0.007736474275588989\n",
      "epoch 7, batch 162, loss = 0.0038915053009986877\n",
      "epoch 7, batch 163, loss = 0.013440387323498726\n",
      "epoch 7, batch 164, loss = 0.00178959418553859\n",
      "epoch 7, batch 165, loss = 0.012144763022661209\n",
      "epoch 7, batch 166, loss = 0.0021655710879713297\n",
      "epoch 7, batch 167, loss = 0.014667188748717308\n",
      "epoch 7, batch 168, loss = 0.02311798557639122\n",
      "epoch 7, batch 169, loss = 0.027106652036309242\n",
      "epoch 7, batch 170, loss = 0.010361915454268456\n",
      "epoch 7, batch 171, loss = 0.0067335693165659904\n",
      "epoch 7, batch 172, loss = 0.036887913942337036\n",
      "epoch 7, batch 173, loss = 0.03089863993227482\n",
      "epoch 7, batch 174, loss = 0.008312598802149296\n",
      "epoch 7, batch 175, loss = 0.003686032025143504\n",
      "epoch 7, batch 176, loss = 0.031008560210466385\n",
      "epoch 7, batch 177, loss = 0.005801564082503319\n",
      "epoch 7, batch 178, loss = 0.01049872301518917\n",
      "epoch 7, batch 179, loss = 0.006122654303908348\n",
      "epoch 7, batch 180, loss = 0.02127734199166298\n",
      "epoch 7, batch 181, loss = 0.017898809164762497\n",
      "epoch 7, batch 182, loss = 0.037997208535671234\n",
      "epoch 7, batch 183, loss = 0.01351146213710308\n",
      "epoch 7, batch 184, loss = 0.01852334663271904\n",
      "epoch 7, batch 185, loss = 0.016216399148106575\n",
      "epoch 7, batch 186, loss = 0.005113893188536167\n",
      "epoch 7, batch 187, loss = 0.00974961742758751\n",
      "epoch 7, batch 188, loss = 0.01748867705464363\n",
      "epoch 7, batch 189, loss = 0.025868285447359085\n",
      "epoch 7, batch 190, loss = 0.008328855969011784\n",
      "epoch 7, batch 191, loss = 0.03242550790309906\n",
      "epoch 7, batch 192, loss = 0.008714942261576653\n",
      "epoch 7, batch 193, loss = 0.03953089192509651\n",
      "epoch 7, batch 194, loss = 0.013438296504318714\n",
      "epoch 7, batch 195, loss = 0.008388865739107132\n",
      "epoch 7, batch 196, loss = 0.01646246388554573\n",
      "epoch 7, batch 197, loss = 0.006542955059558153\n",
      "epoch 7, batch 198, loss = 0.025696640834212303\n",
      "epoch 7, batch 199, loss = 0.011682066135108471\n",
      "epoch 7, batch 200, loss = 0.010715527459979057\n",
      "epoch 8, batch 2, loss = 0.006006499752402306\n",
      "epoch 8, batch 3, loss = 0.010749050416052341\n",
      "epoch 8, batch 4, loss = 0.0014941557310521603\n",
      "epoch 8, batch 5, loss = 0.008268360048532486\n",
      "epoch 8, batch 6, loss = 0.0014668632065877318\n",
      "epoch 8, batch 7, loss = 0.002173040760681033\n",
      "epoch 8, batch 8, loss = 0.011914857663214207\n",
      "epoch 8, batch 9, loss = 0.009695755317807198\n",
      "epoch 8, batch 10, loss = 0.01895257644355297\n",
      "epoch 8, batch 11, loss = 0.026811951771378517\n",
      "epoch 8, batch 12, loss = 0.008554937317967415\n",
      "epoch 8, batch 13, loss = 0.0019115193281322718\n",
      "epoch 8, batch 14, loss = 0.016400974243879318\n",
      "epoch 8, batch 15, loss = 0.016124095767736435\n",
      "epoch 8, batch 16, loss = 0.014640984125435352\n",
      "epoch 8, batch 17, loss = 0.002401782665401697\n",
      "epoch 8, batch 18, loss = 0.009288564324378967\n",
      "epoch 8, batch 19, loss = 0.002962894970551133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, batch 20, loss = 0.028575150296092033\n",
      "epoch 8, batch 21, loss = 0.010350427590310574\n",
      "epoch 8, batch 22, loss = 0.007472672965377569\n",
      "epoch 8, batch 23, loss = 0.0234966017305851\n",
      "epoch 8, batch 24, loss = 0.014474833384156227\n",
      "epoch 8, batch 25, loss = 0.0018480659928172827\n",
      "epoch 8, batch 26, loss = 0.006252857390791178\n",
      "epoch 8, batch 27, loss = 0.016375474631786346\n",
      "epoch 8, batch 28, loss = 0.004150782711803913\n",
      "epoch 8, batch 29, loss = 0.0031537122558802366\n",
      "epoch 8, batch 30, loss = 0.006160053424537182\n",
      "epoch 8, batch 31, loss = 0.007346369791775942\n",
      "epoch 8, batch 32, loss = 0.0026204362511634827\n",
      "epoch 8, batch 33, loss = 0.00144498934969306\n",
      "epoch 8, batch 34, loss = 0.0060773612931370735\n",
      "epoch 8, batch 35, loss = 0.03408611938357353\n",
      "epoch 8, batch 36, loss = 0.011517947539687157\n",
      "epoch 8, batch 37, loss = 0.009746527299284935\n",
      "epoch 8, batch 38, loss = 0.0025839803274720907\n",
      "epoch 8, batch 39, loss = 0.0077044423669576645\n",
      "epoch 8, batch 40, loss = 0.0030757307540625334\n",
      "epoch 8, batch 41, loss = 0.002580342348664999\n",
      "epoch 8, batch 42, loss = 0.008233372122049332\n",
      "epoch 8, batch 43, loss = 0.0023808875121176243\n",
      "epoch 8, batch 44, loss = 0.007163635455071926\n",
      "epoch 8, batch 45, loss = 0.0019214912317693233\n",
      "epoch 8, batch 46, loss = 0.006654927507042885\n",
      "epoch 8, batch 47, loss = 0.025516638532280922\n",
      "epoch 8, batch 48, loss = 0.014943660236895084\n",
      "epoch 8, batch 49, loss = 0.0015723556280136108\n",
      "epoch 8, batch 50, loss = 0.0025261053815484047\n",
      "epoch 8, batch 51, loss = 0.0016568030696362257\n",
      "epoch 8, batch 52, loss = 0.012026399374008179\n",
      "epoch 8, batch 53, loss = 0.001992215868085623\n",
      "epoch 8, batch 54, loss = 0.00611630966886878\n",
      "epoch 8, batch 55, loss = 0.010697107762098312\n",
      "epoch 8, batch 56, loss = 0.008359160274267197\n",
      "epoch 8, batch 57, loss = 0.0017679983284324408\n",
      "epoch 8, batch 58, loss = 0.0020771354902535677\n",
      "epoch 8, batch 59, loss = 0.015333345159888268\n",
      "epoch 8, batch 60, loss = 0.005083970259875059\n",
      "epoch 8, batch 61, loss = 0.009609036147594452\n",
      "epoch 8, batch 62, loss = 0.014891628175973892\n",
      "epoch 8, batch 63, loss = 0.004847225733101368\n",
      "epoch 8, batch 64, loss = 0.007326400838792324\n",
      "epoch 8, batch 65, loss = 0.0012896412517875433\n",
      "epoch 8, batch 66, loss = 0.02368284948170185\n",
      "epoch 8, batch 67, loss = 0.0069379438646137714\n",
      "epoch 8, batch 68, loss = 0.011164345778524876\n",
      "epoch 8, batch 69, loss = 0.012696074321866035\n",
      "epoch 8, batch 70, loss = 0.0024508219212293625\n",
      "epoch 8, batch 71, loss = 0.009845349937677383\n",
      "epoch 8, batch 72, loss = 0.09163536876440048\n",
      "epoch 8, batch 73, loss = 0.043229829519987106\n",
      "epoch 8, batch 74, loss = 0.0016965490067377687\n",
      "epoch 8, batch 75, loss = 0.008515076711773872\n",
      "epoch 8, batch 76, loss = 0.013212391175329685\n",
      "epoch 8, batch 77, loss = 0.005033328663557768\n",
      "epoch 8, batch 78, loss = 0.012628903612494469\n",
      "epoch 8, batch 79, loss = 0.004741996061056852\n",
      "epoch 8, batch 80, loss = 0.016366388648748398\n",
      "epoch 8, batch 81, loss = 0.06336984038352966\n",
      "epoch 8, batch 82, loss = 0.02145283855497837\n",
      "epoch 8, batch 83, loss = 0.04070483520627022\n",
      "epoch 8, batch 84, loss = 0.0031967496033757925\n",
      "epoch 8, batch 85, loss = 0.008871529251337051\n",
      "epoch 8, batch 86, loss = 0.03471650183200836\n",
      "epoch 8, batch 87, loss = 0.002121946308761835\n",
      "epoch 8, batch 88, loss = 0.023492762818932533\n",
      "epoch 8, batch 89, loss = 0.008056201972067356\n",
      "epoch 8, batch 90, loss = 0.0031116243917495012\n",
      "epoch 8, batch 91, loss = 0.03658756613731384\n",
      "epoch 8, batch 92, loss = 0.03979949280619621\n",
      "epoch 8, batch 93, loss = 0.013496995903551579\n",
      "epoch 8, batch 94, loss = 0.025858784094452858\n",
      "epoch 8, batch 95, loss = 0.027756579220294952\n",
      "epoch 8, batch 96, loss = 0.01197480596601963\n",
      "epoch 8, batch 97, loss = 0.017562100663781166\n",
      "epoch 8, batch 98, loss = 0.03521127253770828\n",
      "epoch 8, batch 99, loss = 0.012936370447278023\n",
      "epoch 8, batch 100, loss = 0.03264923393726349\n",
      "epoch 8, batch 101, loss = 0.014001499861478806\n",
      "epoch 8, batch 102, loss = 0.008425186388194561\n",
      "epoch 8, batch 103, loss = 0.004071240779012442\n",
      "epoch 8, batch 104, loss = 0.011293516494333744\n",
      "epoch 8, batch 105, loss = 0.01719815656542778\n",
      "epoch 8, batch 106, loss = 0.010154050774872303\n",
      "epoch 8, batch 107, loss = 0.01757112331688404\n",
      "epoch 8, batch 108, loss = 0.0025400472804903984\n",
      "epoch 8, batch 109, loss = 0.012428177520632744\n",
      "epoch 8, batch 110, loss = 0.01902676746249199\n",
      "epoch 8, batch 111, loss = 0.01268135104328394\n",
      "epoch 8, batch 112, loss = 0.028146836906671524\n",
      "epoch 8, batch 113, loss = 0.007943701930344105\n",
      "epoch 8, batch 114, loss = 0.014160825870931149\n",
      "epoch 8, batch 115, loss = 0.03078184276819229\n",
      "epoch 8, batch 116, loss = 0.009413454681634903\n",
      "epoch 8, batch 117, loss = 0.005727472715079784\n",
      "epoch 8, batch 118, loss = 0.0009939498268067837\n",
      "epoch 8, batch 119, loss = 0.002347736619412899\n",
      "epoch 8, batch 120, loss = 0.011544303968548775\n",
      "epoch 8, batch 121, loss = 0.00462533300742507\n",
      "epoch 8, batch 122, loss = 0.03793545812368393\n",
      "epoch 8, batch 123, loss = 0.012851581908762455\n",
      "epoch 8, batch 124, loss = 0.010355464182794094\n",
      "epoch 8, batch 125, loss = 0.000939279212616384\n",
      "epoch 8, batch 126, loss = 0.01024694461375475\n",
      "epoch 8, batch 127, loss = 0.04664263129234314\n",
      "epoch 8, batch 128, loss = 0.0028750840574502945\n",
      "epoch 8, batch 129, loss = 0.012738795019686222\n",
      "epoch 8, batch 130, loss = 0.016436127945780754\n",
      "epoch 8, batch 131, loss = 0.036215994507074356\n",
      "epoch 8, batch 132, loss = 0.004448289982974529\n",
      "epoch 8, batch 133, loss = 0.011552015319466591\n",
      "epoch 8, batch 134, loss = 0.015901507809758186\n",
      "epoch 8, batch 135, loss = 0.010254102759063244\n",
      "epoch 8, batch 136, loss = 0.022839194163680077\n",
      "epoch 8, batch 137, loss = 0.07242289185523987\n",
      "epoch 8, batch 138, loss = 0.012037741020321846\n",
      "epoch 8, batch 139, loss = 0.0008317923056893051\n",
      "epoch 8, batch 140, loss = 0.024223649874329567\n",
      "epoch 8, batch 141, loss = 0.01156088151037693\n",
      "epoch 8, batch 142, loss = 0.005314918234944344\n",
      "epoch 8, batch 143, loss = 0.004636536352336407\n",
      "epoch 8, batch 144, loss = 0.012812844477593899\n",
      "epoch 8, batch 145, loss = 0.010983130894601345\n",
      "epoch 8, batch 146, loss = 0.009795061312615871\n",
      "epoch 8, batch 147, loss = 0.003481566207483411\n",
      "epoch 8, batch 148, loss = 0.0015652303118258715\n",
      "epoch 8, batch 149, loss = 0.00484552513808012\n",
      "epoch 8, batch 150, loss = 0.0019581266678869724\n",
      "epoch 8, batch 151, loss = 0.0023272938560694456\n",
      "epoch 8, batch 152, loss = 0.007543389685451984\n",
      "epoch 8, batch 153, loss = 0.008256570436060429\n",
      "epoch 8, batch 154, loss = 0.003697938285768032\n",
      "epoch 8, batch 155, loss = 0.02105196937918663\n",
      "epoch 8, batch 156, loss = 0.003985743038356304\n",
      "epoch 8, batch 157, loss = 0.004811966326087713\n",
      "epoch 8, batch 158, loss = 0.012151110917329788\n",
      "epoch 8, batch 159, loss = 0.0021757299546152353\n",
      "epoch 8, batch 160, loss = 0.0008472449262626469\n",
      "epoch 8, batch 161, loss = 0.02440118044614792\n",
      "epoch 8, batch 162, loss = 0.02059829980134964\n",
      "epoch 8, batch 163, loss = 0.014393308199942112\n",
      "epoch 8, batch 164, loss = 0.006839475128799677\n",
      "epoch 8, batch 165, loss = 0.005272200331091881\n",
      "epoch 8, batch 166, loss = 0.00917782261967659\n",
      "epoch 8, batch 167, loss = 0.0023420602083206177\n",
      "epoch 8, batch 168, loss = 0.01494584884494543\n",
      "epoch 8, batch 169, loss = 0.004893286153674126\n",
      "epoch 8, batch 170, loss = 0.008040078915655613\n",
      "epoch 8, batch 171, loss = 0.013398115523159504\n",
      "epoch 8, batch 172, loss = 0.00447160704061389\n",
      "epoch 8, batch 173, loss = 0.007084845099598169\n",
      "epoch 8, batch 174, loss = 0.01138048805296421\n",
      "epoch 8, batch 175, loss = 0.0021790736354887486\n",
      "epoch 8, batch 176, loss = 0.04578292369842529\n",
      "epoch 8, batch 177, loss = 0.027753181755542755\n",
      "epoch 8, batch 178, loss = 0.012784898281097412\n",
      "epoch 8, batch 179, loss = 0.015693722292780876\n",
      "epoch 8, batch 180, loss = 0.03627436235547066\n",
      "epoch 8, batch 181, loss = 0.02276451885700226\n",
      "epoch 8, batch 182, loss = 0.004239980597048998\n",
      "epoch 8, batch 183, loss = 0.0057801841758191586\n",
      "epoch 8, batch 184, loss = 0.064234659075737\n",
      "epoch 8, batch 185, loss = 0.006313225254416466\n",
      "epoch 8, batch 186, loss = 0.01619826816022396\n",
      "epoch 8, batch 187, loss = 0.004434955306351185\n",
      "epoch 8, batch 188, loss = 0.013129695318639278\n",
      "epoch 8, batch 189, loss = 0.014885897748172283\n",
      "epoch 8, batch 190, loss = 0.015282860957086086\n",
      "epoch 8, batch 191, loss = 0.011833621188998222\n",
      "epoch 8, batch 192, loss = 0.0035835551097989082\n",
      "epoch 8, batch 193, loss = 0.0046488018706440926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, batch 194, loss = 0.003614898072555661\n",
      "epoch 8, batch 195, loss = 0.0035480016376823187\n",
      "epoch 8, batch 196, loss = 0.03224550560116768\n",
      "epoch 8, batch 197, loss = 0.006944263353943825\n",
      "epoch 8, batch 198, loss = 0.036759186536073685\n",
      "epoch 8, batch 199, loss = 0.0027522689197212458\n",
      "epoch 8, batch 200, loss = 0.0248302910476923\n",
      "epoch 9, batch 2, loss = 0.006790688261389732\n",
      "epoch 9, batch 3, loss = 0.010500107891857624\n",
      "epoch 9, batch 4, loss = 0.004946013912558556\n",
      "epoch 9, batch 5, loss = 0.01857493817806244\n",
      "epoch 9, batch 6, loss = 0.006327102892100811\n",
      "epoch 9, batch 7, loss = 0.012519380077719688\n",
      "epoch 9, batch 8, loss = 0.008191385306417942\n",
      "epoch 9, batch 9, loss = 0.01317131519317627\n",
      "epoch 9, batch 10, loss = 0.007653334178030491\n",
      "epoch 9, batch 11, loss = 0.010392704978585243\n",
      "epoch 9, batch 12, loss = 0.002303797984495759\n",
      "epoch 9, batch 13, loss = 0.0038939074147492647\n",
      "epoch 9, batch 14, loss = 0.007943462580442429\n",
      "epoch 9, batch 15, loss = 0.0638723373413086\n",
      "epoch 9, batch 16, loss = 0.014071429148316383\n",
      "epoch 9, batch 17, loss = 0.012873576022684574\n",
      "epoch 9, batch 18, loss = 0.0030108019709587097\n",
      "epoch 9, batch 19, loss = 0.00785849615931511\n",
      "epoch 9, batch 20, loss = 0.02043154463171959\n",
      "epoch 9, batch 21, loss = 0.04139649495482445\n",
      "epoch 9, batch 22, loss = 0.009580177254974842\n",
      "epoch 9, batch 23, loss = 0.003139575943350792\n",
      "epoch 9, batch 24, loss = 0.005913421977311373\n",
      "epoch 9, batch 25, loss = 0.02288205362856388\n",
      "epoch 9, batch 26, loss = 0.01571858860552311\n",
      "epoch 9, batch 27, loss = 0.003811984322965145\n",
      "epoch 9, batch 28, loss = 0.027629323303699493\n",
      "epoch 9, batch 29, loss = 0.007774223107844591\n",
      "epoch 9, batch 30, loss = 0.006250538397580385\n",
      "epoch 9, batch 31, loss = 0.0173958633095026\n",
      "epoch 9, batch 32, loss = 0.02449011616408825\n",
      "epoch 9, batch 33, loss = 0.0060648005455732346\n",
      "epoch 9, batch 34, loss = 0.013697178103029728\n",
      "epoch 9, batch 35, loss = 0.008855409920215607\n",
      "epoch 9, batch 36, loss = 0.004523477982729673\n",
      "epoch 9, batch 37, loss = 0.007143085822463036\n",
      "epoch 9, batch 38, loss = 0.0048415036872029305\n",
      "epoch 9, batch 39, loss = 0.012587228789925575\n",
      "epoch 9, batch 40, loss = 0.0023380706552416086\n",
      "epoch 9, batch 41, loss = 0.0212958175688982\n",
      "epoch 9, batch 42, loss = 0.017398342490196228\n",
      "epoch 9, batch 43, loss = 0.0016977235209196806\n",
      "epoch 9, batch 44, loss = 0.005849666427820921\n",
      "epoch 9, batch 45, loss = 0.02234150655567646\n",
      "epoch 9, batch 46, loss = 0.0072286659851670265\n",
      "epoch 9, batch 47, loss = 0.005896783899515867\n",
      "epoch 9, batch 48, loss = 0.009232599288225174\n",
      "epoch 9, batch 49, loss = 0.00944977905601263\n",
      "epoch 9, batch 50, loss = 0.009752376936376095\n",
      "epoch 9, batch 51, loss = 0.010336012579500675\n",
      "epoch 9, batch 52, loss = 0.0029107844457030296\n",
      "epoch 9, batch 53, loss = 0.002340899780392647\n",
      "epoch 9, batch 54, loss = 0.014822056517004967\n",
      "epoch 9, batch 55, loss = 0.012700667604804039\n",
      "epoch 9, batch 56, loss = 0.0024164903443306684\n",
      "epoch 9, batch 57, loss = 0.006666846107691526\n",
      "epoch 9, batch 58, loss = 0.0103455176576972\n",
      "epoch 9, batch 59, loss = 0.012132969684898853\n",
      "epoch 9, batch 60, loss = 0.0065939719788730145\n",
      "epoch 9, batch 61, loss = 0.007005611434578896\n",
      "epoch 9, batch 62, loss = 0.00535484217107296\n",
      "epoch 9, batch 63, loss = 0.005336543545126915\n",
      "epoch 9, batch 64, loss = 0.005315945018082857\n",
      "epoch 9, batch 65, loss = 0.011270280927419662\n",
      "epoch 9, batch 66, loss = 0.006652690004557371\n",
      "epoch 9, batch 67, loss = 0.002578148152679205\n",
      "epoch 9, batch 68, loss = 0.018274256959557533\n",
      "epoch 9, batch 69, loss = 0.0026578339748084545\n",
      "epoch 9, batch 70, loss = 0.004446485545486212\n",
      "epoch 9, batch 71, loss = 0.02316918410360813\n",
      "epoch 9, batch 72, loss = 0.0022986670956015587\n",
      "epoch 9, batch 73, loss = 0.003919225186109543\n",
      "epoch 9, batch 74, loss = 0.0017453300533816218\n",
      "epoch 9, batch 75, loss = 0.0010191341862082481\n",
      "epoch 9, batch 76, loss = 0.004990673158317804\n",
      "epoch 9, batch 77, loss = 0.002838529646396637\n",
      "epoch 9, batch 78, loss = 0.000631874252576381\n",
      "epoch 9, batch 79, loss = 0.018711267039179802\n",
      "epoch 9, batch 80, loss = 0.010660222731530666\n",
      "epoch 9, batch 81, loss = 0.008143377490341663\n",
      "epoch 9, batch 82, loss = 0.016667282208800316\n",
      "epoch 9, batch 83, loss = 0.0006892402307130396\n",
      "epoch 9, batch 84, loss = 0.010159008204936981\n",
      "epoch 9, batch 85, loss = 0.0003458515275269747\n",
      "epoch 9, batch 86, loss = 0.0015190738486126065\n",
      "epoch 9, batch 87, loss = 0.009578041732311249\n",
      "epoch 9, batch 88, loss = 0.01302604004740715\n",
      "epoch 9, batch 89, loss = 0.0047233011573553085\n",
      "epoch 9, batch 90, loss = 0.002624008571729064\n",
      "epoch 9, batch 91, loss = 0.004389019683003426\n",
      "epoch 9, batch 92, loss = 0.020108943805098534\n",
      "epoch 9, batch 93, loss = 0.00626652967184782\n",
      "epoch 9, batch 94, loss = 0.002849714132025838\n",
      "epoch 9, batch 95, loss = 0.0014794270973652601\n",
      "epoch 9, batch 96, loss = 0.030609164386987686\n",
      "epoch 9, batch 97, loss = 0.0031841532327234745\n",
      "epoch 9, batch 98, loss = 0.0029171204660087824\n",
      "epoch 9, batch 99, loss = 0.0005251357797533274\n",
      "epoch 9, batch 100, loss = 0.011811336502432823\n",
      "epoch 9, batch 101, loss = 0.005077528767287731\n",
      "epoch 9, batch 102, loss = 0.0004115469055250287\n",
      "epoch 9, batch 103, loss = 0.028917040675878525\n",
      "epoch 9, batch 104, loss = 0.019509978592395782\n",
      "epoch 9, batch 105, loss = 0.0014251683605834842\n",
      "epoch 9, batch 106, loss = 0.005838530138134956\n",
      "epoch 9, batch 107, loss = 0.004504712298512459\n",
      "epoch 9, batch 108, loss = 0.0013490505516529083\n",
      "epoch 9, batch 109, loss = 0.00603257492184639\n",
      "epoch 9, batch 110, loss = 0.0433821901679039\n",
      "epoch 9, batch 111, loss = 0.003927702084183693\n",
      "epoch 9, batch 112, loss = 0.028018558397889137\n",
      "epoch 9, batch 113, loss = 0.011755011975765228\n",
      "epoch 9, batch 114, loss = 0.06291861087083817\n",
      "epoch 9, batch 115, loss = 0.0022693448700010777\n",
      "epoch 9, batch 116, loss = 0.0035177739337086678\n",
      "epoch 9, batch 117, loss = 0.0012997399317100644\n",
      "epoch 9, batch 118, loss = 0.012910613790154457\n",
      "epoch 9, batch 119, loss = 0.018321359530091286\n",
      "epoch 9, batch 120, loss = 0.020236477255821228\n",
      "epoch 9, batch 121, loss = 0.0007388923550024629\n",
      "epoch 9, batch 122, loss = 0.00425566453486681\n",
      "epoch 9, batch 123, loss = 0.006013466510921717\n",
      "epoch 9, batch 124, loss = 0.00735910190269351\n",
      "epoch 9, batch 125, loss = 0.013750676065683365\n",
      "epoch 9, batch 126, loss = 0.004920448176562786\n",
      "epoch 9, batch 127, loss = 0.005655812099575996\n",
      "epoch 9, batch 128, loss = 0.013275143690407276\n",
      "epoch 9, batch 129, loss = 0.005884658545255661\n",
      "epoch 9, batch 130, loss = 0.004218036308884621\n",
      "epoch 9, batch 131, loss = 0.001913145650178194\n",
      "epoch 9, batch 132, loss = 0.004753364250063896\n",
      "epoch 9, batch 133, loss = 0.002917458303272724\n",
      "epoch 9, batch 134, loss = 0.0037863266188651323\n",
      "epoch 9, batch 135, loss = 0.0022624803241342306\n",
      "epoch 9, batch 136, loss = 0.005219974555075169\n",
      "epoch 9, batch 137, loss = 0.0029073397163301706\n",
      "epoch 9, batch 138, loss = 0.0047924877144396305\n",
      "epoch 9, batch 139, loss = 0.0047385613434016705\n",
      "epoch 9, batch 140, loss = 0.00385559955611825\n",
      "epoch 9, batch 141, loss = 0.0022916309535503387\n",
      "epoch 9, batch 142, loss = 0.008427384309470654\n",
      "epoch 9, batch 143, loss = 0.015277599915862083\n",
      "epoch 9, batch 144, loss = 0.003657605266198516\n",
      "epoch 9, batch 145, loss = 0.0002786655677482486\n",
      "epoch 9, batch 146, loss = 0.007224180269986391\n",
      "epoch 9, batch 147, loss = 0.004212483298033476\n",
      "epoch 9, batch 148, loss = 0.0009491834207437932\n",
      "epoch 9, batch 149, loss = 0.009508783929049969\n",
      "epoch 9, batch 150, loss = 0.0017481434624642134\n",
      "epoch 9, batch 151, loss = 0.000852709636092186\n",
      "epoch 9, batch 152, loss = 0.012460149824619293\n",
      "epoch 9, batch 153, loss = 0.008569072932004929\n",
      "epoch 9, batch 154, loss = 0.014181563630700111\n",
      "epoch 9, batch 155, loss = 0.004307364579290152\n",
      "epoch 9, batch 156, loss = 0.013010736554861069\n",
      "epoch 9, batch 157, loss = 0.002626866102218628\n",
      "epoch 9, batch 158, loss = 0.003588281339034438\n",
      "epoch 9, batch 159, loss = 0.01670481450855732\n",
      "epoch 9, batch 160, loss = 0.006910258438438177\n",
      "epoch 9, batch 161, loss = 0.0025899470783770084\n",
      "epoch 9, batch 162, loss = 0.009287277236580849\n",
      "epoch 9, batch 163, loss = 0.013284402899444103\n",
      "epoch 9, batch 164, loss = 0.014589854516088963\n",
      "epoch 9, batch 165, loss = 0.01685638539493084\n",
      "epoch 9, batch 166, loss = 0.0019918300677090883\n",
      "epoch 9, batch 167, loss = 0.02434857189655304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, batch 168, loss = 0.0032921633683145046\n",
      "epoch 9, batch 169, loss = 0.02299688756465912\n",
      "epoch 9, batch 170, loss = 0.0016867290250957012\n",
      "epoch 9, batch 171, loss = 0.0038762723561376333\n",
      "epoch 9, batch 172, loss = 0.002283311216160655\n",
      "epoch 9, batch 173, loss = 0.03737495839595795\n",
      "epoch 9, batch 174, loss = 0.014910642988979816\n",
      "epoch 9, batch 175, loss = 0.023654501885175705\n",
      "epoch 9, batch 176, loss = 0.016225432977080345\n",
      "epoch 9, batch 177, loss = 0.006804327480494976\n",
      "epoch 9, batch 178, loss = 0.03624279424548149\n",
      "epoch 9, batch 179, loss = 0.019803933799266815\n",
      "epoch 9, batch 180, loss = 0.0025149851571768522\n",
      "epoch 9, batch 181, loss = 0.03985609859228134\n",
      "epoch 9, batch 182, loss = 0.005617210641503334\n",
      "epoch 9, batch 183, loss = 0.011278485879302025\n",
      "epoch 9, batch 184, loss = 0.01636994630098343\n",
      "epoch 9, batch 185, loss = 0.00774864386767149\n",
      "epoch 9, batch 186, loss = 0.0053144400008022785\n",
      "epoch 9, batch 187, loss = 0.02393827959895134\n",
      "epoch 9, batch 188, loss = 0.010634895414113998\n",
      "epoch 9, batch 189, loss = 0.02426838129758835\n",
      "epoch 9, batch 190, loss = 0.001347392681054771\n",
      "epoch 9, batch 191, loss = 0.0016048072138801217\n",
      "epoch 9, batch 192, loss = 0.02066992036998272\n",
      "epoch 9, batch 193, loss = 0.02345026470720768\n",
      "epoch 9, batch 194, loss = 0.015641478821635246\n",
      "epoch 9, batch 195, loss = 0.00805490929633379\n",
      "epoch 9, batch 196, loss = 0.025169162079691887\n",
      "epoch 9, batch 197, loss = 0.012135698460042477\n",
      "epoch 9, batch 198, loss = 0.0055060600861907005\n",
      "epoch 9, batch 199, loss = 0.016901852563023567\n",
      "epoch 9, batch 200, loss = 0.014056907966732979\n",
      "epoch 10, batch 2, loss = 0.01058068498969078\n",
      "epoch 10, batch 3, loss = 0.0043489886447787285\n",
      "epoch 10, batch 4, loss = 0.0038555145729333162\n",
      "epoch 10, batch 5, loss = 0.009267824701964855\n",
      "epoch 10, batch 6, loss = 0.011741834692656994\n",
      "epoch 10, batch 7, loss = 0.010618187487125397\n",
      "epoch 10, batch 8, loss = 0.0063956850208342075\n",
      "epoch 10, batch 9, loss = 0.007569650188088417\n",
      "epoch 10, batch 10, loss = 0.006829853169620037\n",
      "epoch 10, batch 11, loss = 0.00427324278280139\n",
      "epoch 10, batch 12, loss = 0.001386276911944151\n",
      "epoch 10, batch 13, loss = 0.006603842601180077\n",
      "epoch 10, batch 14, loss = 0.0028851190581917763\n",
      "epoch 10, batch 15, loss = 0.0024600743781775236\n",
      "epoch 10, batch 16, loss = 0.006860039196908474\n",
      "epoch 10, batch 17, loss = 0.008741701021790504\n",
      "epoch 10, batch 18, loss = 0.011211253702640533\n",
      "epoch 10, batch 19, loss = 0.006609528791159391\n",
      "epoch 10, batch 20, loss = 0.0039435443468391895\n",
      "epoch 10, batch 21, loss = 0.013673101551830769\n",
      "epoch 10, batch 22, loss = 0.01113823615014553\n",
      "epoch 10, batch 23, loss = 0.02552848495543003\n",
      "epoch 10, batch 24, loss = 0.0028102940414100885\n",
      "epoch 10, batch 25, loss = 0.00653797434642911\n",
      "epoch 10, batch 26, loss = 0.0006928422371856868\n",
      "epoch 10, batch 27, loss = 0.0035230484791100025\n",
      "epoch 10, batch 28, loss = 0.018400205299258232\n",
      "epoch 10, batch 29, loss = 0.026955939829349518\n",
      "epoch 10, batch 30, loss = 0.002274570520967245\n",
      "epoch 10, batch 31, loss = 0.018904317170381546\n",
      "epoch 10, batch 32, loss = 0.001768368063494563\n",
      "epoch 10, batch 33, loss = 0.005511903669685125\n",
      "epoch 10, batch 34, loss = 0.02347656339406967\n",
      "epoch 10, batch 35, loss = 0.015261642634868622\n",
      "epoch 10, batch 36, loss = 0.0010275447275489569\n",
      "epoch 10, batch 37, loss = 0.005743952933698893\n",
      "epoch 10, batch 38, loss = 0.0034065376967191696\n",
      "epoch 10, batch 39, loss = 0.002647684421390295\n",
      "epoch 10, batch 40, loss = 0.004889614414423704\n",
      "epoch 10, batch 41, loss = 0.0029112843330949545\n",
      "epoch 10, batch 42, loss = 0.002179393544793129\n",
      "epoch 10, batch 43, loss = 0.020305795595049858\n",
      "epoch 10, batch 44, loss = 0.0022810136433690786\n",
      "epoch 10, batch 45, loss = 0.010750365443527699\n",
      "epoch 10, batch 46, loss = 0.005826704669743776\n",
      "epoch 10, batch 47, loss = 0.00827206764370203\n",
      "epoch 10, batch 48, loss = 0.004104254767298698\n",
      "epoch 10, batch 49, loss = 0.029759380966424942\n",
      "epoch 10, batch 50, loss = 0.017945552244782448\n",
      "epoch 10, batch 51, loss = 0.01387261226773262\n",
      "epoch 10, batch 52, loss = 0.012211826629936695\n",
      "epoch 10, batch 53, loss = 0.000612047384493053\n",
      "epoch 10, batch 54, loss = 0.002065097913146019\n",
      "epoch 10, batch 55, loss = 0.0043815728276968\n",
      "epoch 10, batch 56, loss = 0.002336806384846568\n",
      "epoch 10, batch 57, loss = 0.001530979759991169\n",
      "epoch 10, batch 58, loss = 0.02320239692926407\n",
      "epoch 10, batch 59, loss = 0.016525667160749435\n",
      "epoch 10, batch 60, loss = 0.0014046416617929935\n",
      "epoch 10, batch 61, loss = 0.002261097077280283\n",
      "epoch 10, batch 62, loss = 0.008476080372929573\n",
      "epoch 10, batch 63, loss = 0.00183602306060493\n",
      "epoch 10, batch 64, loss = 0.001371050369925797\n",
      "epoch 10, batch 65, loss = 0.003638734808191657\n",
      "epoch 10, batch 66, loss = 0.003382825292646885\n",
      "epoch 10, batch 67, loss = 0.006941169500350952\n",
      "epoch 10, batch 68, loss = 0.021389851346611977\n",
      "epoch 10, batch 69, loss = 0.0019639492966234684\n",
      "epoch 10, batch 70, loss = 0.01890585385262966\n",
      "epoch 10, batch 71, loss = 0.010852468200027943\n",
      "epoch 10, batch 72, loss = 0.0022238176316022873\n",
      "epoch 10, batch 73, loss = 0.00829014740884304\n",
      "epoch 10, batch 74, loss = 0.010983598418533802\n",
      "epoch 10, batch 75, loss = 0.0029115222860127687\n",
      "epoch 10, batch 76, loss = 0.01283416897058487\n",
      "epoch 10, batch 77, loss = 0.002555240411311388\n",
      "epoch 10, batch 78, loss = 0.019457153975963593\n",
      "epoch 10, batch 79, loss = 0.00669883843511343\n",
      "epoch 10, batch 80, loss = 0.0032057089265435934\n",
      "epoch 10, batch 81, loss = 0.08920972794294357\n",
      "epoch 10, batch 82, loss = 0.0007726952317170799\n",
      "epoch 10, batch 83, loss = 0.013172640465199947\n",
      "epoch 10, batch 84, loss = 0.009328647516667843\n",
      "epoch 10, batch 85, loss = 0.0013798087602481246\n",
      "epoch 10, batch 86, loss = 0.011219999752938747\n",
      "epoch 10, batch 87, loss = 0.000774666084907949\n",
      "epoch 10, batch 88, loss = 0.0240331944078207\n",
      "epoch 10, batch 89, loss = 0.003961907234042883\n",
      "epoch 10, batch 90, loss = 0.001978934509679675\n",
      "epoch 10, batch 91, loss = 0.0035180021077394485\n",
      "epoch 10, batch 92, loss = 0.003988181706517935\n",
      "epoch 10, batch 93, loss = 0.0011762731010094285\n",
      "epoch 10, batch 94, loss = 0.006447277031838894\n",
      "epoch 10, batch 95, loss = 0.002140921773388982\n",
      "epoch 10, batch 96, loss = 0.0029414051678031683\n",
      "epoch 10, batch 97, loss = 0.0026226716581732035\n",
      "epoch 10, batch 98, loss = 0.03785400092601776\n",
      "epoch 10, batch 99, loss = 0.008355071768164635\n",
      "epoch 10, batch 100, loss = 0.026294339448213577\n",
      "epoch 10, batch 101, loss = 0.018916279077529907\n",
      "epoch 10, batch 102, loss = 0.0026664105243980885\n",
      "epoch 10, batch 103, loss = 0.036369454115629196\n",
      "epoch 10, batch 104, loss = 0.005142367910593748\n",
      "epoch 10, batch 105, loss = 0.001874353620223701\n",
      "epoch 10, batch 106, loss = 0.008108031935989857\n",
      "epoch 10, batch 107, loss = 0.017482435330748558\n",
      "epoch 10, batch 108, loss = 0.005757321137934923\n",
      "epoch 10, batch 109, loss = 0.003777825739234686\n",
      "epoch 10, batch 110, loss = 0.019612174481153488\n",
      "epoch 10, batch 111, loss = 0.013348949141800404\n",
      "epoch 10, batch 112, loss = 0.0019317145925015211\n",
      "epoch 10, batch 113, loss = 0.012661128304898739\n",
      "epoch 10, batch 114, loss = 0.003925031516700983\n",
      "epoch 10, batch 115, loss = 0.01466458197683096\n",
      "epoch 10, batch 116, loss = 0.034379519522190094\n",
      "epoch 10, batch 117, loss = 0.05930706486105919\n",
      "epoch 10, batch 118, loss = 0.0023322957567870617\n",
      "epoch 10, batch 119, loss = 0.0016027824021875858\n",
      "epoch 10, batch 120, loss = 0.01722593419253826\n",
      "epoch 10, batch 121, loss = 0.00874412152916193\n",
      "epoch 10, batch 122, loss = 0.007322774734348059\n",
      "epoch 10, batch 123, loss = 0.007880020886659622\n",
      "epoch 10, batch 124, loss = 0.0036877428647130728\n",
      "epoch 10, batch 125, loss = 0.01233520358800888\n",
      "epoch 10, batch 126, loss = 0.00632946752011776\n",
      "epoch 10, batch 127, loss = 0.003588777268305421\n",
      "epoch 10, batch 128, loss = 0.018289057537913322\n",
      "epoch 10, batch 129, loss = 0.01053039450198412\n",
      "epoch 10, batch 130, loss = 0.005839620716869831\n",
      "epoch 10, batch 131, loss = 0.006941122468560934\n",
      "epoch 10, batch 132, loss = 0.0198868066072464\n",
      "epoch 10, batch 133, loss = 0.008240187540650368\n",
      "epoch 10, batch 134, loss = 0.006059073377400637\n",
      "epoch 10, batch 135, loss = 0.003004853380843997\n",
      "epoch 10, batch 136, loss = 0.0021289873402565718\n",
      "epoch 10, batch 137, loss = 0.005331047810614109\n",
      "epoch 10, batch 138, loss = 0.006882569752633572\n",
      "epoch 10, batch 139, loss = 0.00721092289313674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, batch 140, loss = 0.00450849300250411\n",
      "epoch 10, batch 141, loss = 0.002090448746457696\n",
      "epoch 10, batch 142, loss = 0.0016692642820999026\n",
      "epoch 10, batch 143, loss = 0.002837055828422308\n",
      "epoch 10, batch 144, loss = 0.00878151971846819\n",
      "epoch 10, batch 145, loss = 0.015452180057764053\n",
      "epoch 10, batch 146, loss = 0.0015191402053460479\n",
      "epoch 10, batch 147, loss = 0.01469923835247755\n",
      "epoch 10, batch 148, loss = 0.03875115513801575\n",
      "epoch 10, batch 149, loss = 0.003103553084656596\n",
      "epoch 10, batch 150, loss = 0.0012254859320819378\n",
      "epoch 10, batch 151, loss = 0.01703154481947422\n",
      "epoch 10, batch 152, loss = 0.0012381767155602574\n",
      "epoch 10, batch 153, loss = 0.02125619910657406\n",
      "epoch 10, batch 154, loss = 0.0013653722126036882\n",
      "epoch 10, batch 155, loss = 0.011859162710607052\n",
      "epoch 10, batch 156, loss = 0.002851830329746008\n",
      "epoch 10, batch 157, loss = 0.009850246831774712\n",
      "epoch 10, batch 158, loss = 0.004307026043534279\n",
      "epoch 10, batch 159, loss = 0.009314527735114098\n",
      "epoch 10, batch 160, loss = 0.023016640916466713\n",
      "epoch 10, batch 161, loss = 0.0038775959983468056\n",
      "epoch 10, batch 162, loss = 0.0019171294989064336\n",
      "epoch 10, batch 163, loss = 0.005441158544272184\n",
      "epoch 10, batch 164, loss = 0.042102519422769547\n",
      "epoch 10, batch 165, loss = 0.00237144622951746\n",
      "epoch 10, batch 166, loss = 0.013976470567286015\n",
      "epoch 10, batch 167, loss = 0.004663662984967232\n",
      "epoch 10, batch 168, loss = 0.001902287476696074\n",
      "epoch 10, batch 169, loss = 0.004907498136162758\n",
      "epoch 10, batch 170, loss = 0.005521109793335199\n",
      "epoch 10, batch 171, loss = 0.00370904803276062\n",
      "epoch 10, batch 172, loss = 0.012501686811447144\n",
      "epoch 10, batch 173, loss = 0.0033473533112555742\n",
      "epoch 10, batch 174, loss = 0.0072731380350887775\n",
      "epoch 10, batch 175, loss = 0.0009640106000006199\n",
      "epoch 10, batch 176, loss = 0.0037801831495016813\n",
      "epoch 10, batch 177, loss = 0.010272284969687462\n",
      "epoch 10, batch 178, loss = 0.0018332131439819932\n",
      "epoch 10, batch 179, loss = 0.003115556901320815\n",
      "epoch 10, batch 180, loss = 0.0069976262748241425\n",
      "epoch 10, batch 181, loss = 0.010651299729943275\n",
      "epoch 10, batch 182, loss = 0.006177976261824369\n",
      "epoch 10, batch 183, loss = 0.015073426999151707\n",
      "epoch 10, batch 184, loss = 0.007375391200184822\n",
      "epoch 10, batch 185, loss = 0.009026405401527882\n",
      "epoch 10, batch 186, loss = 0.00214795907959342\n",
      "epoch 10, batch 187, loss = 0.004529665689915419\n",
      "epoch 10, batch 188, loss = 0.0037456753198057413\n",
      "epoch 10, batch 189, loss = 0.011205022223293781\n",
      "epoch 10, batch 190, loss = 0.011103831231594086\n",
      "epoch 10, batch 191, loss = 0.0029354107100516558\n",
      "epoch 10, batch 192, loss = 0.01881021074950695\n",
      "epoch 10, batch 193, loss = 0.0036048151087015867\n",
      "epoch 10, batch 194, loss = 0.012098772451281548\n",
      "epoch 10, batch 195, loss = 0.0005860657547600567\n",
      "epoch 10, batch 196, loss = 0.006816002540290356\n",
      "epoch 10, batch 197, loss = 0.00581417977809906\n",
      "epoch 10, batch 198, loss = 0.004473962355405092\n",
      "epoch 10, batch 199, loss = 0.00332070654258132\n",
      "epoch 10, batch 200, loss = 0.004972907714545727\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "EPOCH      = 10\n",
    "LR         = 0.001\n",
    "\n",
    "model  = MyModel()\n",
    "loader = DataLoader(mnist, batch_size=BATCH_SIZE, shuffle=True)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "for i in range(EPOCH):\n",
    "    j = 0\n",
    "    for x, y in loader:\n",
    "        pred = model(x)\n",
    "        loss = lossfn(torch.log(pred), y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        j += 1\n",
    "        if (j // BATCH_SIZE) % 10 == 0:\n",
    "            print(f\"epoch {i+1}, batch {j+1}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证模型的效果，我们对前10个观测（即之前生成的 `smallx` 和 `smally`）进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.001 0.    0.766 0.    0.    0.    0.233]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.999 0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]]\n",
      "tensor([7, 0, 4, 9, 7, 8, 5, 6, 7, 7])\n",
      "tensor([7, 0, 4, 9, 7, 8, 5, 6, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "ypred = model(smallx)\n",
    "print(np.round(ypred.detach().cpu().numpy(), 3))\n",
    "print(torch.argmax(ypred, dim=1))\n",
    "print(smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试集表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy = 0.9907000064849854\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "for image, labels in val_loader:\n",
    "    y_pred = model(image)\n",
    "    _, pred = y_pred.max(1)\n",
    "    correct_count += (pred == labels).sum()\n",
    "    \n",
    "accuracy = correct_count / len(val_set)\n",
    "\n",
    "print(\"\\nModel Accuracy =\", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果模型搭建和训练都正常，那么每一行中概率最大的取值所在的位置应该正好对应真实的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们用模型对一些真实的手写数字图片进行预测。请你利用绘图软件（如 Windows 自带的绘图，或 Photoshop 等）准备10张正方形黑色底色的图片，每张用鼠标绘制一个数字（请使用较粗的笔划），从0到9，然后以0.png，1.png等文件名存储下来，放到当前目录一个名为 digits 的文件夹中。以下是几个例子：\n",
    "![](digits/sample0.png) ![](digits/sample5.png) ![](digits/sample8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来利用 Pillow 软件包读取图片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=261x261 at 0x27B7CB70DC0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(\"digits/sample0.png\")\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时如果直接将其转为 Numpy 数组会得到三个通道："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 261, 3)\n"
     ]
    }
   ],
   "source": [
    "im_arr = np.array(im)\n",
    "print(im_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们先强制转换为灰度图片（单通道），再缩放至模型的图片大小 28 x 28："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABQElEQVR4nGNgoAlgROaIiop/5vny8cO7vxiSksEuwiyf+SQZ7+yd+4KBgYGBgQUmJRcQp3D/z8ebWmLvNIzlK94h6WQKyNf7+HbT9s+fBf4w2iRKxx5Csi3s9rvjkfJQDs+6l1FIxspksM9acAOmkomP7Q+EwcDAwMDgafxmDlyOge33z1cISSY1hhV3EXboqL5+jSQpwPjtP0JSUODeY4QkI9N/ToQckx7Lva9IdnJ8eIiQ1Av6dP4vQvL3nf/JEjA5sSyFuweQfMlgdO2NL5QpNOfVFXsGFND5Zo0SAwMDAwN74YuH/qhyDDpnPnQwMTAwMCTcu5fPjCbJGHn3ZhQDA3P4rQ/d3AzogL317a32oP4nbxeLYsgxMMivfvL9+ZdX85SQzYOzRAJtxa6d3vgNi0YGBgYGRjYmHDKDBQAAHZJkUSWMm9UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x27B7CB98CD0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = im.convert(\"L\")\n",
    "im.thumbnail((28, 28))\n",
    "im_arr = np.array(im)\n",
    "print(im_arr.shape)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了传递给模型对象，还需要先将数值归一化到 [0,1] 区间，转换为 PyTorch 的 Tensor 类型，并增加一个批次和一个通道的维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "test0 = torch.tensor(im_arr / 255.0, dtype=torch.float32).view(1, 1, 28, 28)\n",
    "print(test0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后对图片标签进行预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.116 0.    0.046 0.    0.    0.    0.    0.    0.    0.837]]\n",
      "预测标签： 9\n"
     ]
    }
   ],
   "source": [
    "pred0 = model(test0)\n",
    "print(np.round(pred0.detach().cpu().numpy(), 3))\n",
    "print(\"预测标签：\", torch.argmax(pred0).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测结果是否符合真实情形？请对你自己绘制出的10张图片进行类似的预测操作，并评价其效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大部分测试样本上模型预测表现较好，但对于一些样本，此模型不能很好地区分0，4与9这三个数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_image(im, model):\n",
    "    im_arr = np.array(im)\n",
    "    im = im.convert(\"L\")\n",
    "    im.thumbnail((28, 28))\n",
    "    im_arr = np.array(im)\n",
    "    test = torch.tensor(im_arr / 255.0, dtype=torch.float32).view(1, 1, 28, 28)\n",
    "    pred = model(test)\n",
    "    return torch.argmax(pred).item(), np.round(pred.detach().cpu().numpy(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0.004 0.987 0.    0.    0.004 0.    0.005 0.    0.    0.   ]]\n",
      "预测标签： 1\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample1.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "预测标签： 2\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample2.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "预测标签： 3\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample3.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "预测标签： 4\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample4.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "预测标签： 5\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample5.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "预测标签： 6\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample6.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0.   0.05 0.09 0.   0.   0.   0.   0.86 0.   0.  ]]\n",
      "预测标签： 7\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample7.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "预测标签： 8\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample8.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0.    0.001 0.    0.    0.71  0.    0.    0.276 0.    0.012]]\n",
      "预测标签： 4\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample9.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分布概率： [[0.99 0.   0.01 0.   0.   0.   0.   0.   0.   0.  ]]\n",
      "预测标签： 0\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/0.png\")\n",
    "pred, prob = pred_image(im, model)\n",
    "print(\"分布概率：\", prob)\n",
    "print(\"预测标签：\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 目标：通过对英文名数据进行训练，构建一个 RNN 模型，实现英文名的自动生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 主要步骤：获取和整理数据，对字符串进行 one-hot 编码，创建模型结构，定义损失函数，编写训练循环，最后生成人名字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取和整理数据。数据文件已存为 `data/names.txt`，先将其读取为字符串列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668\n",
      "['Abbas', 'Abbey', 'Abbott', 'Abdi', 'Abel']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "lines = io.open(\"data/names.txt\").read().strip().split('\\n')\n",
    "print(len(lines))\n",
    "print(lines[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，共读取了3668个名字。为了简单起见，我们将所有的大写字母转换为小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbas', 'abbey', 'abbott', 'abdi', 'abel', 'abraham', 'abrahams', 'abrams', 'ackary', 'ackroyd']\n"
     ]
    }
   ],
   "source": [
    "names = [s.lower() for s in lines]\n",
    "print(names[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要构建一个字符的字典。对于英文名来说很简单，即26个字母。我们可以通过下面的代码直接得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "char_dict = string.ascii_lowercase\n",
    "char_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 下面准备好 one-hot 编码所需的函数。编写函数 `char2index(char)`，将一个字母转换为其所在字典的位置。例如 `char2index(\"a\")` 要返回0，`char2index(\"z\")` 要返回25，等等。提示：使用字符串的 `.find()` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def char2index(char):\n",
    "    if char in char_dict:\n",
    "        return char_dict.find(char)\n",
    "    else:\n",
    "        return 26\n",
    "\n",
    "print(char2index(\"z\") == 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写 `char2tensor(char)` 函数，将一个字母转换为 one-hot 向量，即该向量中第 i 个元素为1，其余为0，其中 i 表示该字母在字典中的位置。\n",
    "\n",
    "**注意，该向量的长度应为27，因为我们要预留终止符，用 `[0.0, 0.0, ..., 1.0]` 表示**。\n",
    "\n",
    "`char2tensor(\"a\")` 应返回 `torch.tensor([1.0, 0.0, ...])`，`char2tensor(\"z\")` 应返回 `torch.tensor([0.0, ..., 1.0, 0.0])`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.zeros(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def char2tensor(char):\n",
    "    coding = torch.zeros(27)\n",
    "    coding[char2index(char)] = torch.tensor(1.0)\n",
    "    return coding\n",
    "\n",
    "print(char2tensor(\"a\"))\n",
    "print(char2tensor(\"z\"))\n",
    "print(char2tensor(\"z\").shape[0] == 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 构建模型。我们使用最简单的 RNN 结构，即隐藏单元是输入和上一期隐藏单元的线性变换加上 Tanh 激活函数，输出单元是隐藏单元的线性变换加上 Softmax 激活函数。输出的结果代表下一个字符的概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = torch.nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.h2o = torch.nn.Linear(hidden_size, 27)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = torch.nn.functional.softmax(self.h2o(hidden), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做一个简单的测试。请在下面的代码中加入适当的语句，使得每次运行的结果不变。根据其输出结果，请问当前模型预测字符a的下一个字符是什么？为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0290, 0.0327, 0.0320, 0.0354, 0.0432, 0.0395, 0.0417, 0.0371, 0.0409,\n",
      "         0.0275, 0.0328, 0.0359, 0.0373, 0.0391, 0.0339, 0.0366, 0.0430, 0.0364,\n",
      "         0.0472, 0.0435, 0.0376, 0.0327, 0.0418, 0.0370, 0.0279, 0.0426, 0.0357]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=27, hidden_size=10)\n",
    "input = char2tensor(\"a\")\n",
    "hidden = rnn.init_hidden()\n",
    "output, hidden = rnn(input.view(1, 27), hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 接下来我们定义好损失函数。与第1题中类似，预测值是一个概率分布，而真实的标签是0到26中的一个整数，代表真实的下一个字符在字典中的位置。假设当前处理的名字为\"abel\"，那么字符a的输出结果对应的标签是什么？请完成下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4199, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target = char2index('b')\n",
    "\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "loss = lossfn(torch.log(output), torch.tensor([target]))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 明确单个字符的损失函数的计算方法后，请在下面计算出\"abel\"这个观测整体的损失函数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2381, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "name = 'abel'\n",
    "rnn = RNN(input_size=27, hidden_size=10)\n",
    "hidden = rnn.init_hidden()\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "loss = 0\n",
    "\n",
    "for i in range(len(name)):\n",
    "    input = char2tensor(name[i])\n",
    "    output, hidden = rnn(input.view(1,27), hidden)\n",
    "\n",
    "    if i == len(name) - 1:\n",
    "        target = 26\n",
    "    else:\n",
    "        target = char2index(name[i + 1])\n",
    "\n",
    "    loss = loss + lossfn(torch.log(output), torch.tensor([target]))\n",
    "\n",
    "loss = loss / len(name)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 将上述过程在数据上进行反复迭代，训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, obs 0, loss = 3.3991856575012207\n",
      "epoch 0, obs 1000, loss = 2.918344020843506\n",
      "epoch 0, obs 2000, loss = 2.6398532390594482\n",
      "epoch 0, obs 3000, loss = 3.1096529960632324\n",
      "epoch 1, obs 0, loss = 2.776276111602783\n",
      "epoch 1, obs 1000, loss = 2.5617995262145996\n",
      "epoch 1, obs 2000, loss = 2.415987014770508\n",
      "epoch 1, obs 3000, loss = 2.4527029991149902\n",
      "epoch 2, obs 0, loss = 2.5975751876831055\n",
      "epoch 2, obs 1000, loss = 2.585355520248413\n",
      "epoch 2, obs 2000, loss = 2.5821340084075928\n",
      "epoch 2, obs 3000, loss = 2.7673685550689697\n",
      "epoch 3, obs 0, loss = 2.584155559539795\n",
      "epoch 3, obs 1000, loss = 2.1915831565856934\n",
      "epoch 3, obs 2000, loss = 2.4389891624450684\n",
      "epoch 3, obs 3000, loss = 2.0150928497314453\n",
      "epoch 4, obs 0, loss = 2.4576306343078613\n",
      "epoch 4, obs 1000, loss = 2.9122250080108643\n",
      "epoch 4, obs 2000, loss = 2.5470449924468994\n",
      "epoch 4, obs 3000, loss = 2.1251466274261475\n",
      "epoch 5, obs 0, loss = 2.283302068710327\n",
      "epoch 5, obs 1000, loss = 2.564319133758545\n",
      "epoch 5, obs 2000, loss = 2.1136810779571533\n",
      "epoch 5, obs 3000, loss = 2.19895601272583\n",
      "epoch 6, obs 0, loss = 2.385223388671875\n",
      "epoch 6, obs 1000, loss = 2.7023868560791016\n",
      "epoch 6, obs 2000, loss = 2.240619659423828\n",
      "epoch 6, obs 3000, loss = 2.557904005050659\n",
      "epoch 7, obs 0, loss = 1.9776948690414429\n",
      "epoch 7, obs 1000, loss = 1.7167218923568726\n",
      "epoch 7, obs 2000, loss = 1.6471465826034546\n",
      "epoch 7, obs 3000, loss = 1.972745418548584\n",
      "epoch 8, obs 0, loss = 2.3119893074035645\n",
      "epoch 8, obs 1000, loss = 2.0465922355651855\n",
      "epoch 8, obs 2000, loss = 2.7650790214538574\n",
      "epoch 8, obs 3000, loss = 1.8902024030685425\n",
      "epoch 9, obs 0, loss = 2.1596932411193848\n",
      "epoch 9, obs 1000, loss = 2.3838236331939697\n",
      "epoch 9, obs 2000, loss = 2.1008870601654053\n",
      "epoch 9, obs 3000, loss = 3.150839328765869\n",
      "epoch 10, obs 0, loss = 2.3192951679229736\n",
      "epoch 10, obs 1000, loss = 1.7946990728378296\n",
      "epoch 10, obs 2000, loss = 2.095365047454834\n",
      "epoch 10, obs 3000, loss = 2.1764092445373535\n",
      "epoch 11, obs 0, loss = 2.5558249950408936\n",
      "epoch 11, obs 1000, loss = 2.033247232437134\n",
      "epoch 11, obs 2000, loss = 1.846058964729309\n",
      "epoch 11, obs 3000, loss = 2.865107774734497\n",
      "epoch 12, obs 0, loss = 2.4461772441864014\n",
      "epoch 12, obs 1000, loss = 1.8531479835510254\n",
      "epoch 12, obs 2000, loss = 2.5126984119415283\n",
      "epoch 12, obs 3000, loss = 1.866956114768982\n",
      "epoch 13, obs 0, loss = 2.2160885334014893\n",
      "epoch 13, obs 1000, loss = 2.3633995056152344\n",
      "epoch 13, obs 2000, loss = 1.83836829662323\n",
      "epoch 13, obs 3000, loss = 2.1760966777801514\n",
      "epoch 14, obs 0, loss = 2.1095714569091797\n",
      "epoch 14, obs 1000, loss = 1.859795093536377\n",
      "epoch 14, obs 2000, loss = 2.0996763706207275\n",
      "epoch 14, obs 3000, loss = 2.054687261581421\n",
      "epoch 15, obs 0, loss = 1.71103835105896\n",
      "epoch 15, obs 1000, loss = 1.7189702987670898\n",
      "epoch 15, obs 2000, loss = 1.5404363870620728\n",
      "epoch 15, obs 3000, loss = 3.1953582763671875\n",
      "epoch 16, obs 0, loss = 1.4377115964889526\n",
      "epoch 16, obs 1000, loss = 2.4978291988372803\n",
      "epoch 16, obs 2000, loss = 2.1674790382385254\n",
      "epoch 16, obs 3000, loss = 2.0198209285736084\n",
      "epoch 17, obs 0, loss = 1.6866999864578247\n",
      "epoch 17, obs 1000, loss = 2.8642983436584473\n",
      "epoch 17, obs 2000, loss = 2.06840181350708\n",
      "epoch 17, obs 3000, loss = 1.9931614398956299\n",
      "epoch 18, obs 0, loss = 2.2352402210235596\n",
      "epoch 18, obs 1000, loss = 1.878004789352417\n",
      "epoch 18, obs 2000, loss = 2.3398401737213135\n",
      "epoch 18, obs 3000, loss = 2.3058712482452393\n",
      "epoch 19, obs 0, loss = 2.0393431186676025\n",
      "epoch 19, obs 1000, loss = 2.102548837661743\n",
      "epoch 19, obs 2000, loss = 2.333542823791504\n",
      "epoch 19, obs 3000, loss = 1.82425057888031\n",
      "epoch 20, obs 0, loss = 2.6910452842712402\n",
      "epoch 20, obs 1000, loss = 2.234786033630371\n",
      "epoch 20, obs 2000, loss = 2.3193938732147217\n",
      "epoch 20, obs 3000, loss = 3.005868434906006\n",
      "epoch 21, obs 0, loss = 2.7048325538635254\n",
      "epoch 21, obs 1000, loss = 1.6238880157470703\n",
      "epoch 21, obs 2000, loss = 2.3899199962615967\n",
      "epoch 21, obs 3000, loss = 2.7344350814819336\n",
      "epoch 22, obs 0, loss = 2.0762550830841064\n",
      "epoch 22, obs 1000, loss = 2.0778727531433105\n",
      "epoch 22, obs 2000, loss = 2.9288816452026367\n",
      "epoch 22, obs 3000, loss = 1.9944665431976318\n",
      "epoch 23, obs 0, loss = 2.7135043144226074\n",
      "epoch 23, obs 1000, loss = 1.496219277381897\n",
      "epoch 23, obs 2000, loss = 2.049546241760254\n",
      "epoch 23, obs 3000, loss = 1.9015257358551025\n",
      "epoch 24, obs 0, loss = 2.394380569458008\n",
      "epoch 24, obs 1000, loss = 1.745930790901184\n",
      "epoch 24, obs 2000, loss = 2.2664432525634766\n",
      "epoch 24, obs 3000, loss = 2.154221534729004\n",
      "epoch 25, obs 0, loss = 2.3689167499542236\n",
      "epoch 25, obs 1000, loss = 2.5220205783843994\n",
      "epoch 25, obs 2000, loss = 1.818599820137024\n",
      "epoch 25, obs 3000, loss = 2.1449363231658936\n",
      "epoch 26, obs 0, loss = 2.6090574264526367\n",
      "epoch 26, obs 1000, loss = 1.8941987752914429\n",
      "epoch 26, obs 2000, loss = 2.006479024887085\n",
      "epoch 26, obs 3000, loss = 2.390723943710327\n",
      "epoch 27, obs 0, loss = 1.7413338422775269\n",
      "epoch 27, obs 1000, loss = 1.7901968955993652\n",
      "epoch 27, obs 2000, loss = 2.0087668895721436\n",
      "epoch 27, obs 3000, loss = 1.5851432085037231\n",
      "epoch 28, obs 0, loss = 2.222456455230713\n",
      "epoch 28, obs 1000, loss = 1.7988321781158447\n",
      "epoch 28, obs 2000, loss = 2.963691234588623\n",
      "epoch 28, obs 3000, loss = 1.8033971786499023\n",
      "epoch 29, obs 0, loss = 2.240217447280884\n",
      "epoch 29, obs 1000, loss = 2.281973123550415\n",
      "epoch 29, obs 2000, loss = 1.7399836778640747\n",
      "epoch 29, obs 3000, loss = 2.760715961456299\n",
      "epoch 30, obs 0, loss = 2.0453226566314697\n",
      "epoch 30, obs 1000, loss = 1.9564697742462158\n",
      "epoch 30, obs 2000, loss = 1.925641655921936\n",
      "epoch 30, obs 3000, loss = 1.5345377922058105\n",
      "epoch 31, obs 0, loss = 2.12668776512146\n",
      "epoch 31, obs 1000, loss = 2.5530741214752197\n",
      "epoch 31, obs 2000, loss = 1.5978453159332275\n",
      "epoch 31, obs 3000, loss = 2.5964345932006836\n",
      "epoch 32, obs 0, loss = 1.7524241209030151\n",
      "epoch 32, obs 1000, loss = 2.743039846420288\n",
      "epoch 32, obs 2000, loss = 2.1489384174346924\n",
      "epoch 32, obs 3000, loss = 1.4831092357635498\n",
      "epoch 33, obs 0, loss = 2.0390851497650146\n",
      "epoch 33, obs 1000, loss = 2.813689947128296\n",
      "epoch 33, obs 2000, loss = 1.7920725345611572\n",
      "epoch 33, obs 3000, loss = 1.9520386457443237\n",
      "epoch 34, obs 0, loss = 1.867772102355957\n",
      "epoch 34, obs 1000, loss = 2.5545527935028076\n",
      "epoch 34, obs 2000, loss = 2.145887613296509\n",
      "epoch 34, obs 3000, loss = 2.4498167037963867\n",
      "epoch 35, obs 0, loss = 1.8324720859527588\n",
      "epoch 35, obs 1000, loss = 2.408722400665283\n",
      "epoch 35, obs 2000, loss = 2.7887473106384277\n",
      "epoch 35, obs 3000, loss = 1.9220558404922485\n",
      "epoch 36, obs 0, loss = 1.9851397275924683\n",
      "epoch 36, obs 1000, loss = 2.118201494216919\n",
      "epoch 36, obs 2000, loss = 1.5666478872299194\n",
      "epoch 36, obs 3000, loss = 1.3815141916275024\n",
      "epoch 37, obs 0, loss = 1.6772555112838745\n",
      "epoch 37, obs 1000, loss = 1.906529188156128\n",
      "epoch 37, obs 2000, loss = 1.5776368379592896\n",
      "epoch 37, obs 3000, loss = 1.9461513757705688\n",
      "epoch 38, obs 0, loss = 2.4258594512939453\n",
      "epoch 38, obs 1000, loss = 2.1929931640625\n",
      "epoch 38, obs 2000, loss = 3.0075104236602783\n",
      "epoch 38, obs 3000, loss = 2.1542623043060303\n",
      "epoch 39, obs 0, loss = 2.0809285640716553\n",
      "epoch 39, obs 1000, loss = 1.8822883367538452\n",
      "epoch 39, obs 2000, loss = 2.0476081371307373\n",
      "epoch 39, obs 3000, loss = 1.8187958002090454\n",
      "epoch 40, obs 0, loss = 1.44320547580719\n",
      "epoch 40, obs 1000, loss = 2.6095190048217773\n",
      "epoch 40, obs 2000, loss = 1.7925220727920532\n",
      "epoch 40, obs 3000, loss = 2.117598533630371\n",
      "epoch 41, obs 0, loss = 2.760441303253174\n",
      "epoch 41, obs 1000, loss = 2.0689802169799805\n",
      "epoch 41, obs 2000, loss = 2.265063524246216\n",
      "epoch 41, obs 3000, loss = 1.9979454278945923\n",
      "epoch 42, obs 0, loss = 1.8715239763259888\n",
      "epoch 42, obs 1000, loss = 2.143951177597046\n",
      "epoch 42, obs 2000, loss = 2.6502082347869873\n",
      "epoch 42, obs 3000, loss = 1.719042181968689\n",
      "epoch 43, obs 0, loss = 2.2406322956085205\n",
      "epoch 43, obs 1000, loss = 1.9284414052963257\n",
      "epoch 43, obs 2000, loss = 2.0200116634368896\n",
      "epoch 43, obs 3000, loss = 1.8103766441345215\n",
      "epoch 44, obs 0, loss = 3.3934316635131836\n",
      "epoch 44, obs 1000, loss = 1.9908273220062256\n",
      "epoch 44, obs 2000, loss = 2.0639679431915283\n",
      "epoch 44, obs 3000, loss = 1.701993465423584\n",
      "epoch 45, obs 0, loss = 2.6029787063598633\n",
      "epoch 45, obs 1000, loss = 2.895801305770874\n",
      "epoch 45, obs 2000, loss = 2.5619044303894043\n",
      "epoch 45, obs 3000, loss = 2.2468907833099365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46, obs 0, loss = 2.150540590286255\n",
      "epoch 46, obs 1000, loss = 2.5037455558776855\n",
      "epoch 46, obs 2000, loss = 1.8907172679901123\n",
      "epoch 46, obs 3000, loss = 1.9620060920715332\n",
      "epoch 47, obs 0, loss = 1.9173694849014282\n",
      "epoch 47, obs 1000, loss = 1.7572441101074219\n",
      "epoch 47, obs 2000, loss = 1.3365674018859863\n",
      "epoch 47, obs 3000, loss = 2.493136405944824\n",
      "epoch 48, obs 0, loss = 2.7950522899627686\n",
      "epoch 48, obs 1000, loss = 1.458345651626587\n",
      "epoch 48, obs 2000, loss = 2.025412082672119\n",
      "epoch 48, obs 3000, loss = 2.5827722549438477\n",
      "epoch 49, obs 0, loss = 2.926356554031372\n",
      "epoch 49, obs 1000, loss = 2.778064727783203\n",
      "epoch 49, obs 2000, loss = 1.9001984596252441\n",
      "epoch 49, obs 3000, loss = 2.080321788787842\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "\n",
    "n = len(names)\n",
    "n_hidden = 16\n",
    "n_input = 27\n",
    "nepoch = 50\n",
    "\n",
    "rnn = RNN(n_input, n_hidden)\n",
    "opt = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "train_ind = np.arange(n)\n",
    "losses = []\n",
    "\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "\n",
    "# Loop over epochs\n",
    "for k in range(nepoch):\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(train_ind)\n",
    "    # Loop over observations. Each observation is a name\n",
    "    for i in range(n):\n",
    "        name = names[train_ind[i]]\n",
    "        nchar = len(name)\n",
    "        # Loop over the characters in the name\n",
    "        # Each input character has a target, which is the index of the next character in the dictionary\n",
    "        # For the last character in the name, the target is the end-of-sequence symbol, which has index 26\n",
    "        loss = 0.0\n",
    "        hidden = rnn.init_hidden()\n",
    "        for j in range(nchar):\n",
    "            input = char2tensor(name[j])\n",
    "            output, hidden = rnn(input.view(1, n_input), hidden)\n",
    "\n",
    "            if j == nchar - 1:\n",
    "                target = 26\n",
    "            else:\n",
    "                target = char2index(name[j + 1])\n",
    "\n",
    "            loss = loss + lossfn(torch.log(output), torch.tensor([target]))\n",
    "    \n",
    "        loss = loss / nchar\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"epoch {k}, obs {i}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d8b12850d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 编写一个函数 `random_first_letter()`，它随机返回字典中的一个字符，我们将利用它来随机生成第一个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n",
      "c\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "def random_first_letter():\n",
    "    idx = np.random.randint(0, 25)\n",
    "    return char_dict[idx]\n",
    "\n",
    "print(random_first_letter())\n",
    "print(random_first_letter())\n",
    "print(random_first_letter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请简要说明如下的代码的含义（可以在代码中加入注释），然后利用它随机生成10个名字。评价生成的结果，并简要说明可以如何改进模型的效果？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_name(max_len=20):\n",
    "    # 将模型从训练调整到预测模式，去掉训练中的dropout或batch normlization层等\n",
    "    rnn.eval()\n",
    "    # 随机产生名字的第一个字母，并将其转化成tensor形式，作为RNN的第一个输入\n",
    "    first_letter = random_first_letter()\n",
    "    char_ind = [char2index(first_letter)]\n",
    "    input = char2tensor(first_letter)\n",
    "    # 初始化隐藏状态\n",
    "    hidden = rnn.init_hidden()\n",
    "    # 将第一个字母与初始隐藏状态带入RNN模型中，不断产生下一个字母，直至达到名字最大长度或生成的下一个字母为终止符\n",
    "    for i in range(max_len - 1):\n",
    "        output, hidden = rnn(input.view(1, n_input), hidden)\n",
    "        # 找到生成的下一个字母对应的tensor中的下标为1的位置\n",
    "        ind = torch.argmax(output).item()\n",
    "        if ind == 26:\n",
    "            break\n",
    "        char_ind.append(ind)\n",
    "        # 将输入更新为当前产生的字母对应的tensor\n",
    "        input.zero_()\n",
    "        input[ind] = 1.0\n",
    "    return \"\".join([char_dict[i] for i in char_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的单词比较符合英语单词的一般形式，但大多不能作为名字，而且学习到的字母组合方式比较单一。模型训练时的损失值波动性也较大，很难降低。可能需要尝试更加复杂一点的模型结构，并且增加训练数据数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darder\n",
      "vinger\n",
      "burn\n",
      "arder\n",
      "sone\n",
      "order\n",
      "harder\n",
      "corder\n",
      "order\n",
      "qurder\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(random_name(max_len=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用卷积函数实现任意大整数的乘法。给定两个整数，如 183612 和 23333，用两个列表表达它们的序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [1, 8, 3, 6, 1, 2]\n",
    "n2 = [2, 3, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请编写一个函数 `integer_mult(n1, n2)`，返回 `n1 * n2` 对应的整数序列。注意不要直接调用乘法表达式（设想有两个非常大的整数，直接相乘可能会导致数值溢出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 8, 4, 2, 1, 8, 7, 9, 6]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def integer_mult(n1, n2):\n",
    "    poly_multi_result = np.convolve(n1, n2)\n",
    "    to_next = 0\n",
    "    params = []\n",
    "    for j in poly_multi_result[::-1]:\n",
    "        params.append((j + to_next) % 10)\n",
    "        to_next = (j + to_next) // 10\n",
    "    while to_next:\n",
    "        params.append(to_next % 10)\n",
    "        to_next = to_next // 10  \n",
    "    return params[::-1]\n",
    "\n",
    "res = integer_mult(n1, n2)\n",
    "print(res)\n",
    "print(res == [4, 2, 8, 4, 2, 1, 8, 7, 9, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 先实现多项式的乘法。例如，给定 $p(x)=1+2x+x^4$ 和 $q(x)=x+3x^2+5x^3$，计算 $r(x)=p(x)q(x)$。我们将 $p(x)$ 编码为 `p = [1, 2, 0, 0, 1]`，$q(x)$ 编码为 `q = [0, 1, 3, 5]`，请编写函数 `poly_mult(p, q)`，使得 `poly_mult(p, q) == [0, 1, 5, 11, 10, 1, 3, 5]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "p = [1, 2, 0, 0, 1]\n",
    "q = [0, 1, 3, 5]\n",
    "\n",
    "def poly_mult(p, q):\n",
    "    return np.convolve(p, q)\n",
    "\n",
    "print(poly_mult(p, q) == [0, 1, 5, 11, 10,  1, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对于任意的一个整数，将其看成是某个多项式在 $x=10$ 处的取值，如 $123 = p_1(10)$，$p_1(x)=3+2x+x^2$，$5310 = p_2(10)$，$p_2(x)=x+3x^2+5x^3$，注意需要适当将序列反序。因此，要计算 $123\\times 5310$，相当于计算 $r(10)$ 的值，但为了避免直接进行乘法运算（防止溢出），可以先计算 $r(x)$ 的表达式（等价于其系数向量），然后建立起 $r(x)$ 的系数与 $r(10)$ 之间的联系（见如下第3点）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如果一个多项式 $r(x)$ 所有的系数都是0到9之间的整数，那么 $r(x)$ 和 $r(10)$ 的关系非常直接，比如若 $r(x)=1+2x+5x^2+3x^3$，则 $r(10)=3521$。但如果有系数超过10，就需要考虑进位的影响，比如 $r(x)=1+11x+2x^2$，$r(10)=311$。此时可以从 $r(x)$ 的第一项开始逐项进位，构造一个新的多项式 $r'(x)=1+x+3x^2$，满足 $r'(10)=r(10)$，且 $r'(x)$ 所有的系数都不超过10。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 综合利用以上信息，完成本题的算法编写。并测试 23742389754298365 * 809723950 的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_mult(n1, n2):\n",
    "    poly_multi_result = np.convolve(n1, n2)\n",
    "    to_next = 0\n",
    "    params = []\n",
    "    for j in poly_multi_result[::-1]:\n",
    "        params.append((j + to_next) % 10)\n",
    "        to_next = (j + to_next) // 10\n",
    "    while to_next:\n",
    "        params.append(to_next % 10)\n",
    "        to_next = to_next // 10  \n",
    "    return params[::-1]\n",
    "\n",
    "def big_number_multi(params):\n",
    "    result = 0\n",
    "    for i, j in enumerate(params[::-1]):\n",
    "        result = result + (10**i)*j\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将两个数字表示成列表表达形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [2,3,7,4,2,3,8,9,7,5,4,2,9,8,3,6,5]\n",
    "n2 = [8,0,9,7,2,3,9,5,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算乘积多项式系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9, 2, 2, 4, 7, 8, 1, 6, 1, 4, 2, 9, 0, 0, 0, 1, 5, 8, 6, 3, 4, 1, 7, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "poly_params = integer_mult(n1, n2)\n",
    "print(poly_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将$x=10$带入多项式中进行计算，得到乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9224781614290002e+25"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_number_multi(poly_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
