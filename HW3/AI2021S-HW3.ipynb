{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业3：深度学习框架实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业将练习深度学习框架的使用，大部分内容用 PyTorch 实现。第1题利用卷积层和全连接层实现手写数字的识别，第2题利用 RNN 来实现英文名的自动生成，第3题是算法题，利用卷积运算实现任意大整数的乘法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 目标：通过对 MNIST 数据进行训练，构建一个简单的图像分类模型，对图片中的数字进行识别。你将利用该模型对自己真实手写出的数字进行预测，观察模型效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 主要步骤：获取数据，创建模型结构，定义损失函数，编写训练循环，实施预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取数据。我们使用知名的 MNIST 数据集，它可以从 PyTorch 中利用工具函数下载得到。原始的 MNIST 数据训练集大小为60000，我们随机抽取其中的10000个观测进行简单的训练。以下函数会在当前目录建立一个名为 data 的文件夹，其中会包含下载得到的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：请在任何程序的最开始加上随机数种子的设置。请保持这一习惯。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ee4731da30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "loader = DataLoader(mnist, batch_size=10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一次性取出随机抽取到的10000个观测，其中 x 是图片数据，y 是图片对应的数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个习惯性动作是查看数据的大小和维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以利用下面的函数展示图片的内容。如选择第一张图片，先将其转换成 Numpy 数组，再绘制图形："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = x[0].squeeze().cpu().numpy()\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来请你选择5个你喜欢的数字（10000以下），然后取出对应位置的图片，并画出它们的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for num in [111,222,333,444,555]:\n",
    "    img = x[num].squeeze().cpu().numpy()\n",
    "    print(img.shape)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 搭建模型。我们搭建一个类似于 LeNet-5 的网络，结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-82eabb4c17e90d467197d013f7629f3c_720w.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要创建2个卷积层、2个汇聚层（池化层）和2个全连接层，**暂时忽略所有的激活函数**。所有隐藏层的函数细节都可以在[官方文档](https://pytorch.org/docs/stable/nn.html)中按分类找到。每一个隐藏层本质上都是将一个数组变换成另一个数组的函数，因此为了确认编写的模型是正确的，可以先用一个小数据进行测试，观察输入和输出的维度。例如，我们先取出前10个观测，此时输入的维度是 `[10, 1, 28, 28]`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "smallx = x[0:10]\n",
    "smally = y[0:10]\n",
    "print(smallx.shape)\n",
    "print(smally.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建第1个卷积层，并测试输出的维度。注意到我们可以直接将隐藏层当成一个函数来调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "conv1 = torch.nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "res = conv1(smallx)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，输出的维度为 `[20, 24, 24]`（不包括第1位的数据批次维度），与之前图中的结果吻合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，请按照图中提示编写层对象 `pool1`、`conv2`、`pool2`、`fc1` 和 `fc2`，并顺次测试输入与输出的维度，使其与上图匹配。注意，在将一个大小为 `[10, 50, 4, 4]` 的数组（假设叫 `somearray`）传递给 `fc1` 之前，需要先将其变形为只有两个维度的数组，做法是 `somearray.view(-1, 50*4*4)`，其中 -1 表示该位置的大小不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 12, 12])\n",
      "torch.Size([10, 50, 8, 8])\n",
      "torch.Size([10, 50, 4, 4])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "res = pool1(res)\n",
    "print(res.shape)\n",
    "\n",
    "conv2 = torch.nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "res = conv2(res)\n",
    "print(res.shape)\n",
    "\n",
    "pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "res = pool2(res)\n",
    "print(res.shape)\n",
    "\n",
    "fc1 = torch.nn.Linear(50 * 4 * 4, 500)\n",
    "res = fc1(res.view(-1, 50 * 4 * 4))\n",
    "print(res.shape)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "res = relu(res)\n",
    "print(res.shape)\n",
    "\n",
    "fc2 = torch.nn.Linear(500, 10)\n",
    "res = fc2(res)\n",
    "print(res.shape)\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "res = softmax(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建模型类。在确保隐藏层维度都正确后，将所有的隐藏层封装到一个模型类中，其中模型结构在 `__init__()` 中定义，具体的计算过程在 `forward()` 中实现。此时需要加入激活函数。在本模型中，**请在 `conv1`、`conv2` 和 `fc1` 后加入 ReLU 激活函数，并在 `fc2` 后加入 Softmax 激活函数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(50 * 4 * 4, 500)\n",
    "        self.fc2 = torch.nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.fc1(x.view(-1, 50 * 4 * 4))\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次测试输入输出的维度是否正确。如果模型编写正确，输出的维度应该是 `[10, 10]`，且输出结果为0到1之间的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "tensor([[0.0992, 0.0989, 0.0982, 0.1089, 0.0943, 0.0912, 0.0996, 0.1020, 0.1021,\n",
      "         0.1056],\n",
      "        [0.0997, 0.1004, 0.0978, 0.1085, 0.0938, 0.0907, 0.0982, 0.1045, 0.1020,\n",
      "         0.1045],\n",
      "        [0.0990, 0.0993, 0.0979, 0.1081, 0.0913, 0.0921, 0.0976, 0.1088, 0.1030,\n",
      "         0.1031],\n",
      "        [0.0955, 0.0996, 0.1010, 0.1081, 0.0931, 0.0913, 0.0988, 0.1044, 0.1031,\n",
      "         0.1051],\n",
      "        [0.0995, 0.0988, 0.0985, 0.1091, 0.0919, 0.0918, 0.0980, 0.1059, 0.1017,\n",
      "         0.1049],\n",
      "        [0.0977, 0.0994, 0.0996, 0.1073, 0.0924, 0.0922, 0.1018, 0.1019, 0.1040,\n",
      "         0.1038],\n",
      "        [0.0972, 0.1011, 0.1010, 0.1062, 0.0931, 0.0908, 0.1006, 0.1048, 0.1037,\n",
      "         0.1016],\n",
      "        [0.0996, 0.0984, 0.0987, 0.1068, 0.0904, 0.0900, 0.0992, 0.1076, 0.1020,\n",
      "         0.1075],\n",
      "        [0.0972, 0.0998, 0.1012, 0.1084, 0.0925, 0.0905, 0.1022, 0.1014, 0.1023,\n",
      "         0.1045],\n",
      "        [0.0979, 0.0998, 0.0985, 0.1080, 0.0959, 0.0910, 0.0979, 0.1049, 0.1027,\n",
      "         0.1033]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([7, 4, 5, 4, 0, 7, 1, 0, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = MyModel()\n",
    "pred = model(smallx)\n",
    "print(pred.shape)\n",
    "print(pred)\n",
    "print(smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred` 的每一行加总为1，其中每一个元素代表对应类别的预测概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以直接打印模型对象，观察隐藏层的结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 损失函数。对于分类问题，损失函数通常选取为负对数似然函数。在 PyTorch 中，可以使用 `torch.nn.NLLLoss` 来完成计算。其用法是先定义一个损失函数对象，然后在预测值和真实标签上调用该函数对象。注意：损失函数对象的第一个参数是预测概率的**对数值**，第二个参数是真实的标签。[文档说明](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3231, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn = torch.nn.NLLLoss()\n",
    "lossfn(torch.log(pred), smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 利用课上介绍的循环模板和代码示例，对模型进行迭代训练。对于本数据，选取 mini-batch 大小为200，共遍历数据10遍，优化器选为 Adam，学习率为0.001。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 1, loss = 2.3062708377838135\n",
      "epoch 0, batch 2, loss = 2.2662487030029297\n",
      "epoch 0, batch 3, loss = 2.217352867126465\n",
      "epoch 0, batch 4, loss = 2.156526565551758\n",
      "epoch 0, batch 5, loss = 2.0445191860198975\n",
      "epoch 0, batch 6, loss = 1.9561676979064941\n",
      "epoch 0, batch 7, loss = 1.8460077047348022\n",
      "epoch 0, batch 8, loss = 1.7074928283691406\n",
      "epoch 0, batch 9, loss = 1.595689296722412\n",
      "epoch 0, batch 10, loss = 1.3743484020233154\n",
      "epoch 0, batch 11, loss = 1.2930766344070435\n",
      "epoch 0, batch 12, loss = 1.2297788858413696\n",
      "epoch 0, batch 13, loss = 1.0254333019256592\n",
      "epoch 0, batch 14, loss = 0.9789087176322937\n",
      "epoch 0, batch 15, loss = 0.8862656354904175\n",
      "epoch 0, batch 16, loss = 0.7562286257743835\n",
      "epoch 0, batch 17, loss = 0.8120880722999573\n",
      "epoch 0, batch 18, loss = 0.6880616545677185\n",
      "epoch 0, batch 19, loss = 0.6391095519065857\n",
      "epoch 0, batch 20, loss = 0.6934168934822083\n",
      "epoch 0, batch 21, loss = 0.5670079588890076\n",
      "epoch 0, batch 22, loss = 0.4876691401004791\n",
      "epoch 0, batch 23, loss = 0.49494481086730957\n",
      "epoch 0, batch 24, loss = 0.5368302464485168\n",
      "epoch 0, batch 25, loss = 0.5332321524620056\n",
      "epoch 0, batch 26, loss = 0.45893532037734985\n",
      "epoch 0, batch 27, loss = 0.6689633727073669\n",
      "epoch 0, batch 28, loss = 0.41956210136413574\n",
      "epoch 0, batch 29, loss = 0.4515537917613983\n",
      "epoch 0, batch 30, loss = 0.41342514753341675\n",
      "epoch 0, batch 31, loss = 0.5369466543197632\n",
      "epoch 0, batch 32, loss = 0.32673388719558716\n",
      "epoch 0, batch 33, loss = 0.4598439335823059\n",
      "epoch 0, batch 34, loss = 0.4351347088813782\n",
      "epoch 0, batch 35, loss = 0.3957277238368988\n",
      "epoch 0, batch 36, loss = 0.4207669794559479\n",
      "epoch 0, batch 37, loss = 0.3906921148300171\n",
      "epoch 0, batch 38, loss = 0.41566818952560425\n",
      "epoch 0, batch 39, loss = 0.3088740408420563\n",
      "epoch 0, batch 40, loss = 0.34124183654785156\n",
      "epoch 0, batch 41, loss = 0.32169830799102783\n",
      "epoch 0, batch 42, loss = 0.3168512284755707\n",
      "epoch 0, batch 43, loss = 0.4221283793449402\n",
      "epoch 0, batch 44, loss = 0.30699804425239563\n",
      "epoch 0, batch 45, loss = 0.29573798179626465\n",
      "epoch 0, batch 46, loss = 0.23625831305980682\n",
      "epoch 0, batch 47, loss = 0.22725297510623932\n",
      "epoch 0, batch 48, loss = 0.26540255546569824\n",
      "epoch 0, batch 49, loss = 0.24800966680049896\n",
      "epoch 0, batch 50, loss = 0.403260737657547\n",
      "epoch 0, batch 51, loss = 0.294903963804245\n",
      "epoch 0, batch 52, loss = 0.3435552716255188\n",
      "epoch 0, batch 53, loss = 0.31746184825897217\n",
      "epoch 0, batch 54, loss = 0.2752275764942169\n",
      "epoch 0, batch 55, loss = 0.2747398316860199\n",
      "epoch 0, batch 56, loss = 0.28251913189888\n",
      "epoch 0, batch 57, loss = 0.20410412549972534\n",
      "epoch 0, batch 58, loss = 0.40107783675193787\n",
      "epoch 0, batch 59, loss = 0.20453234016895294\n",
      "epoch 0, batch 60, loss = 0.35798537731170654\n",
      "epoch 0, batch 61, loss = 0.24193458259105682\n",
      "epoch 0, batch 62, loss = 0.20388808846473694\n",
      "epoch 0, batch 63, loss = 0.15370406210422516\n",
      "epoch 0, batch 64, loss = 0.2566589117050171\n",
      "epoch 0, batch 65, loss = 0.2364710569381714\n",
      "epoch 0, batch 66, loss = 0.2511358857154846\n",
      "epoch 0, batch 67, loss = 0.23293094336986542\n",
      "epoch 0, batch 68, loss = 0.22939178347587585\n",
      "epoch 0, batch 69, loss = 0.2200823277235031\n",
      "epoch 0, batch 70, loss = 0.1899711638689041\n",
      "epoch 0, batch 71, loss = 0.2777894139289856\n",
      "epoch 0, batch 72, loss = 0.14571085572242737\n",
      "epoch 0, batch 73, loss = 0.18971234560012817\n",
      "epoch 0, batch 74, loss = 0.1939767301082611\n",
      "epoch 0, batch 75, loss = 0.1974717527627945\n",
      "epoch 0, batch 76, loss = 0.1568111628293991\n",
      "epoch 0, batch 77, loss = 0.1818723827600479\n",
      "epoch 0, batch 78, loss = 0.21635429561138153\n",
      "epoch 0, batch 79, loss = 0.23256437480449677\n",
      "epoch 0, batch 80, loss = 0.11696405708789825\n",
      "epoch 0, batch 81, loss = 0.15916334092617035\n",
      "epoch 0, batch 82, loss = 0.20361287891864777\n",
      "epoch 0, batch 83, loss = 0.22859641909599304\n",
      "epoch 0, batch 84, loss = 0.17491839826107025\n",
      "epoch 0, batch 85, loss = 0.16175289452075958\n",
      "epoch 0, batch 86, loss = 0.21921318769454956\n",
      "epoch 0, batch 87, loss = 0.19688086211681366\n",
      "epoch 0, batch 88, loss = 0.17826960980892181\n",
      "epoch 0, batch 89, loss = 0.19893333315849304\n",
      "epoch 0, batch 90, loss = 0.14742080867290497\n",
      "epoch 0, batch 91, loss = 0.13311660289764404\n",
      "epoch 0, batch 92, loss = 0.1479545384645462\n",
      "epoch 0, batch 93, loss = 0.21008513867855072\n",
      "epoch 0, batch 94, loss = 0.2573048770427704\n",
      "epoch 0, batch 95, loss = 0.2011006772518158\n",
      "epoch 0, batch 96, loss = 0.24258847534656525\n",
      "epoch 0, batch 97, loss = 0.17715463042259216\n",
      "epoch 0, batch 98, loss = 0.1967153698205948\n",
      "epoch 0, batch 99, loss = 0.21393577754497528\n",
      "epoch 0, batch 100, loss = 0.14604058861732483\n",
      "epoch 0, batch 101, loss = 0.22050964832305908\n",
      "epoch 0, batch 102, loss = 0.12691524624824524\n",
      "epoch 0, batch 103, loss = 0.27334877848625183\n",
      "epoch 0, batch 104, loss = 0.2042587697505951\n",
      "epoch 0, batch 105, loss = 0.11476544290781021\n",
      "epoch 0, batch 106, loss = 0.1398109793663025\n",
      "epoch 0, batch 107, loss = 0.3071812391281128\n",
      "epoch 0, batch 108, loss = 0.20678867399692535\n",
      "epoch 0, batch 109, loss = 0.14415991306304932\n",
      "epoch 0, batch 110, loss = 0.11629557609558105\n",
      "epoch 0, batch 111, loss = 0.15270677208900452\n",
      "epoch 0, batch 112, loss = 0.16617488861083984\n",
      "epoch 0, batch 113, loss = 0.1612195372581482\n",
      "epoch 0, batch 114, loss = 0.18569828569889069\n",
      "epoch 0, batch 115, loss = 0.14015129208564758\n",
      "epoch 0, batch 116, loss = 0.20452548563480377\n",
      "epoch 0, batch 117, loss = 0.18715812265872955\n",
      "epoch 0, batch 118, loss = 0.09728574752807617\n",
      "epoch 0, batch 119, loss = 0.09844048321247101\n",
      "epoch 0, batch 120, loss = 0.10737375169992447\n",
      "epoch 0, batch 121, loss = 0.15469735860824585\n",
      "epoch 0, batch 122, loss = 0.18760377168655396\n",
      "epoch 0, batch 123, loss = 0.08570360392332077\n",
      "epoch 0, batch 124, loss = 0.10082390904426575\n",
      "epoch 0, batch 125, loss = 0.0951743870973587\n",
      "epoch 0, batch 126, loss = 0.13330315053462982\n",
      "epoch 0, batch 127, loss = 0.10725545138120651\n",
      "epoch 0, batch 128, loss = 0.17716176807880402\n",
      "epoch 0, batch 129, loss = 0.07738257199525833\n",
      "epoch 0, batch 130, loss = 0.1179819330573082\n",
      "epoch 0, batch 131, loss = 0.10017099976539612\n",
      "epoch 0, batch 132, loss = 0.12826775014400482\n",
      "epoch 0, batch 133, loss = 0.11106723546981812\n",
      "epoch 0, batch 134, loss = 0.13559867441654205\n",
      "epoch 0, batch 135, loss = 0.11260969191789627\n",
      "epoch 0, batch 136, loss = 0.14254385232925415\n",
      "epoch 0, batch 137, loss = 0.12864387035369873\n",
      "epoch 0, batch 138, loss = 0.14744903147220612\n",
      "epoch 0, batch 139, loss = 0.11887911707162857\n",
      "epoch 0, batch 140, loss = 0.1924353986978531\n",
      "epoch 0, batch 141, loss = 0.11412987112998962\n",
      "epoch 0, batch 142, loss = 0.1371350884437561\n",
      "epoch 0, batch 143, loss = 0.08984468132257462\n",
      "epoch 0, batch 144, loss = 0.10562669485807419\n",
      "epoch 0, batch 145, loss = 0.11828409880399704\n",
      "epoch 0, batch 146, loss = 0.12273185700178146\n",
      "epoch 0, batch 147, loss = 0.1186237558722496\n",
      "epoch 0, batch 148, loss = 0.18279273808002472\n",
      "epoch 0, batch 149, loss = 0.1420104056596756\n",
      "epoch 0, batch 150, loss = 0.11128678172826767\n",
      "epoch 0, batch 151, loss = 0.1768038123846054\n",
      "epoch 0, batch 152, loss = 0.050825268030166626\n",
      "epoch 0, batch 153, loss = 0.08631287515163422\n",
      "epoch 0, batch 154, loss = 0.1383943259716034\n",
      "epoch 0, batch 155, loss = 0.14939865469932556\n",
      "epoch 0, batch 156, loss = 0.11913326382637024\n",
      "epoch 0, batch 157, loss = 0.18603086471557617\n",
      "epoch 0, batch 158, loss = 0.13055077195167542\n",
      "epoch 0, batch 159, loss = 0.1157616674900055\n",
      "epoch 0, batch 160, loss = 0.08598930388689041\n",
      "epoch 0, batch 161, loss = 0.10109203308820724\n",
      "epoch 0, batch 162, loss = 0.13405334949493408\n",
      "epoch 0, batch 163, loss = 0.09761032462120056\n",
      "epoch 0, batch 164, loss = 0.09405449777841568\n",
      "epoch 0, batch 165, loss = 0.08012539148330688\n",
      "epoch 0, batch 166, loss = 0.07033656537532806\n",
      "epoch 0, batch 167, loss = 0.1277945339679718\n",
      "epoch 0, batch 168, loss = 0.05911718308925629\n",
      "epoch 0, batch 169, loss = 0.0896630585193634\n",
      "epoch 0, batch 170, loss = 0.08193781226873398\n",
      "epoch 0, batch 171, loss = 0.1085362359881401\n",
      "epoch 0, batch 172, loss = 0.04663095623254776\n",
      "epoch 0, batch 173, loss = 0.15920063853263855\n",
      "epoch 0, batch 174, loss = 0.11444271355867386\n",
      "epoch 0, batch 175, loss = 0.17252594232559204\n",
      "epoch 0, batch 176, loss = 0.10964465886354446\n",
      "epoch 0, batch 177, loss = 0.03453188017010689\n",
      "epoch 0, batch 178, loss = 0.07611694186925888\n",
      "epoch 0, batch 179, loss = 0.1141807809472084\n",
      "epoch 0, batch 180, loss = 0.18344055116176605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 181, loss = 0.06629195809364319\n",
      "epoch 0, batch 182, loss = 0.07971838116645813\n",
      "epoch 0, batch 183, loss = 0.10388351231813431\n",
      "epoch 0, batch 184, loss = 0.09996087104082108\n",
      "epoch 0, batch 185, loss = 0.07845267653465271\n",
      "epoch 0, batch 186, loss = 0.04842749238014221\n",
      "epoch 0, batch 187, loss = 0.0733163133263588\n",
      "epoch 0, batch 188, loss = 0.140195831656456\n",
      "epoch 0, batch 189, loss = 0.10690390318632126\n",
      "epoch 0, batch 190, loss = 0.06771335750818253\n",
      "epoch 0, batch 191, loss = 0.051959529519081116\n",
      "epoch 0, batch 192, loss = 0.12803632020950317\n",
      "epoch 0, batch 193, loss = 0.0887066051363945\n",
      "epoch 0, batch 194, loss = 0.06522863358259201\n",
      "epoch 0, batch 195, loss = 0.14818891882896423\n",
      "epoch 0, batch 196, loss = 0.07969710230827332\n",
      "epoch 0, batch 197, loss = 0.13492965698242188\n",
      "epoch 0, batch 198, loss = 0.08202555030584335\n",
      "epoch 0, batch 199, loss = 0.04681164398789406\n",
      "epoch 1, batch 1, loss = 0.08927862346172333\n",
      "epoch 1, batch 2, loss = 0.04046344384551048\n",
      "epoch 1, batch 3, loss = 0.07164601981639862\n",
      "epoch 1, batch 4, loss = 0.038762371987104416\n",
      "epoch 1, batch 5, loss = 0.038220569491386414\n",
      "epoch 1, batch 6, loss = 0.12550988793373108\n",
      "epoch 1, batch 7, loss = 0.09319312870502472\n",
      "epoch 1, batch 8, loss = 0.1006212830543518\n",
      "epoch 1, batch 9, loss = 0.06676318496465683\n",
      "epoch 1, batch 10, loss = 0.05124830827116966\n",
      "epoch 1, batch 11, loss = 0.10378851741552353\n",
      "epoch 1, batch 12, loss = 0.10209045559167862\n",
      "epoch 1, batch 13, loss = 0.04147971048951149\n",
      "epoch 1, batch 14, loss = 0.03780880570411682\n",
      "epoch 1, batch 15, loss = 0.08836674690246582\n",
      "epoch 1, batch 16, loss = 0.042660053819417953\n",
      "epoch 1, batch 17, loss = 0.03476271778345108\n",
      "epoch 1, batch 18, loss = 0.05226915329694748\n",
      "epoch 1, batch 19, loss = 0.1250706911087036\n",
      "epoch 1, batch 20, loss = 0.07011720538139343\n",
      "epoch 1, batch 21, loss = 0.07051364332437515\n",
      "epoch 1, batch 22, loss = 0.05908776819705963\n",
      "epoch 1, batch 23, loss = 0.02645520493388176\n",
      "epoch 1, batch 24, loss = 0.042059965431690216\n",
      "epoch 1, batch 25, loss = 0.10921376943588257\n",
      "epoch 1, batch 26, loss = 0.03934653103351593\n",
      "epoch 1, batch 27, loss = 0.06705513596534729\n",
      "epoch 1, batch 28, loss = 0.05638689920306206\n",
      "epoch 1, batch 29, loss = 0.06248670071363449\n",
      "epoch 1, batch 30, loss = 0.14605194330215454\n",
      "epoch 1, batch 31, loss = 0.025606758892536163\n",
      "epoch 1, batch 32, loss = 0.035157572478055954\n",
      "epoch 1, batch 33, loss = 0.058721646666526794\n",
      "epoch 1, batch 34, loss = 0.02853524126112461\n",
      "epoch 1, batch 35, loss = 0.05265476554632187\n",
      "epoch 1, batch 36, loss = 0.04051194339990616\n",
      "epoch 1, batch 37, loss = 0.08796069025993347\n",
      "epoch 1, batch 38, loss = 0.11250728368759155\n",
      "epoch 1, batch 39, loss = 0.031766802072525024\n",
      "epoch 1, batch 40, loss = 0.08329732716083527\n",
      "epoch 1, batch 41, loss = 0.051079560071229935\n",
      "epoch 1, batch 42, loss = 0.08018819987773895\n",
      "epoch 1, batch 43, loss = 0.12520301342010498\n",
      "epoch 1, batch 44, loss = 0.049619074910879135\n",
      "epoch 1, batch 45, loss = 0.04948766157031059\n",
      "epoch 1, batch 46, loss = 0.053578633815050125\n",
      "epoch 1, batch 47, loss = 0.07623933255672455\n",
      "epoch 1, batch 48, loss = 0.057684797793626785\n",
      "epoch 1, batch 49, loss = 0.08390326797962189\n",
      "epoch 1, batch 50, loss = 0.12133019417524338\n",
      "epoch 1, batch 51, loss = 0.06931807100772858\n",
      "epoch 1, batch 52, loss = 0.10732545703649521\n",
      "epoch 1, batch 53, loss = 0.027578193694353104\n",
      "epoch 1, batch 54, loss = 0.05046037584543228\n",
      "epoch 1, batch 55, loss = 0.032972633838653564\n",
      "epoch 1, batch 56, loss = 0.10177526623010635\n",
      "epoch 1, batch 57, loss = 0.0860477164387703\n",
      "epoch 1, batch 58, loss = 0.02072994038462639\n",
      "epoch 1, batch 59, loss = 0.08011435717344284\n",
      "epoch 1, batch 60, loss = 0.08168084919452667\n",
      "epoch 1, batch 61, loss = 0.058411236852407455\n",
      "epoch 1, batch 62, loss = 0.08691324293613434\n",
      "epoch 1, batch 63, loss = 0.06550332903862\n",
      "epoch 1, batch 64, loss = 0.05601317435503006\n",
      "epoch 1, batch 65, loss = 0.04723091050982475\n",
      "epoch 1, batch 66, loss = 0.12850289046764374\n",
      "epoch 1, batch 67, loss = 0.046626023948192596\n",
      "epoch 1, batch 68, loss = 0.04519705846905708\n",
      "epoch 1, batch 69, loss = 0.056891538202762604\n",
      "epoch 1, batch 70, loss = 0.0643308162689209\n",
      "epoch 1, batch 71, loss = 0.026408467441797256\n",
      "epoch 1, batch 72, loss = 0.036354608833789825\n",
      "epoch 1, batch 73, loss = 0.024070344865322113\n",
      "epoch 1, batch 74, loss = 0.08752059191465378\n",
      "epoch 1, batch 75, loss = 0.052340272814035416\n",
      "epoch 1, batch 76, loss = 0.08051899820566177\n",
      "epoch 1, batch 77, loss = 0.04900078848004341\n",
      "epoch 1, batch 78, loss = 0.0445467010140419\n",
      "epoch 1, batch 79, loss = 0.036222074180841446\n",
      "epoch 1, batch 80, loss = 0.02522539161145687\n",
      "epoch 1, batch 81, loss = 0.09008049964904785\n",
      "epoch 1, batch 82, loss = 0.04487474262714386\n",
      "epoch 1, batch 83, loss = 0.055723611265420914\n",
      "epoch 1, batch 84, loss = 0.02863026224076748\n",
      "epoch 1, batch 85, loss = 0.10788523405790329\n",
      "epoch 1, batch 86, loss = 0.05371277406811714\n",
      "epoch 1, batch 87, loss = 0.0481996089220047\n",
      "epoch 1, batch 88, loss = 0.06348148733377457\n",
      "epoch 1, batch 89, loss = 0.0763779804110527\n",
      "epoch 1, batch 90, loss = 0.06811463832855225\n",
      "epoch 1, batch 91, loss = 0.07179218530654907\n",
      "epoch 1, batch 92, loss = 0.10908064246177673\n",
      "epoch 1, batch 93, loss = 0.029055984690785408\n",
      "epoch 1, batch 94, loss = 0.03777679055929184\n",
      "epoch 1, batch 95, loss = 0.03575460985302925\n",
      "epoch 1, batch 96, loss = 0.05916629359126091\n",
      "epoch 1, batch 97, loss = 0.0583382211625576\n",
      "epoch 1, batch 98, loss = 0.07844386249780655\n",
      "epoch 1, batch 99, loss = 0.037134915590286255\n",
      "epoch 1, batch 100, loss = 0.04719921573996544\n",
      "epoch 1, batch 101, loss = 0.03564470261335373\n",
      "epoch 1, batch 102, loss = 0.03639117255806923\n",
      "epoch 1, batch 103, loss = 0.0737547054886818\n",
      "epoch 1, batch 104, loss = 0.05326749384403229\n",
      "epoch 1, batch 105, loss = 0.03663737326860428\n",
      "epoch 1, batch 106, loss = 0.02295231632888317\n",
      "epoch 1, batch 107, loss = 0.06983306258916855\n",
      "epoch 1, batch 108, loss = 0.08318550139665604\n",
      "epoch 1, batch 109, loss = 0.06209883093833923\n",
      "epoch 1, batch 110, loss = 0.061813678592443466\n",
      "epoch 1, batch 111, loss = 0.042300015687942505\n",
      "epoch 1, batch 112, loss = 0.14788104593753815\n",
      "epoch 1, batch 113, loss = 0.0402936190366745\n",
      "epoch 1, batch 114, loss = 0.05635647475719452\n",
      "epoch 1, batch 115, loss = 0.04001389071345329\n",
      "epoch 1, batch 116, loss = 0.03666490688920021\n",
      "epoch 1, batch 117, loss = 0.04630642384290695\n",
      "epoch 1, batch 118, loss = 0.08333144336938858\n",
      "epoch 1, batch 119, loss = 0.04407307133078575\n",
      "epoch 1, batch 120, loss = 0.05819088965654373\n",
      "epoch 1, batch 121, loss = 0.06919882446527481\n",
      "epoch 1, batch 122, loss = 0.08421072363853455\n",
      "epoch 1, batch 123, loss = 0.05851689353585243\n",
      "epoch 1, batch 124, loss = 0.04929385706782341\n",
      "epoch 1, batch 125, loss = 0.09438861161470413\n",
      "epoch 1, batch 126, loss = 0.06768009066581726\n",
      "epoch 1, batch 127, loss = 0.0983128771185875\n",
      "epoch 1, batch 128, loss = 0.08663074672222137\n",
      "epoch 1, batch 129, loss = 0.022030169144272804\n",
      "epoch 1, batch 130, loss = 0.0370330773293972\n",
      "epoch 1, batch 131, loss = 0.04310254007577896\n",
      "epoch 1, batch 132, loss = 0.10284644365310669\n",
      "epoch 1, batch 133, loss = 0.044333506375551224\n",
      "epoch 1, batch 134, loss = 0.05117473751306534\n",
      "epoch 1, batch 135, loss = 0.07924956828355789\n",
      "epoch 1, batch 136, loss = 0.04697651416063309\n",
      "epoch 1, batch 137, loss = 0.057410284876823425\n",
      "epoch 1, batch 138, loss = 0.03439805656671524\n",
      "epoch 1, batch 139, loss = 0.01822442188858986\n",
      "epoch 1, batch 140, loss = 0.05610218644142151\n",
      "epoch 1, batch 141, loss = 0.043606650084257126\n",
      "epoch 1, batch 142, loss = 0.07059423625469208\n",
      "epoch 1, batch 143, loss = 0.045571520924568176\n",
      "epoch 1, batch 144, loss = 0.16773128509521484\n",
      "epoch 1, batch 145, loss = 0.07126489281654358\n",
      "epoch 1, batch 146, loss = 0.023341048508882523\n",
      "epoch 1, batch 147, loss = 0.06625818461179733\n",
      "epoch 1, batch 148, loss = 0.07971657067537308\n",
      "epoch 1, batch 149, loss = 0.06312116980552673\n",
      "epoch 1, batch 150, loss = 0.03350159525871277\n",
      "epoch 1, batch 151, loss = 0.06271279603242874\n",
      "epoch 1, batch 152, loss = 0.015293688513338566\n",
      "epoch 1, batch 153, loss = 0.04849739372730255\n",
      "epoch 1, batch 154, loss = 0.05090567469596863\n",
      "epoch 1, batch 155, loss = 0.05478980019688606\n",
      "epoch 1, batch 156, loss = 0.053948670625686646\n",
      "epoch 1, batch 157, loss = 0.04115690290927887\n",
      "epoch 1, batch 158, loss = 0.1158645749092102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 159, loss = 0.055864885449409485\n",
      "epoch 1, batch 160, loss = 0.03313896805047989\n",
      "epoch 1, batch 161, loss = 0.10791648179292679\n",
      "epoch 1, batch 162, loss = 0.06980407983064651\n",
      "epoch 1, batch 163, loss = 0.07742577791213989\n",
      "epoch 1, batch 164, loss = 0.07209393382072449\n",
      "epoch 1, batch 165, loss = 0.07283291220664978\n",
      "epoch 1, batch 166, loss = 0.03793560341000557\n",
      "epoch 1, batch 167, loss = 0.031333278864622116\n",
      "epoch 1, batch 168, loss = 0.027600280940532684\n",
      "epoch 1, batch 169, loss = 0.03445244953036308\n",
      "epoch 1, batch 170, loss = 0.04090753197669983\n",
      "epoch 1, batch 171, loss = 0.11447129398584366\n",
      "epoch 1, batch 172, loss = 0.07979805022478104\n",
      "epoch 1, batch 173, loss = 0.0435931496322155\n",
      "epoch 1, batch 174, loss = 0.026894161477684975\n",
      "epoch 1, batch 175, loss = 0.06435298174619675\n",
      "epoch 1, batch 176, loss = 0.0667998269200325\n",
      "epoch 1, batch 177, loss = 0.11154774576425552\n",
      "epoch 1, batch 178, loss = 0.04279303178191185\n",
      "epoch 1, batch 179, loss = 0.036438655108213425\n",
      "epoch 1, batch 180, loss = 0.028357993811368942\n",
      "epoch 1, batch 181, loss = 0.0815996378660202\n",
      "epoch 1, batch 182, loss = 0.03159407898783684\n",
      "epoch 1, batch 183, loss = 0.03492604196071625\n",
      "epoch 1, batch 184, loss = 0.07460486888885498\n",
      "epoch 1, batch 185, loss = 0.040300510823726654\n",
      "epoch 1, batch 186, loss = 0.09103579819202423\n",
      "epoch 1, batch 187, loss = 0.0353471115231514\n",
      "epoch 1, batch 188, loss = 0.04395736753940582\n",
      "epoch 1, batch 189, loss = 0.042791176587343216\n",
      "epoch 1, batch 190, loss = 0.042303845286369324\n",
      "epoch 1, batch 191, loss = 0.0657975971698761\n",
      "epoch 1, batch 192, loss = 0.04173325002193451\n",
      "epoch 1, batch 193, loss = 0.07579034566879272\n",
      "epoch 1, batch 194, loss = 0.02704308182001114\n",
      "epoch 1, batch 195, loss = 0.10108103603124619\n",
      "epoch 1, batch 196, loss = 0.04257215932011604\n",
      "epoch 1, batch 197, loss = 0.04982293024659157\n",
      "epoch 1, batch 198, loss = 0.11039260774850845\n",
      "epoch 1, batch 199, loss = 0.062260065227746964\n",
      "epoch 2, batch 1, loss = 0.015582595020532608\n",
      "epoch 2, batch 2, loss = 0.047363586723804474\n",
      "epoch 2, batch 3, loss = 0.0320681557059288\n",
      "epoch 2, batch 4, loss = 0.016299378126859665\n",
      "epoch 2, batch 5, loss = 0.03270665556192398\n",
      "epoch 2, batch 6, loss = 0.06447400152683258\n",
      "epoch 2, batch 7, loss = 0.07270004600286484\n",
      "epoch 2, batch 8, loss = 0.025709399953484535\n",
      "epoch 2, batch 9, loss = 0.053888678550720215\n",
      "epoch 2, batch 10, loss = 0.029771847650408745\n",
      "epoch 2, batch 11, loss = 0.05916571244597435\n",
      "epoch 2, batch 12, loss = 0.05019637942314148\n",
      "epoch 2, batch 13, loss = 0.04124227538704872\n",
      "epoch 2, batch 14, loss = 0.059092067182064056\n",
      "epoch 2, batch 15, loss = 0.0660933256149292\n",
      "epoch 2, batch 16, loss = 0.02659612149000168\n",
      "epoch 2, batch 17, loss = 0.019727855920791626\n",
      "epoch 2, batch 18, loss = 0.07663942128419876\n",
      "epoch 2, batch 19, loss = 0.06332836300134659\n",
      "epoch 2, batch 20, loss = 0.08807060122489929\n",
      "epoch 2, batch 21, loss = 0.015303649008274078\n",
      "epoch 2, batch 22, loss = 0.025879094377160072\n",
      "epoch 2, batch 23, loss = 0.0347772091627121\n",
      "epoch 2, batch 24, loss = 0.022402167320251465\n",
      "epoch 2, batch 25, loss = 0.024050021544098854\n",
      "epoch 2, batch 26, loss = 0.021833386272192\n",
      "epoch 2, batch 27, loss = 0.06098385155200958\n",
      "epoch 2, batch 28, loss = 0.04694473743438721\n",
      "epoch 2, batch 29, loss = 0.04279984533786774\n",
      "epoch 2, batch 30, loss = 0.014203221537172794\n",
      "epoch 2, batch 31, loss = 0.01684163324534893\n",
      "epoch 2, batch 32, loss = 0.03596964478492737\n",
      "epoch 2, batch 33, loss = 0.05557804927229881\n",
      "epoch 2, batch 34, loss = 0.09247014671564102\n",
      "epoch 2, batch 35, loss = 0.02132461965084076\n",
      "epoch 2, batch 36, loss = 0.0902113988995552\n",
      "epoch 2, batch 37, loss = 0.02065863087773323\n",
      "epoch 2, batch 38, loss = 0.03531407192349434\n",
      "epoch 2, batch 39, loss = 0.018032880499958992\n",
      "epoch 2, batch 40, loss = 0.030674373731017113\n",
      "epoch 2, batch 41, loss = 0.03379165381193161\n",
      "epoch 2, batch 42, loss = 0.026698686182498932\n",
      "epoch 2, batch 43, loss = 0.11304613202810287\n",
      "epoch 2, batch 44, loss = 0.03320559114217758\n",
      "epoch 2, batch 45, loss = 0.055581726133823395\n",
      "epoch 2, batch 46, loss = 0.01078744512051344\n",
      "epoch 2, batch 47, loss = 0.023501895368099213\n",
      "epoch 2, batch 48, loss = 0.08485643565654755\n",
      "epoch 2, batch 49, loss = 0.02842547930777073\n",
      "epoch 2, batch 50, loss = 0.03683415427803993\n",
      "epoch 2, batch 51, loss = 0.046923961490392685\n",
      "epoch 2, batch 52, loss = 0.011939375661313534\n",
      "epoch 2, batch 53, loss = 0.06126131862401962\n",
      "epoch 2, batch 54, loss = 0.09130746871232986\n",
      "epoch 2, batch 55, loss = 0.008193019777536392\n",
      "epoch 2, batch 56, loss = 0.03902120143175125\n",
      "epoch 2, batch 57, loss = 0.06355553865432739\n",
      "epoch 2, batch 58, loss = 0.05629467964172363\n",
      "epoch 2, batch 59, loss = 0.06166032329201698\n",
      "epoch 2, batch 60, loss = 0.048432379961013794\n",
      "epoch 2, batch 61, loss = 0.03164202347397804\n",
      "epoch 2, batch 62, loss = 0.03603195771574974\n",
      "epoch 2, batch 63, loss = 0.0568094365298748\n",
      "epoch 2, batch 64, loss = 0.013023171573877335\n",
      "epoch 2, batch 65, loss = 0.06497519463300705\n",
      "epoch 2, batch 66, loss = 0.03546827659010887\n",
      "epoch 2, batch 67, loss = 0.04548392444849014\n",
      "epoch 2, batch 68, loss = 0.04234899953007698\n",
      "epoch 2, batch 69, loss = 0.05960168316960335\n",
      "epoch 2, batch 70, loss = 0.03529639542102814\n",
      "epoch 2, batch 71, loss = 0.016268476843833923\n",
      "epoch 2, batch 72, loss = 0.01843281462788582\n",
      "epoch 2, batch 73, loss = 0.04304025322198868\n",
      "epoch 2, batch 74, loss = 0.03847429156303406\n",
      "epoch 2, batch 75, loss = 0.062333375215530396\n",
      "epoch 2, batch 76, loss = 0.08195391297340393\n",
      "epoch 2, batch 77, loss = 0.0699729472398758\n",
      "epoch 2, batch 78, loss = 0.05847230926156044\n",
      "epoch 2, batch 79, loss = 0.027555368840694427\n",
      "epoch 2, batch 80, loss = 0.07328428328037262\n",
      "epoch 2, batch 81, loss = 0.04661674052476883\n",
      "epoch 2, batch 82, loss = 0.05696311593055725\n",
      "epoch 2, batch 83, loss = 0.05401725694537163\n",
      "epoch 2, batch 84, loss = 0.051269423216581345\n",
      "epoch 2, batch 85, loss = 0.017324453219771385\n",
      "epoch 2, batch 86, loss = 0.05892414227128029\n",
      "epoch 2, batch 87, loss = 0.07133208960294724\n",
      "epoch 2, batch 88, loss = 0.015579048544168472\n",
      "epoch 2, batch 89, loss = 0.016323663294315338\n",
      "epoch 2, batch 90, loss = 0.015214609913527966\n",
      "epoch 2, batch 91, loss = 0.05710567161440849\n",
      "epoch 2, batch 92, loss = 0.06124695762991905\n",
      "epoch 2, batch 93, loss = 0.024474993348121643\n",
      "epoch 2, batch 94, loss = 0.044165391474962234\n",
      "epoch 2, batch 95, loss = 0.045983992516994476\n",
      "epoch 2, batch 96, loss = 0.06820768117904663\n",
      "epoch 2, batch 97, loss = 0.02816159650683403\n",
      "epoch 2, batch 98, loss = 0.028702320531010628\n",
      "epoch 2, batch 99, loss = 0.046356528997421265\n",
      "epoch 2, batch 100, loss = 0.03985140472650528\n",
      "epoch 2, batch 101, loss = 0.03829739987850189\n",
      "epoch 2, batch 102, loss = 0.039128631353378296\n",
      "epoch 2, batch 103, loss = 0.05527720972895622\n",
      "epoch 2, batch 104, loss = 0.041777607053518295\n",
      "epoch 2, batch 105, loss = 0.06102539226412773\n",
      "epoch 2, batch 106, loss = 0.07773027569055557\n",
      "epoch 2, batch 107, loss = 0.02971235103905201\n",
      "epoch 2, batch 108, loss = 0.04257844388484955\n",
      "epoch 2, batch 109, loss = 0.012945476919412613\n",
      "epoch 2, batch 110, loss = 0.031396061182022095\n",
      "epoch 2, batch 111, loss = 0.02503856271505356\n",
      "epoch 2, batch 112, loss = 0.037566713988780975\n",
      "epoch 2, batch 113, loss = 0.08488655090332031\n",
      "epoch 2, batch 114, loss = 0.03554060310125351\n",
      "epoch 2, batch 115, loss = 0.03298131376504898\n",
      "epoch 2, batch 116, loss = 0.0161213930696249\n",
      "epoch 2, batch 117, loss = 0.015417573042213917\n",
      "epoch 2, batch 118, loss = 0.04672640934586525\n",
      "epoch 2, batch 119, loss = 0.042236872017383575\n",
      "epoch 2, batch 120, loss = 0.02295357547700405\n",
      "epoch 2, batch 121, loss = 0.018787872046232224\n",
      "epoch 2, batch 122, loss = 0.05582050234079361\n",
      "epoch 2, batch 123, loss = 0.046006713062524796\n",
      "epoch 2, batch 124, loss = 0.06299734115600586\n",
      "epoch 2, batch 125, loss = 0.05726432800292969\n",
      "epoch 2, batch 126, loss = 0.046287402510643005\n",
      "epoch 2, batch 127, loss = 0.05078372359275818\n",
      "epoch 2, batch 128, loss = 0.08194369077682495\n",
      "epoch 2, batch 129, loss = 0.03739925101399422\n",
      "epoch 2, batch 130, loss = 0.035409584641456604\n",
      "epoch 2, batch 131, loss = 0.03291601315140724\n",
      "epoch 2, batch 132, loss = 0.01591290906071663\n",
      "epoch 2, batch 133, loss = 0.030788114294409752\n",
      "epoch 2, batch 134, loss = 0.05982256308197975\n",
      "epoch 2, batch 135, loss = 0.029927680268883705\n",
      "epoch 2, batch 136, loss = 0.053957499563694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 137, loss = 0.027026213705539703\n",
      "epoch 2, batch 138, loss = 0.04867221415042877\n",
      "epoch 2, batch 139, loss = 0.021680451929569244\n",
      "epoch 2, batch 140, loss = 0.020601162686944008\n",
      "epoch 2, batch 141, loss = 0.049203887581825256\n",
      "epoch 2, batch 142, loss = 0.0136695746332407\n",
      "epoch 2, batch 143, loss = 0.07444575428962708\n",
      "epoch 2, batch 144, loss = 0.023060068488121033\n",
      "epoch 2, batch 145, loss = 0.0644705593585968\n",
      "epoch 2, batch 146, loss = 0.0720953494310379\n",
      "epoch 2, batch 147, loss = 0.052084263414144516\n",
      "epoch 2, batch 148, loss = 0.07969390600919724\n",
      "epoch 2, batch 149, loss = 0.041005924344062805\n",
      "epoch 2, batch 150, loss = 0.037279583513736725\n",
      "epoch 2, batch 151, loss = 0.03731745108962059\n",
      "epoch 2, batch 152, loss = 0.047995977103710175\n",
      "epoch 2, batch 153, loss = 0.04410340264439583\n",
      "epoch 2, batch 154, loss = 0.012542725540697575\n",
      "epoch 2, batch 155, loss = 0.037221573293209076\n",
      "epoch 2, batch 156, loss = 0.04350011795759201\n",
      "epoch 2, batch 157, loss = 0.05626704543828964\n",
      "epoch 2, batch 158, loss = 0.030768664553761482\n",
      "epoch 2, batch 159, loss = 0.13031937181949615\n",
      "epoch 2, batch 160, loss = 0.03108265809714794\n",
      "epoch 2, batch 161, loss = 0.052300915122032166\n",
      "epoch 2, batch 162, loss = 0.042995620518922806\n",
      "epoch 2, batch 163, loss = 0.03670840337872505\n",
      "epoch 2, batch 164, loss = 0.03692883253097534\n",
      "epoch 2, batch 165, loss = 0.08526213467121124\n",
      "epoch 2, batch 166, loss = 0.04803186282515526\n",
      "epoch 2, batch 167, loss = 0.006857031490653753\n",
      "epoch 2, batch 168, loss = 0.041169118136167526\n",
      "epoch 2, batch 169, loss = 0.04944370687007904\n",
      "epoch 2, batch 170, loss = 0.06141720712184906\n",
      "epoch 2, batch 171, loss = 0.05112417787313461\n",
      "epoch 2, batch 172, loss = 0.05123327299952507\n",
      "epoch 2, batch 173, loss = 0.06948228180408478\n",
      "epoch 2, batch 174, loss = 0.05793265998363495\n",
      "epoch 2, batch 175, loss = 0.018232418224215508\n",
      "epoch 2, batch 176, loss = 0.059495959430933\n",
      "epoch 2, batch 177, loss = 0.053215157240629196\n",
      "epoch 2, batch 178, loss = 0.10287889838218689\n",
      "epoch 2, batch 179, loss = 0.0656747817993164\n",
      "epoch 2, batch 180, loss = 0.054777372628450394\n",
      "epoch 2, batch 181, loss = 0.032626260071992874\n",
      "epoch 2, batch 182, loss = 0.04905106499791145\n",
      "epoch 2, batch 183, loss = 0.03262830525636673\n",
      "epoch 2, batch 184, loss = 0.01947355456650257\n",
      "epoch 2, batch 185, loss = 0.047457050532102585\n",
      "epoch 2, batch 186, loss = 0.029879944398999214\n",
      "epoch 2, batch 187, loss = 0.036905352026224136\n",
      "epoch 2, batch 188, loss = 0.03607172891497612\n",
      "epoch 2, batch 189, loss = 0.05361529439687729\n",
      "epoch 2, batch 190, loss = 0.06237855926156044\n",
      "epoch 2, batch 191, loss = 0.08028969168663025\n",
      "epoch 2, batch 192, loss = 0.05032295361161232\n",
      "epoch 2, batch 193, loss = 0.04792136326432228\n",
      "epoch 2, batch 194, loss = 0.06459483504295349\n",
      "epoch 2, batch 195, loss = 0.019216015934944153\n",
      "epoch 2, batch 196, loss = 0.03516145795583725\n",
      "epoch 2, batch 197, loss = 0.04749342054128647\n",
      "epoch 2, batch 198, loss = 0.06302905827760696\n",
      "epoch 2, batch 199, loss = 0.020623354241251945\n",
      "epoch 3, batch 1, loss = 0.017915192991495132\n",
      "epoch 3, batch 2, loss = 0.007282068021595478\n",
      "epoch 3, batch 3, loss = 0.03731047362089157\n",
      "epoch 3, batch 4, loss = 0.02690812386572361\n",
      "epoch 3, batch 5, loss = 0.05518142133951187\n",
      "epoch 3, batch 6, loss = 0.02187608927488327\n",
      "epoch 3, batch 7, loss = 0.03100827895104885\n",
      "epoch 3, batch 8, loss = 0.02605508826673031\n",
      "epoch 3, batch 9, loss = 0.06537142395973206\n",
      "epoch 3, batch 10, loss = 0.04314904659986496\n",
      "epoch 3, batch 11, loss = 0.03392917290329933\n",
      "epoch 3, batch 12, loss = 0.018907181918621063\n",
      "epoch 3, batch 13, loss = 0.004844577517360449\n",
      "epoch 3, batch 14, loss = 0.02095635235309601\n",
      "epoch 3, batch 15, loss = 0.011882994323968887\n",
      "epoch 3, batch 16, loss = 0.013550860807299614\n",
      "epoch 3, batch 17, loss = 0.01511801965534687\n",
      "epoch 3, batch 18, loss = 0.03778546303510666\n",
      "epoch 3, batch 19, loss = 0.058881066739559174\n",
      "epoch 3, batch 20, loss = 0.007278934121131897\n",
      "epoch 3, batch 21, loss = 0.035253629088401794\n",
      "epoch 3, batch 22, loss = 0.01634673774242401\n",
      "epoch 3, batch 23, loss = 0.010464758612215519\n",
      "epoch 3, batch 24, loss = 0.01937299221754074\n",
      "epoch 3, batch 25, loss = 0.013502896763384342\n",
      "epoch 3, batch 26, loss = 0.04099171981215477\n",
      "epoch 3, batch 27, loss = 0.02065356820821762\n",
      "epoch 3, batch 28, loss = 0.011634862050414085\n",
      "epoch 3, batch 29, loss = 0.045158710330724716\n",
      "epoch 3, batch 30, loss = 0.03631444275379181\n",
      "epoch 3, batch 31, loss = 0.036326225847005844\n",
      "epoch 3, batch 32, loss = 0.04179879650473595\n",
      "epoch 3, batch 33, loss = 0.0341905876994133\n",
      "epoch 3, batch 34, loss = 0.01290915161371231\n",
      "epoch 3, batch 35, loss = 0.044133201241493225\n",
      "epoch 3, batch 36, loss = 0.03154800832271576\n",
      "epoch 3, batch 37, loss = 0.05059976875782013\n",
      "epoch 3, batch 38, loss = 0.06249959394335747\n",
      "epoch 3, batch 39, loss = 0.014163803309202194\n",
      "epoch 3, batch 40, loss = 0.018711643293499947\n",
      "epoch 3, batch 41, loss = 0.01872633770108223\n",
      "epoch 3, batch 42, loss = 0.04623120278120041\n",
      "epoch 3, batch 43, loss = 0.014261234551668167\n",
      "epoch 3, batch 44, loss = 0.03885521739721298\n",
      "epoch 3, batch 45, loss = 0.04426729679107666\n",
      "epoch 3, batch 46, loss = 0.016805794090032578\n",
      "epoch 3, batch 47, loss = 0.011264758184552193\n",
      "epoch 3, batch 48, loss = 0.03755604475736618\n",
      "epoch 3, batch 49, loss = 0.044463444501161575\n",
      "epoch 3, batch 50, loss = 0.009024478495121002\n",
      "epoch 3, batch 51, loss = 0.021293863654136658\n",
      "epoch 3, batch 52, loss = 0.026911258697509766\n",
      "epoch 3, batch 53, loss = 0.012651351280510426\n",
      "epoch 3, batch 54, loss = 0.016727475449442863\n",
      "epoch 3, batch 55, loss = 0.03234122693538666\n",
      "epoch 3, batch 56, loss = 0.040006957948207855\n",
      "epoch 3, batch 57, loss = 0.038563475012779236\n",
      "epoch 3, batch 58, loss = 0.039767391979694366\n",
      "epoch 3, batch 59, loss = 0.04234864562749863\n",
      "epoch 3, batch 60, loss = 0.05060674250125885\n",
      "epoch 3, batch 61, loss = 0.006771655287593603\n",
      "epoch 3, batch 62, loss = 0.01609566994011402\n",
      "epoch 3, batch 63, loss = 0.018103448674082756\n",
      "epoch 3, batch 64, loss = 0.05152947083115578\n",
      "epoch 3, batch 65, loss = 0.05654611438512802\n",
      "epoch 3, batch 66, loss = 0.015539049170911312\n",
      "epoch 3, batch 67, loss = 0.02662930078804493\n",
      "epoch 3, batch 68, loss = 0.03511535003781319\n",
      "epoch 3, batch 69, loss = 0.01778247393667698\n",
      "epoch 3, batch 70, loss = 0.05563652142882347\n",
      "epoch 3, batch 71, loss = 0.05601784586906433\n",
      "epoch 3, batch 72, loss = 0.008438230492174625\n",
      "epoch 3, batch 73, loss = 0.03526150807738304\n",
      "epoch 3, batch 74, loss = 0.02869531325995922\n",
      "epoch 3, batch 75, loss = 0.00540970079600811\n",
      "epoch 3, batch 76, loss = 0.11390787363052368\n",
      "epoch 3, batch 77, loss = 0.01401243545114994\n",
      "epoch 3, batch 78, loss = 0.01777397096157074\n",
      "epoch 3, batch 79, loss = 0.06613568216562271\n",
      "epoch 3, batch 80, loss = 0.01901097036898136\n",
      "epoch 3, batch 81, loss = 0.014113891869783401\n",
      "epoch 3, batch 82, loss = 0.027070311829447746\n",
      "epoch 3, batch 83, loss = 0.028407728299498558\n",
      "epoch 3, batch 84, loss = 0.08636634051799774\n",
      "epoch 3, batch 85, loss = 0.03834009915590286\n",
      "epoch 3, batch 86, loss = 0.017581207677721977\n",
      "epoch 3, batch 87, loss = 0.02542591653764248\n",
      "epoch 3, batch 88, loss = 0.037476055324077606\n",
      "epoch 3, batch 89, loss = 0.02339947782456875\n",
      "epoch 3, batch 90, loss = 0.011298415251076221\n",
      "epoch 3, batch 91, loss = 0.027999911457300186\n",
      "epoch 3, batch 92, loss = 0.01674525812268257\n",
      "epoch 3, batch 93, loss = 0.02452041395008564\n",
      "epoch 3, batch 94, loss = 0.049951132386922836\n",
      "epoch 3, batch 95, loss = 0.046124283224344254\n",
      "epoch 3, batch 96, loss = 0.01460288092494011\n",
      "epoch 3, batch 97, loss = 0.017906934022903442\n",
      "epoch 3, batch 98, loss = 0.025990372523665428\n",
      "epoch 3, batch 99, loss = 0.05604496970772743\n",
      "epoch 3, batch 100, loss = 0.016414407640695572\n",
      "epoch 3, batch 101, loss = 0.008963597007095814\n",
      "epoch 3, batch 102, loss = 0.02081637643277645\n",
      "epoch 3, batch 103, loss = 0.0255439355969429\n",
      "epoch 3, batch 104, loss = 0.022829782217741013\n",
      "epoch 3, batch 105, loss = 0.00844250526279211\n",
      "epoch 3, batch 106, loss = 0.016703927889466286\n",
      "epoch 3, batch 107, loss = 0.02456774190068245\n",
      "epoch 3, batch 108, loss = 0.05763824284076691\n",
      "epoch 3, batch 109, loss = 0.024423880502581596\n",
      "epoch 3, batch 110, loss = 0.03087650239467621\n",
      "epoch 3, batch 111, loss = 0.02069801278412342\n",
      "epoch 3, batch 112, loss = 0.020674176514148712\n",
      "epoch 3, batch 113, loss = 0.00901342649012804\n",
      "epoch 3, batch 114, loss = 0.019383614882826805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 115, loss = 0.0477493554353714\n",
      "epoch 3, batch 116, loss = 0.12459567189216614\n",
      "epoch 3, batch 117, loss = 0.058158788830041885\n",
      "epoch 3, batch 118, loss = 0.02026744931936264\n",
      "epoch 3, batch 119, loss = 0.0415709987282753\n",
      "epoch 3, batch 120, loss = 0.012301316484808922\n",
      "epoch 3, batch 121, loss = 0.01693156361579895\n",
      "epoch 3, batch 122, loss = 0.024768244475126266\n",
      "epoch 3, batch 123, loss = 0.045597635209560394\n",
      "epoch 3, batch 124, loss = 0.03111327812075615\n",
      "epoch 3, batch 125, loss = 0.016809113323688507\n",
      "epoch 3, batch 126, loss = 0.02474023588001728\n",
      "epoch 3, batch 127, loss = 0.02919384464621544\n",
      "epoch 3, batch 128, loss = 0.04111257568001747\n",
      "epoch 3, batch 129, loss = 0.03518546372652054\n",
      "epoch 3, batch 130, loss = 0.02593463845551014\n",
      "epoch 3, batch 131, loss = 0.020235631614923477\n",
      "epoch 3, batch 132, loss = 0.06135888025164604\n",
      "epoch 3, batch 133, loss = 0.04483520984649658\n",
      "epoch 3, batch 134, loss = 0.024643681943416595\n",
      "epoch 3, batch 135, loss = 0.024095673114061356\n",
      "epoch 3, batch 136, loss = 0.0063953702338039875\n",
      "epoch 3, batch 137, loss = 0.03142804279923439\n",
      "epoch 3, batch 138, loss = 0.052134837955236435\n",
      "epoch 3, batch 139, loss = 0.04021706059575081\n",
      "epoch 3, batch 140, loss = 0.038825586438179016\n",
      "epoch 3, batch 141, loss = 0.03823533654212952\n",
      "epoch 3, batch 142, loss = 0.027983596548438072\n",
      "epoch 3, batch 143, loss = 0.062163930386304855\n",
      "epoch 3, batch 144, loss = 0.037871263921260834\n",
      "epoch 3, batch 145, loss = 0.03996868059039116\n",
      "epoch 3, batch 146, loss = 0.027441972866654396\n",
      "epoch 3, batch 147, loss = 0.0129168089479208\n",
      "epoch 3, batch 148, loss = 0.028469925746321678\n",
      "epoch 3, batch 149, loss = 0.039485152810811996\n",
      "epoch 3, batch 150, loss = 0.07117996364831924\n",
      "epoch 3, batch 151, loss = 0.02733408845961094\n",
      "epoch 3, batch 152, loss = 0.030931077897548676\n",
      "epoch 3, batch 153, loss = 0.017221255227923393\n",
      "epoch 3, batch 154, loss = 0.0834645926952362\n",
      "epoch 3, batch 155, loss = 0.07833662629127502\n",
      "epoch 3, batch 156, loss = 0.02802460454404354\n",
      "epoch 3, batch 157, loss = 0.07272014021873474\n",
      "epoch 3, batch 158, loss = 0.05176122486591339\n",
      "epoch 3, batch 159, loss = 0.039657678455114365\n",
      "epoch 3, batch 160, loss = 0.020930631086230278\n",
      "epoch 3, batch 161, loss = 0.032417573034763336\n",
      "epoch 3, batch 162, loss = 0.029010366648435593\n",
      "epoch 3, batch 163, loss = 0.02687852643430233\n",
      "epoch 3, batch 164, loss = 0.05222729593515396\n",
      "epoch 3, batch 165, loss = 0.022898590192198753\n",
      "epoch 3, batch 166, loss = 0.03440166637301445\n",
      "epoch 3, batch 167, loss = 0.04102364182472229\n",
      "epoch 3, batch 168, loss = 0.019373107701539993\n",
      "epoch 3, batch 169, loss = 0.028879420831799507\n",
      "epoch 3, batch 170, loss = 0.049589306116104126\n",
      "epoch 3, batch 171, loss = 0.033037420362234116\n",
      "epoch 3, batch 172, loss = 0.021728361025452614\n",
      "epoch 3, batch 173, loss = 0.04952199012041092\n",
      "epoch 3, batch 174, loss = 0.019215790554881096\n",
      "epoch 3, batch 175, loss = 0.030354738235473633\n",
      "epoch 3, batch 176, loss = 0.019462408497929573\n",
      "epoch 3, batch 177, loss = 0.04734986275434494\n",
      "epoch 3, batch 178, loss = 0.022786982357501984\n",
      "epoch 3, batch 179, loss = 0.018590375781059265\n",
      "epoch 3, batch 180, loss = 0.04149109870195389\n",
      "epoch 3, batch 181, loss = 0.04261556267738342\n",
      "epoch 3, batch 182, loss = 0.024828186258673668\n",
      "epoch 3, batch 183, loss = 0.005223378073424101\n",
      "epoch 3, batch 184, loss = 0.0358242429792881\n",
      "epoch 3, batch 185, loss = 0.018580758944153786\n",
      "epoch 3, batch 186, loss = 0.02756463550031185\n",
      "epoch 3, batch 187, loss = 0.019369179382920265\n",
      "epoch 3, batch 188, loss = 0.020090598613023758\n",
      "epoch 3, batch 189, loss = 0.06042429432272911\n",
      "epoch 3, batch 190, loss = 0.01713729277253151\n",
      "epoch 3, batch 191, loss = 0.010393415577709675\n",
      "epoch 3, batch 192, loss = 0.018830586224794388\n",
      "epoch 3, batch 193, loss = 0.016261128708720207\n",
      "epoch 3, batch 194, loss = 0.04679335653781891\n",
      "epoch 3, batch 195, loss = 0.03102102130651474\n",
      "epoch 3, batch 196, loss = 0.017576681450009346\n",
      "epoch 3, batch 197, loss = 0.009831901639699936\n",
      "epoch 3, batch 198, loss = 0.026545356959104538\n",
      "epoch 3, batch 199, loss = 0.022599197924137115\n",
      "epoch 4, batch 1, loss = 0.04455107823014259\n",
      "epoch 4, batch 2, loss = 0.008597183972597122\n",
      "epoch 4, batch 3, loss = 0.01841720938682556\n",
      "epoch 4, batch 4, loss = 0.010455221869051456\n",
      "epoch 4, batch 5, loss = 0.03258765488862991\n",
      "epoch 4, batch 6, loss = 0.0334680899977684\n",
      "epoch 4, batch 7, loss = 0.018337421119213104\n",
      "epoch 4, batch 8, loss = 0.022255852818489075\n",
      "epoch 4, batch 9, loss = 0.024598898366093636\n",
      "epoch 4, batch 10, loss = 0.02885686419904232\n",
      "epoch 4, batch 11, loss = 0.022232020273804665\n",
      "epoch 4, batch 12, loss = 0.007757949642837048\n",
      "epoch 4, batch 13, loss = 0.008655345067381859\n",
      "epoch 4, batch 14, loss = 0.03675997629761696\n",
      "epoch 4, batch 15, loss = 0.02297121286392212\n",
      "epoch 4, batch 16, loss = 0.0068444120697677135\n",
      "epoch 4, batch 17, loss = 0.015364215709269047\n",
      "epoch 4, batch 18, loss = 0.03740505874156952\n",
      "epoch 4, batch 19, loss = 0.05834182724356651\n",
      "epoch 4, batch 20, loss = 0.04947328194975853\n",
      "epoch 4, batch 21, loss = 0.013012504205107689\n",
      "epoch 4, batch 22, loss = 0.011542867869138718\n",
      "epoch 4, batch 23, loss = 0.023293232545256615\n",
      "epoch 4, batch 24, loss = 0.027159078046679497\n",
      "epoch 4, batch 25, loss = 0.0067308261059224606\n",
      "epoch 4, batch 26, loss = 0.048518914729356766\n",
      "epoch 4, batch 27, loss = 0.01615135930478573\n",
      "epoch 4, batch 28, loss = 0.05335080623626709\n",
      "epoch 4, batch 29, loss = 0.019871510565280914\n",
      "epoch 4, batch 30, loss = 0.015552856959402561\n",
      "epoch 4, batch 31, loss = 0.026286225765943527\n",
      "epoch 4, batch 32, loss = 0.04589485004544258\n",
      "epoch 4, batch 33, loss = 0.0857711136341095\n",
      "epoch 4, batch 34, loss = 0.005225164350122213\n",
      "epoch 4, batch 35, loss = 0.019654342904686928\n",
      "epoch 4, batch 36, loss = 0.07495985925197601\n",
      "epoch 4, batch 37, loss = 0.009752001613378525\n",
      "epoch 4, batch 38, loss = 0.019122514873743057\n",
      "epoch 4, batch 39, loss = 0.019612597301602364\n",
      "epoch 4, batch 40, loss = 0.012919341214001179\n",
      "epoch 4, batch 41, loss = 0.005284852348268032\n",
      "epoch 4, batch 42, loss = 0.023473400622606277\n",
      "epoch 4, batch 43, loss = 0.046648744493722916\n",
      "epoch 4, batch 44, loss = 0.04907806217670441\n",
      "epoch 4, batch 45, loss = 0.0033912507351487875\n",
      "epoch 4, batch 46, loss = 0.009062591940164566\n",
      "epoch 4, batch 47, loss = 0.027951404452323914\n",
      "epoch 4, batch 48, loss = 0.012591680511832237\n",
      "epoch 4, batch 49, loss = 0.007818667218089104\n",
      "epoch 4, batch 50, loss = 0.013294953852891922\n",
      "epoch 4, batch 51, loss = 0.035620760172605515\n",
      "epoch 4, batch 52, loss = 0.024603690952062607\n",
      "epoch 4, batch 53, loss = 0.00457365857437253\n",
      "epoch 4, batch 54, loss = 0.028610367327928543\n",
      "epoch 4, batch 55, loss = 0.013940474018454552\n",
      "epoch 4, batch 56, loss = 0.03239043802022934\n",
      "epoch 4, batch 57, loss = 0.01878894865512848\n",
      "epoch 4, batch 58, loss = 0.007475763559341431\n",
      "epoch 4, batch 59, loss = 0.00986462365835905\n",
      "epoch 4, batch 60, loss = 0.009397772140800953\n",
      "epoch 4, batch 61, loss = 0.0165876355022192\n",
      "epoch 4, batch 62, loss = 0.018237609416246414\n",
      "epoch 4, batch 63, loss = 0.02275889925658703\n",
      "epoch 4, batch 64, loss = 0.02633049711585045\n",
      "epoch 4, batch 65, loss = 0.01239634770900011\n",
      "epoch 4, batch 66, loss = 0.027438394725322723\n",
      "epoch 4, batch 67, loss = 0.021201858296990395\n",
      "epoch 4, batch 68, loss = 0.020873522385954857\n",
      "epoch 4, batch 69, loss = 0.007665969897061586\n",
      "epoch 4, batch 70, loss = 0.0458119735121727\n",
      "epoch 4, batch 71, loss = 0.012926178984344006\n",
      "epoch 4, batch 72, loss = 0.011900922283530235\n",
      "epoch 4, batch 73, loss = 0.0041345651261508465\n",
      "epoch 4, batch 74, loss = 0.012956882826983929\n",
      "epoch 4, batch 75, loss = 0.01760493591427803\n",
      "epoch 4, batch 76, loss = 0.028503574430942535\n",
      "epoch 4, batch 77, loss = 0.0070913685485720634\n",
      "epoch 4, batch 78, loss = 0.05313153564929962\n",
      "epoch 4, batch 79, loss = 0.051376186311244965\n",
      "epoch 4, batch 80, loss = 0.035632096230983734\n",
      "epoch 4, batch 81, loss = 0.011363700963556767\n",
      "epoch 4, batch 82, loss = 0.004106891341507435\n",
      "epoch 4, batch 83, loss = 0.039389342069625854\n",
      "epoch 4, batch 84, loss = 0.011684413067996502\n",
      "epoch 4, batch 85, loss = 0.03300224244594574\n",
      "epoch 4, batch 86, loss = 0.02731601893901825\n",
      "epoch 4, batch 87, loss = 0.04886873811483383\n",
      "epoch 4, batch 88, loss = 0.028420288115739822\n",
      "epoch 4, batch 89, loss = 0.009341378696262836\n",
      "epoch 4, batch 90, loss = 0.020428700372576714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, batch 91, loss = 0.029024654999375343\n",
      "epoch 4, batch 92, loss = 0.029478197917342186\n",
      "epoch 4, batch 93, loss = 0.020274769514799118\n",
      "epoch 4, batch 94, loss = 0.006607164163142443\n",
      "epoch 4, batch 95, loss = 0.052216872572898865\n",
      "epoch 4, batch 96, loss = 0.016406310722231865\n",
      "epoch 4, batch 97, loss = 0.01084564533084631\n",
      "epoch 4, batch 98, loss = 0.017590779811143875\n",
      "epoch 4, batch 99, loss = 0.0721297338604927\n",
      "epoch 4, batch 100, loss = 0.008654020726680756\n",
      "epoch 4, batch 101, loss = 0.048518262803554535\n",
      "epoch 4, batch 102, loss = 0.024308206513524055\n",
      "epoch 4, batch 103, loss = 0.015533163212239742\n",
      "epoch 4, batch 104, loss = 0.039200130850076675\n",
      "epoch 4, batch 105, loss = 0.020726921036839485\n",
      "epoch 4, batch 106, loss = 0.028920037671923637\n",
      "epoch 4, batch 107, loss = 0.004861426539719105\n",
      "epoch 4, batch 108, loss = 0.04189920425415039\n",
      "epoch 4, batch 109, loss = 0.05144326761364937\n",
      "epoch 4, batch 110, loss = 0.011166797019541264\n",
      "epoch 4, batch 111, loss = 0.029211824759840965\n",
      "epoch 4, batch 112, loss = 0.011390311643481255\n",
      "epoch 4, batch 113, loss = 0.017162183299660683\n",
      "epoch 4, batch 114, loss = 0.021204136312007904\n",
      "epoch 4, batch 115, loss = 0.03621891513466835\n",
      "epoch 4, batch 116, loss = 0.015917614102363586\n",
      "epoch 4, batch 117, loss = 0.02919619157910347\n",
      "epoch 4, batch 118, loss = 0.012199882417917252\n",
      "epoch 4, batch 119, loss = 0.02544247917830944\n",
      "epoch 4, batch 120, loss = 0.012959926389157772\n",
      "epoch 4, batch 121, loss = 0.01874723471701145\n",
      "epoch 4, batch 122, loss = 0.06358049064874649\n",
      "epoch 4, batch 123, loss = 0.012554261833429337\n",
      "epoch 4, batch 124, loss = 0.03525770083069801\n",
      "epoch 4, batch 125, loss = 0.049673911184072495\n",
      "epoch 4, batch 126, loss = 0.059557124972343445\n",
      "epoch 4, batch 127, loss = 0.005727915558964014\n",
      "epoch 4, batch 128, loss = 0.050438638776540756\n",
      "epoch 4, batch 129, loss = 0.005872875917702913\n",
      "epoch 4, batch 130, loss = 0.044014737010002136\n",
      "epoch 4, batch 131, loss = 0.021216245368123055\n",
      "epoch 4, batch 132, loss = 0.0030173086561262608\n",
      "epoch 4, batch 133, loss = 0.03181615471839905\n",
      "epoch 4, batch 134, loss = 0.020237445831298828\n",
      "epoch 4, batch 135, loss = 0.051237400621175766\n",
      "epoch 4, batch 136, loss = 0.05322856828570366\n",
      "epoch 4, batch 137, loss = 0.04157833009958267\n",
      "epoch 4, batch 138, loss = 0.013715684413909912\n",
      "epoch 4, batch 139, loss = 0.007144475355744362\n",
      "epoch 4, batch 140, loss = 0.007973214611411095\n",
      "epoch 4, batch 141, loss = 0.04354410991072655\n",
      "epoch 4, batch 142, loss = 0.011157236061990261\n",
      "epoch 4, batch 143, loss = 0.020028434693813324\n",
      "epoch 4, batch 144, loss = 0.016623780131340027\n",
      "epoch 4, batch 145, loss = 0.02672526426613331\n",
      "epoch 4, batch 146, loss = 0.04099659621715546\n",
      "epoch 4, batch 147, loss = 0.004765282850712538\n",
      "epoch 4, batch 148, loss = 0.02178405225276947\n",
      "epoch 4, batch 149, loss = 0.02011628821492195\n",
      "epoch 4, batch 150, loss = 0.04964151233434677\n",
      "epoch 4, batch 151, loss = 0.04832111671566963\n",
      "epoch 4, batch 152, loss = 0.026949502527713776\n",
      "epoch 4, batch 153, loss = 0.025241194292902946\n",
      "epoch 4, batch 154, loss = 0.1047084704041481\n",
      "epoch 4, batch 155, loss = 0.03782835975289345\n",
      "epoch 4, batch 156, loss = 0.01935650035738945\n",
      "epoch 4, batch 157, loss = 0.008920134045183659\n",
      "epoch 4, batch 158, loss = 0.02435353770852089\n",
      "epoch 4, batch 159, loss = 0.030149735510349274\n",
      "epoch 4, batch 160, loss = 0.022386258468031883\n",
      "epoch 4, batch 161, loss = 0.01659049466252327\n",
      "epoch 4, batch 162, loss = 0.008497173897922039\n",
      "epoch 4, batch 163, loss = 0.0034488525707274675\n",
      "epoch 4, batch 164, loss = 0.02529488131403923\n",
      "epoch 4, batch 165, loss = 0.006256397347897291\n",
      "epoch 4, batch 166, loss = 0.011927004903554916\n",
      "epoch 4, batch 167, loss = 0.006806637160480022\n",
      "epoch 4, batch 168, loss = 0.03398957476019859\n",
      "epoch 4, batch 169, loss = 0.030498232692480087\n",
      "epoch 4, batch 170, loss = 0.02803378738462925\n",
      "epoch 4, batch 171, loss = 0.03682270646095276\n",
      "epoch 4, batch 172, loss = 0.1037662997841835\n",
      "epoch 4, batch 173, loss = 0.016405470669269562\n",
      "epoch 4, batch 174, loss = 0.018954919651150703\n",
      "epoch 4, batch 175, loss = 0.02428770251572132\n",
      "epoch 4, batch 176, loss = 0.012339461594820023\n",
      "epoch 4, batch 177, loss = 0.003480305662378669\n",
      "epoch 4, batch 178, loss = 0.027915887534618378\n",
      "epoch 4, batch 179, loss = 0.011866927146911621\n",
      "epoch 4, batch 180, loss = 0.0242979284375906\n",
      "epoch 4, batch 181, loss = 0.00391819141805172\n",
      "epoch 4, batch 182, loss = 0.011792627163231373\n",
      "epoch 4, batch 183, loss = 0.0024815108627080917\n",
      "epoch 4, batch 184, loss = 0.005625277291983366\n",
      "epoch 4, batch 185, loss = 0.06394542008638382\n",
      "epoch 4, batch 186, loss = 0.015411347150802612\n",
      "epoch 4, batch 187, loss = 0.03627008944749832\n",
      "epoch 4, batch 188, loss = 0.01676810346543789\n",
      "epoch 4, batch 189, loss = 0.021815365180373192\n",
      "epoch 4, batch 190, loss = 0.02121649496257305\n",
      "epoch 4, batch 191, loss = 0.029336757957935333\n",
      "epoch 4, batch 192, loss = 0.052963268011808395\n",
      "epoch 4, batch 193, loss = 0.01339663751423359\n",
      "epoch 4, batch 194, loss = 0.025568325072526932\n",
      "epoch 4, batch 195, loss = 0.019319280982017517\n",
      "epoch 4, batch 196, loss = 0.02712179906666279\n",
      "epoch 4, batch 197, loss = 0.014723223634064198\n",
      "epoch 4, batch 198, loss = 0.03982965648174286\n",
      "epoch 4, batch 199, loss = 0.0037689898163080215\n",
      "epoch 5, batch 1, loss = 0.027100998908281326\n",
      "epoch 5, batch 2, loss = 0.007618842646479607\n",
      "epoch 5, batch 3, loss = 0.005270771216601133\n",
      "epoch 5, batch 4, loss = 0.04123925045132637\n",
      "epoch 5, batch 5, loss = 0.012517455033957958\n",
      "epoch 5, batch 6, loss = 0.02535763755440712\n",
      "epoch 5, batch 7, loss = 0.011235091835260391\n",
      "epoch 5, batch 8, loss = 0.008308113552629948\n",
      "epoch 5, batch 9, loss = 0.04192260280251503\n",
      "epoch 5, batch 10, loss = 0.00430232472717762\n",
      "epoch 5, batch 11, loss = 0.021375536918640137\n",
      "epoch 5, batch 12, loss = 0.022784315049648285\n",
      "epoch 5, batch 13, loss = 0.024977674707770348\n",
      "epoch 5, batch 14, loss = 0.009616853669285774\n",
      "epoch 5, batch 15, loss = 0.005168704781681299\n",
      "epoch 5, batch 16, loss = 0.018686873838305473\n",
      "epoch 5, batch 17, loss = 0.02739696577191353\n",
      "epoch 5, batch 18, loss = 0.029893185943365097\n",
      "epoch 5, batch 19, loss = 0.007003469858318567\n",
      "epoch 5, batch 20, loss = 0.028687987476587296\n",
      "epoch 5, batch 21, loss = 0.018823405727744102\n",
      "epoch 5, batch 22, loss = 0.02362753078341484\n",
      "epoch 5, batch 23, loss = 0.031249871477484703\n",
      "epoch 5, batch 24, loss = 0.015626955777406693\n",
      "epoch 5, batch 25, loss = 0.002344416920095682\n",
      "epoch 5, batch 26, loss = 0.019556866958737373\n",
      "epoch 5, batch 27, loss = 0.01949872262775898\n",
      "epoch 5, batch 28, loss = 0.016832171007990837\n",
      "epoch 5, batch 29, loss = 0.011659444309771061\n",
      "epoch 5, batch 30, loss = 0.008231470361351967\n",
      "epoch 5, batch 31, loss = 0.017314419150352478\n",
      "epoch 5, batch 32, loss = 0.05206519737839699\n",
      "epoch 5, batch 33, loss = 0.013988664373755455\n",
      "epoch 5, batch 34, loss = 0.010487599298357964\n",
      "epoch 5, batch 35, loss = 0.012548434548079967\n",
      "epoch 5, batch 36, loss = 0.005031665787100792\n",
      "epoch 5, batch 37, loss = 0.004307132679969072\n",
      "epoch 5, batch 38, loss = 0.02876119129359722\n",
      "epoch 5, batch 39, loss = 0.014409941621124744\n",
      "epoch 5, batch 40, loss = 0.011976134032011032\n",
      "epoch 5, batch 41, loss = 0.02192777395248413\n",
      "epoch 5, batch 42, loss = 0.011679332703351974\n",
      "epoch 5, batch 43, loss = 0.013998502865433693\n",
      "epoch 5, batch 44, loss = 0.018949851393699646\n",
      "epoch 5, batch 45, loss = 0.004553408361971378\n",
      "epoch 5, batch 46, loss = 0.013891246169805527\n",
      "epoch 5, batch 47, loss = 0.006892991252243519\n",
      "epoch 5, batch 48, loss = 0.013917185366153717\n",
      "epoch 5, batch 49, loss = 0.005759647116065025\n",
      "epoch 5, batch 50, loss = 0.022795259952545166\n",
      "epoch 5, batch 51, loss = 0.003939690999686718\n",
      "epoch 5, batch 52, loss = 0.031570419669151306\n",
      "epoch 5, batch 53, loss = 0.01905469410121441\n",
      "epoch 5, batch 54, loss = 0.002993494039401412\n",
      "epoch 5, batch 55, loss = 0.004910822957754135\n",
      "epoch 5, batch 56, loss = 0.007291642017662525\n",
      "epoch 5, batch 57, loss = 0.028785254806280136\n",
      "epoch 5, batch 58, loss = 0.031224798411130905\n",
      "epoch 5, batch 59, loss = 0.03812785446643829\n",
      "epoch 5, batch 60, loss = 0.010978901758790016\n",
      "epoch 5, batch 61, loss = 0.008919906802475452\n",
      "epoch 5, batch 62, loss = 0.02163994498550892\n",
      "epoch 5, batch 63, loss = 0.009936295449733734\n",
      "epoch 5, batch 64, loss = 0.0069427695125341415\n",
      "epoch 5, batch 65, loss = 0.024411683902144432\n",
      "epoch 5, batch 66, loss = 0.02469271421432495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, batch 67, loss = 0.01848466880619526\n",
      "epoch 5, batch 68, loss = 0.014031586237251759\n",
      "epoch 5, batch 69, loss = 0.004739040974527597\n",
      "epoch 5, batch 70, loss = 0.016915950924158096\n",
      "epoch 5, batch 71, loss = 0.013273621909320354\n",
      "epoch 5, batch 72, loss = 0.014934947714209557\n",
      "epoch 5, batch 73, loss = 0.02793673239648342\n",
      "epoch 5, batch 74, loss = 0.002850265009328723\n",
      "epoch 5, batch 75, loss = 0.012759027071297169\n",
      "epoch 5, batch 76, loss = 0.006994262803345919\n",
      "epoch 5, batch 77, loss = 0.02281450293958187\n",
      "epoch 5, batch 78, loss = 0.008233216591179371\n",
      "epoch 5, batch 79, loss = 0.0067109293304383755\n",
      "epoch 5, batch 80, loss = 0.016501732170581818\n",
      "epoch 5, batch 81, loss = 0.019206224009394646\n",
      "epoch 5, batch 82, loss = 0.005740748252719641\n",
      "epoch 5, batch 83, loss = 0.0016255587106570601\n",
      "epoch 5, batch 84, loss = 0.01955419033765793\n",
      "epoch 5, batch 85, loss = 0.004384930711239576\n",
      "epoch 5, batch 86, loss = 0.00481990072876215\n",
      "epoch 5, batch 87, loss = 0.002317777369171381\n",
      "epoch 5, batch 88, loss = 0.01897651143372059\n",
      "epoch 5, batch 89, loss = 0.03501957282423973\n",
      "epoch 5, batch 90, loss = 0.040279027074575424\n",
      "epoch 5, batch 91, loss = 0.021193213760852814\n",
      "epoch 5, batch 92, loss = 0.0034887376241385937\n",
      "epoch 5, batch 93, loss = 0.024769235402345657\n",
      "epoch 5, batch 94, loss = 0.013977935537695885\n",
      "epoch 5, batch 95, loss = 0.004727061837911606\n",
      "epoch 5, batch 96, loss = 0.010406163521111012\n",
      "epoch 5, batch 97, loss = 0.1228499636054039\n",
      "epoch 5, batch 98, loss = 0.004868127405643463\n",
      "epoch 5, batch 99, loss = 0.01761203445494175\n",
      "epoch 5, batch 100, loss = 0.004341606516391039\n",
      "epoch 5, batch 101, loss = 0.0100655946880579\n",
      "epoch 5, batch 102, loss = 0.01507128681987524\n",
      "epoch 5, batch 103, loss = 0.007032326422631741\n",
      "epoch 5, batch 104, loss = 0.006250925827771425\n",
      "epoch 5, batch 105, loss = 0.05508192628622055\n",
      "epoch 5, batch 106, loss = 0.02505195140838623\n",
      "epoch 5, batch 107, loss = 0.009795553982257843\n",
      "epoch 5, batch 108, loss = 0.0029172345530241728\n",
      "epoch 5, batch 109, loss = 0.008914795704185963\n",
      "epoch 5, batch 110, loss = 0.018340900540351868\n",
      "epoch 5, batch 111, loss = 0.005190993659198284\n",
      "epoch 5, batch 112, loss = 0.005405044183135033\n",
      "epoch 5, batch 113, loss = 0.03769847750663757\n",
      "epoch 5, batch 114, loss = 0.0194780845195055\n",
      "epoch 5, batch 115, loss = 0.02995297871530056\n",
      "epoch 5, batch 116, loss = 0.030078481882810593\n",
      "epoch 5, batch 117, loss = 0.0343896709382534\n",
      "epoch 5, batch 118, loss = 0.0044317240826785564\n",
      "epoch 5, batch 119, loss = 0.029957078397274017\n",
      "epoch 5, batch 120, loss = 0.025214502587914467\n",
      "epoch 5, batch 121, loss = 0.018943393602967262\n",
      "epoch 5, batch 122, loss = 0.009641993790864944\n",
      "epoch 5, batch 123, loss = 0.026141129434108734\n",
      "epoch 5, batch 124, loss = 0.012784561142325401\n",
      "epoch 5, batch 125, loss = 0.013346782885491848\n",
      "epoch 5, batch 126, loss = 0.009519322775304317\n",
      "epoch 5, batch 127, loss = 0.021131861954927444\n",
      "epoch 5, batch 128, loss = 0.025008544325828552\n",
      "epoch 5, batch 129, loss = 0.01213774923235178\n",
      "epoch 5, batch 130, loss = 0.010501015931367874\n",
      "epoch 5, batch 131, loss = 0.00653713708743453\n",
      "epoch 5, batch 132, loss = 0.02216608077287674\n",
      "epoch 5, batch 133, loss = 0.03343243896961212\n",
      "epoch 5, batch 134, loss = 0.0411229282617569\n",
      "epoch 5, batch 135, loss = 0.013718713074922562\n",
      "epoch 5, batch 136, loss = 0.012098980136215687\n",
      "epoch 5, batch 137, loss = 0.005575247574597597\n",
      "epoch 5, batch 138, loss = 0.02360992692410946\n",
      "epoch 5, batch 139, loss = 0.00821419432759285\n",
      "epoch 5, batch 140, loss = 0.03513339161872864\n",
      "epoch 5, batch 141, loss = 0.006340915337204933\n",
      "epoch 5, batch 142, loss = 0.02333160676062107\n",
      "epoch 5, batch 143, loss = 0.012718481943011284\n",
      "epoch 5, batch 144, loss = 0.01456250436604023\n",
      "epoch 5, batch 145, loss = 0.007050675805658102\n",
      "epoch 5, batch 146, loss = 0.016239158809185028\n",
      "epoch 5, batch 147, loss = 0.012713300064206123\n",
      "epoch 5, batch 148, loss = 0.03302408382296562\n",
      "epoch 5, batch 149, loss = 0.016619736328721046\n",
      "epoch 5, batch 150, loss = 0.023604096844792366\n",
      "epoch 5, batch 151, loss = 0.034334681928157806\n",
      "epoch 5, batch 152, loss = 0.015249000862240791\n",
      "epoch 5, batch 153, loss = 0.0225087720900774\n",
      "epoch 5, batch 154, loss = 0.019304366782307625\n",
      "epoch 5, batch 155, loss = 0.0172997135668993\n",
      "epoch 5, batch 156, loss = 0.03808668255805969\n",
      "epoch 5, batch 157, loss = 0.012750841677188873\n",
      "epoch 5, batch 158, loss = 0.059937380254268646\n",
      "epoch 5, batch 159, loss = 0.054473549127578735\n",
      "epoch 5, batch 160, loss = 0.019116327166557312\n",
      "epoch 5, batch 161, loss = 0.009466526098549366\n",
      "epoch 5, batch 162, loss = 0.009541012346744537\n",
      "epoch 5, batch 163, loss = 0.010642925277352333\n",
      "epoch 5, batch 164, loss = 0.024235418066382408\n",
      "epoch 5, batch 165, loss = 0.008014869876205921\n",
      "epoch 5, batch 166, loss = 0.004083599429577589\n",
      "epoch 5, batch 167, loss = 0.028516728430986404\n",
      "epoch 5, batch 168, loss = 0.015904156491160393\n",
      "epoch 5, batch 169, loss = 0.02405332773923874\n",
      "epoch 5, batch 170, loss = 0.011288774199783802\n",
      "epoch 5, batch 171, loss = 0.004122053273022175\n",
      "epoch 5, batch 172, loss = 0.005317366681993008\n",
      "epoch 5, batch 173, loss = 0.0026705556083470583\n",
      "epoch 5, batch 174, loss = 0.0056348275393247604\n",
      "epoch 5, batch 175, loss = 0.028040152043104172\n",
      "epoch 5, batch 176, loss = 0.020958757027983665\n",
      "epoch 5, batch 177, loss = 0.029603583738207817\n",
      "epoch 5, batch 178, loss = 0.0085719283670187\n",
      "epoch 5, batch 179, loss = 0.002815983258187771\n",
      "epoch 5, batch 180, loss = 0.0048012807965278625\n",
      "epoch 5, batch 181, loss = 0.0036630723625421524\n",
      "epoch 5, batch 182, loss = 0.005616976413875818\n",
      "epoch 5, batch 183, loss = 0.0146824661642313\n",
      "epoch 5, batch 184, loss = 0.015810461714863777\n",
      "epoch 5, batch 185, loss = 0.051459889858961105\n",
      "epoch 5, batch 186, loss = 0.02785632386803627\n",
      "epoch 5, batch 187, loss = 0.009814941324293613\n",
      "epoch 5, batch 188, loss = 0.01230084989219904\n",
      "epoch 5, batch 189, loss = 0.0027714865282177925\n",
      "epoch 5, batch 190, loss = 0.030361078679561615\n",
      "epoch 5, batch 191, loss = 0.009314842522144318\n",
      "epoch 5, batch 192, loss = 0.029572736471891403\n",
      "epoch 5, batch 193, loss = 0.010751642286777496\n",
      "epoch 5, batch 194, loss = 0.004460952710360289\n",
      "epoch 5, batch 195, loss = 0.0043831802904605865\n",
      "epoch 5, batch 196, loss = 0.004704432096332312\n",
      "epoch 5, batch 197, loss = 0.028125979006290436\n",
      "epoch 5, batch 198, loss = 0.019368022680282593\n",
      "epoch 5, batch 199, loss = 0.02066558413207531\n",
      "epoch 6, batch 1, loss = 0.004559009801596403\n",
      "epoch 6, batch 2, loss = 0.006045004818588495\n",
      "epoch 6, batch 3, loss = 0.009464154951274395\n",
      "epoch 6, batch 4, loss = 0.008322209119796753\n",
      "epoch 6, batch 5, loss = 0.007334851194173098\n",
      "epoch 6, batch 6, loss = 0.030333952978253365\n",
      "epoch 6, batch 7, loss = 0.015934089198708534\n",
      "epoch 6, batch 8, loss = 0.0025095120072364807\n",
      "epoch 6, batch 9, loss = 0.0063644214533269405\n",
      "epoch 6, batch 10, loss = 0.008032944984734058\n",
      "epoch 6, batch 11, loss = 0.019995776936411858\n",
      "epoch 6, batch 12, loss = 0.011247273534536362\n",
      "epoch 6, batch 13, loss = 0.020774686709046364\n",
      "epoch 6, batch 14, loss = 0.018902486190199852\n",
      "epoch 6, batch 15, loss = 0.002289111725986004\n",
      "epoch 6, batch 16, loss = 0.011272363364696503\n",
      "epoch 6, batch 17, loss = 0.01939631812274456\n",
      "epoch 6, batch 18, loss = 0.04389852657914162\n",
      "epoch 6, batch 19, loss = 0.008505835197865963\n",
      "epoch 6, batch 20, loss = 0.016171785071492195\n",
      "epoch 6, batch 21, loss = 0.020610608160495758\n",
      "epoch 6, batch 22, loss = 0.034729164093732834\n",
      "epoch 6, batch 23, loss = 0.008300986140966415\n",
      "epoch 6, batch 24, loss = 0.020747115835547447\n",
      "epoch 6, batch 25, loss = 0.008455436676740646\n",
      "epoch 6, batch 26, loss = 0.010475789196789265\n",
      "epoch 6, batch 27, loss = 0.013996782712638378\n",
      "epoch 6, batch 28, loss = 0.028840575367212296\n",
      "epoch 6, batch 29, loss = 0.022322040051221848\n",
      "epoch 6, batch 30, loss = 0.014190074987709522\n",
      "epoch 6, batch 31, loss = 0.003925999626517296\n",
      "epoch 6, batch 32, loss = 0.008060376159846783\n",
      "epoch 6, batch 33, loss = 0.00596397090703249\n",
      "epoch 6, batch 34, loss = 0.00773338507860899\n",
      "epoch 6, batch 35, loss = 0.055205002427101135\n",
      "epoch 6, batch 36, loss = 0.028748268261551857\n",
      "epoch 6, batch 37, loss = 0.019452570006251335\n",
      "epoch 6, batch 38, loss = 0.01612081006169319\n",
      "epoch 6, batch 39, loss = 0.0060587553307414055\n",
      "epoch 6, batch 40, loss = 0.028977803885936737\n",
      "epoch 6, batch 41, loss = 0.010921191424131393\n",
      "epoch 6, batch 42, loss = 0.019517885521054268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, batch 43, loss = 0.011533145792782307\n",
      "epoch 6, batch 44, loss = 0.008558295667171478\n",
      "epoch 6, batch 45, loss = 0.003262409009039402\n",
      "epoch 6, batch 46, loss = 0.017691288143396378\n",
      "epoch 6, batch 47, loss = 0.006784700322896242\n",
      "epoch 6, batch 48, loss = 0.02783006988465786\n",
      "epoch 6, batch 49, loss = 0.015650764107704163\n",
      "epoch 6, batch 50, loss = 0.017367849126458168\n",
      "epoch 6, batch 51, loss = 0.03812933340668678\n",
      "epoch 6, batch 52, loss = 0.014555769972503185\n",
      "epoch 6, batch 53, loss = 0.032908596098423004\n",
      "epoch 6, batch 54, loss = 0.009184044785797596\n",
      "epoch 6, batch 55, loss = 0.012117832899093628\n",
      "epoch 6, batch 56, loss = 0.004570022225379944\n",
      "epoch 6, batch 57, loss = 0.011664333753287792\n",
      "epoch 6, batch 58, loss = 0.005445196758955717\n",
      "epoch 6, batch 59, loss = 0.005537320859730244\n",
      "epoch 6, batch 60, loss = 0.00860249251127243\n",
      "epoch 6, batch 61, loss = 0.010495749302208424\n",
      "epoch 6, batch 62, loss = 0.007932706736028194\n",
      "epoch 6, batch 63, loss = 0.016348185017704964\n",
      "epoch 6, batch 64, loss = 0.0055487374775111675\n",
      "epoch 6, batch 65, loss = 0.0077836173586547375\n",
      "epoch 6, batch 66, loss = 0.013603019528090954\n",
      "epoch 6, batch 67, loss = 0.011151091195642948\n",
      "epoch 6, batch 68, loss = 0.01175791583955288\n",
      "epoch 6, batch 69, loss = 0.004459680989384651\n",
      "epoch 6, batch 70, loss = 0.008610156364738941\n",
      "epoch 6, batch 71, loss = 0.019681917503476143\n",
      "epoch 6, batch 72, loss = 0.0017711487598717213\n",
      "epoch 6, batch 73, loss = 0.0029144734144210815\n",
      "epoch 6, batch 74, loss = 0.016169385984539986\n",
      "epoch 6, batch 75, loss = 0.0031832861714065075\n",
      "epoch 6, batch 76, loss = 0.004267075564712286\n",
      "epoch 6, batch 77, loss = 0.0191251989454031\n",
      "epoch 6, batch 78, loss = 0.01517411321401596\n",
      "epoch 6, batch 79, loss = 0.010934213176369667\n",
      "epoch 6, batch 80, loss = 0.02430792525410652\n",
      "epoch 6, batch 81, loss = 0.0126519575715065\n",
      "epoch 6, batch 82, loss = 0.01997791789472103\n",
      "epoch 6, batch 83, loss = 0.010480381548404694\n",
      "epoch 6, batch 84, loss = 0.013332215137779713\n",
      "epoch 6, batch 85, loss = 0.00963388942182064\n",
      "epoch 6, batch 86, loss = 0.020572282373905182\n",
      "epoch 6, batch 87, loss = 0.004137458745390177\n",
      "epoch 6, batch 88, loss = 0.019031301140785217\n",
      "epoch 6, batch 89, loss = 0.03288383036851883\n",
      "epoch 6, batch 90, loss = 0.016689253970980644\n",
      "epoch 6, batch 91, loss = 0.003790981136262417\n",
      "epoch 6, batch 92, loss = 0.006363882217556238\n",
      "epoch 6, batch 93, loss = 0.004270858597010374\n",
      "epoch 6, batch 94, loss = 0.013633009046316147\n",
      "epoch 6, batch 95, loss = 0.03161201626062393\n",
      "epoch 6, batch 96, loss = 0.007128681521862745\n",
      "epoch 6, batch 97, loss = 0.004766025114804506\n",
      "epoch 6, batch 98, loss = 0.011451277881860733\n",
      "epoch 6, batch 99, loss = 0.019339239224791527\n",
      "epoch 6, batch 100, loss = 0.019738705828785896\n",
      "epoch 6, batch 101, loss = 0.051553841680288315\n",
      "epoch 6, batch 102, loss = 0.031189149245619774\n",
      "epoch 6, batch 103, loss = 0.007869228720664978\n",
      "epoch 6, batch 104, loss = 0.005411412101238966\n",
      "epoch 6, batch 105, loss = 0.02495744079351425\n",
      "epoch 6, batch 106, loss = 0.006006704177707434\n",
      "epoch 6, batch 107, loss = 0.025049857795238495\n",
      "epoch 6, batch 108, loss = 0.0585666187107563\n",
      "epoch 6, batch 109, loss = 0.015253627672791481\n",
      "epoch 6, batch 110, loss = 0.010539458133280277\n",
      "epoch 6, batch 111, loss = 0.007409790530800819\n",
      "epoch 6, batch 112, loss = 0.001641507027670741\n",
      "epoch 6, batch 113, loss = 0.010755414143204689\n",
      "epoch 6, batch 114, loss = 0.049703363329172134\n",
      "epoch 6, batch 115, loss = 0.01206541620194912\n",
      "epoch 6, batch 116, loss = 0.01959022879600525\n",
      "epoch 6, batch 117, loss = 0.012885655276477337\n",
      "epoch 6, batch 118, loss = 0.003765291767194867\n",
      "epoch 6, batch 119, loss = 0.015784122049808502\n",
      "epoch 6, batch 120, loss = 0.01472518127411604\n",
      "epoch 6, batch 121, loss = 0.016731860116124153\n",
      "epoch 6, batch 122, loss = 0.020867107436060905\n",
      "epoch 6, batch 123, loss = 0.025748498737812042\n",
      "epoch 6, batch 124, loss = 0.018224377185106277\n",
      "epoch 6, batch 125, loss = 0.012203690595924854\n",
      "epoch 6, batch 126, loss = 0.003899391507729888\n",
      "epoch 6, batch 127, loss = 0.017122186720371246\n",
      "epoch 6, batch 128, loss = 0.0038296275306493044\n",
      "epoch 6, batch 129, loss = 0.004900400526821613\n",
      "epoch 6, batch 130, loss = 0.03504495322704315\n",
      "epoch 6, batch 131, loss = 0.03288504108786583\n",
      "epoch 6, batch 132, loss = 0.028989411890506744\n",
      "epoch 6, batch 133, loss = 0.020787160843610764\n",
      "epoch 6, batch 134, loss = 0.020980769768357277\n",
      "epoch 6, batch 135, loss = 0.004250944126397371\n",
      "epoch 6, batch 136, loss = 0.009494731202721596\n",
      "epoch 6, batch 137, loss = 0.0025926646776497364\n",
      "epoch 6, batch 138, loss = 0.012260972522199154\n",
      "epoch 6, batch 139, loss = 0.00385872065089643\n",
      "epoch 6, batch 140, loss = 0.026634009554982185\n",
      "epoch 6, batch 141, loss = 0.005055778194218874\n",
      "epoch 6, batch 142, loss = 0.017049219459295273\n",
      "epoch 6, batch 143, loss = 0.012851256877183914\n",
      "epoch 6, batch 144, loss = 0.01341084111481905\n",
      "epoch 6, batch 145, loss = 0.03767189010977745\n",
      "epoch 6, batch 146, loss = 0.0035287512000650167\n",
      "epoch 6, batch 147, loss = 0.0038348124362528324\n",
      "epoch 6, batch 148, loss = 0.02569061517715454\n",
      "epoch 6, batch 149, loss = 0.049269918352365494\n",
      "epoch 6, batch 150, loss = 0.010747438296675682\n",
      "epoch 6, batch 151, loss = 0.0125651303678751\n",
      "epoch 6, batch 152, loss = 0.01063424814492464\n",
      "epoch 6, batch 153, loss = 0.019939633086323738\n",
      "epoch 6, batch 154, loss = 0.014694376848638058\n",
      "epoch 6, batch 155, loss = 0.025308405980467796\n",
      "epoch 6, batch 156, loss = 0.008457845076918602\n",
      "epoch 6, batch 157, loss = 0.0166893620043993\n",
      "epoch 6, batch 158, loss = 0.002162646735087037\n",
      "epoch 6, batch 159, loss = 0.011115455999970436\n",
      "epoch 6, batch 160, loss = 0.005468239076435566\n",
      "epoch 6, batch 161, loss = 0.006721107754856348\n",
      "epoch 6, batch 162, loss = 0.03229089826345444\n",
      "epoch 6, batch 163, loss = 0.009706846438348293\n",
      "epoch 6, batch 164, loss = 0.03839655965566635\n",
      "epoch 6, batch 165, loss = 0.009481903165578842\n",
      "epoch 6, batch 166, loss = 0.004233245737850666\n",
      "epoch 6, batch 167, loss = 0.030398542061448097\n",
      "epoch 6, batch 168, loss = 0.0009631739812903106\n",
      "epoch 6, batch 169, loss = 0.0027944522444158792\n",
      "epoch 6, batch 170, loss = 0.04260904714465141\n",
      "epoch 6, batch 171, loss = 0.03396294265985489\n",
      "epoch 6, batch 172, loss = 0.027723481878638268\n",
      "epoch 6, batch 173, loss = 0.026091687381267548\n",
      "epoch 6, batch 174, loss = 0.00788307748734951\n",
      "epoch 6, batch 175, loss = 0.004014871548861265\n",
      "epoch 6, batch 176, loss = 0.08878003805875778\n",
      "epoch 6, batch 177, loss = 0.01368365902453661\n",
      "epoch 6, batch 178, loss = 0.021651938557624817\n",
      "epoch 6, batch 179, loss = 0.019066890701651573\n",
      "epoch 6, batch 180, loss = 0.00905037671327591\n",
      "epoch 6, batch 181, loss = 0.02043919265270233\n",
      "epoch 6, batch 182, loss = 0.01626344956457615\n",
      "epoch 6, batch 183, loss = 0.005123307928442955\n",
      "epoch 6, batch 184, loss = 0.032421305775642395\n",
      "epoch 6, batch 185, loss = 0.007020759396255016\n",
      "epoch 6, batch 186, loss = 0.0040953559800982475\n",
      "epoch 6, batch 187, loss = 0.0015048625646159053\n",
      "epoch 6, batch 188, loss = 0.025377415120601654\n",
      "epoch 6, batch 189, loss = 0.005162854213267565\n",
      "epoch 6, batch 190, loss = 0.011755917221307755\n",
      "epoch 6, batch 191, loss = 0.01173089724034071\n",
      "epoch 6, batch 192, loss = 0.003525746753439307\n",
      "epoch 6, batch 193, loss = 0.0018668200355023146\n",
      "epoch 6, batch 194, loss = 0.03986843675374985\n",
      "epoch 6, batch 195, loss = 0.028819218277931213\n",
      "epoch 6, batch 196, loss = 0.010464790277183056\n",
      "epoch 6, batch 197, loss = 0.018957681953907013\n",
      "epoch 6, batch 198, loss = 0.003121868474408984\n",
      "epoch 6, batch 199, loss = 0.015650596469640732\n",
      "epoch 7, batch 1, loss = 0.007551522459834814\n",
      "epoch 7, batch 2, loss = 0.013242559507489204\n",
      "epoch 7, batch 3, loss = 0.003959356341511011\n",
      "epoch 7, batch 4, loss = 0.000708286534063518\n",
      "epoch 7, batch 5, loss = 0.019619790837168694\n",
      "epoch 7, batch 6, loss = 0.005595566239207983\n",
      "epoch 7, batch 7, loss = 0.006082210689783096\n",
      "epoch 7, batch 8, loss = 0.003940942697227001\n",
      "epoch 7, batch 9, loss = 0.006731313653290272\n",
      "epoch 7, batch 10, loss = 0.004727277439087629\n",
      "epoch 7, batch 11, loss = 0.0043946439400315285\n",
      "epoch 7, batch 12, loss = 0.007301427889615297\n",
      "epoch 7, batch 13, loss = 0.005754267796874046\n",
      "epoch 7, batch 14, loss = 0.015495682135224342\n",
      "epoch 7, batch 15, loss = 0.003459199098870158\n",
      "epoch 7, batch 16, loss = 0.003200938692316413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, batch 17, loss = 0.025721250101923943\n",
      "epoch 7, batch 18, loss = 0.008245247416198254\n",
      "epoch 7, batch 19, loss = 0.003728794166818261\n",
      "epoch 7, batch 20, loss = 0.0008197352872230113\n",
      "epoch 7, batch 21, loss = 0.0026773614808917046\n",
      "epoch 7, batch 22, loss = 0.01460067369043827\n",
      "epoch 7, batch 23, loss = 0.004258529748767614\n",
      "epoch 7, batch 24, loss = 0.007351172622293234\n",
      "epoch 7, batch 25, loss = 0.001489975256845355\n",
      "epoch 7, batch 26, loss = 0.008558965288102627\n",
      "epoch 7, batch 27, loss = 0.00403332244604826\n",
      "epoch 7, batch 28, loss = 0.0013725783210247755\n",
      "epoch 7, batch 29, loss = 0.004795475397258997\n",
      "epoch 7, batch 30, loss = 0.011702162213623524\n",
      "epoch 7, batch 31, loss = 0.0010761817684397101\n",
      "epoch 7, batch 32, loss = 0.01057769451290369\n",
      "epoch 7, batch 33, loss = 0.005968935322016478\n",
      "epoch 7, batch 34, loss = 0.0019035206642001867\n",
      "epoch 7, batch 35, loss = 0.004637280013412237\n",
      "epoch 7, batch 36, loss = 0.0282047800719738\n",
      "epoch 7, batch 37, loss = 0.009536572732031345\n",
      "epoch 7, batch 38, loss = 0.0038851986173540354\n",
      "epoch 7, batch 39, loss = 0.008089731447398663\n",
      "epoch 7, batch 40, loss = 0.04379943758249283\n",
      "epoch 7, batch 41, loss = 0.023523574694991112\n",
      "epoch 7, batch 42, loss = 0.0016155572375282645\n",
      "epoch 7, batch 43, loss = 0.0037191971205174923\n",
      "epoch 7, batch 44, loss = 0.001330066006630659\n",
      "epoch 7, batch 45, loss = 0.0020991163328289986\n",
      "epoch 7, batch 46, loss = 0.003514418378472328\n",
      "epoch 7, batch 47, loss = 0.017295459285378456\n",
      "epoch 7, batch 48, loss = 0.009250663220882416\n",
      "epoch 7, batch 49, loss = 0.002583028981462121\n",
      "epoch 7, batch 50, loss = 0.00993277132511139\n",
      "epoch 7, batch 51, loss = 0.002064756816253066\n",
      "epoch 7, batch 52, loss = 0.014202742837369442\n",
      "epoch 7, batch 53, loss = 0.0036076826509088278\n",
      "epoch 7, batch 54, loss = 0.016332607716321945\n",
      "epoch 7, batch 55, loss = 0.01378520205616951\n",
      "epoch 7, batch 56, loss = 0.001490991679020226\n",
      "epoch 7, batch 57, loss = 0.0037048212252557278\n",
      "epoch 7, batch 58, loss = 0.005952413659542799\n",
      "epoch 7, batch 59, loss = 0.015341312624514103\n",
      "epoch 7, batch 60, loss = 0.011693017557263374\n",
      "epoch 7, batch 61, loss = 0.0018300064839422703\n",
      "epoch 7, batch 62, loss = 0.011970516294240952\n",
      "epoch 7, batch 63, loss = 0.007212400436401367\n",
      "epoch 7, batch 64, loss = 0.06467439979314804\n",
      "epoch 7, batch 65, loss = 0.016212161630392075\n",
      "epoch 7, batch 66, loss = 0.014825594611465931\n",
      "epoch 7, batch 67, loss = 0.015662282705307007\n",
      "epoch 7, batch 68, loss = 0.009924065321683884\n",
      "epoch 7, batch 69, loss = 0.00580584118142724\n",
      "epoch 7, batch 70, loss = 0.012629931792616844\n",
      "epoch 7, batch 71, loss = 0.011556928046047688\n",
      "epoch 7, batch 72, loss = 0.03101721964776516\n",
      "epoch 7, batch 73, loss = 0.011807712726294994\n",
      "epoch 7, batch 74, loss = 0.025421833619475365\n",
      "epoch 7, batch 75, loss = 0.033926814794540405\n",
      "epoch 7, batch 76, loss = 0.023493068292737007\n",
      "epoch 7, batch 77, loss = 0.016262372955679893\n",
      "epoch 7, batch 78, loss = 0.01998954266309738\n",
      "epoch 7, batch 79, loss = 0.045329559594392776\n",
      "epoch 7, batch 80, loss = 0.012485615909099579\n",
      "epoch 7, batch 81, loss = 0.0035132691264152527\n",
      "epoch 7, batch 82, loss = 0.014646022580564022\n",
      "epoch 7, batch 83, loss = 0.012209927663207054\n",
      "epoch 7, batch 84, loss = 0.05307507887482643\n",
      "epoch 7, batch 85, loss = 0.007092539221048355\n",
      "epoch 7, batch 86, loss = 0.015980232506990433\n",
      "epoch 7, batch 87, loss = 0.02479921095073223\n",
      "epoch 7, batch 88, loss = 0.014458706602454185\n",
      "epoch 7, batch 89, loss = 0.008983796462416649\n",
      "epoch 7, batch 90, loss = 0.013633210211992264\n",
      "epoch 7, batch 91, loss = 0.043190985918045044\n",
      "epoch 7, batch 92, loss = 0.060906168073415756\n",
      "epoch 7, batch 93, loss = 0.007798418402671814\n",
      "epoch 7, batch 94, loss = 0.07192966341972351\n",
      "epoch 7, batch 95, loss = 0.008892377838492393\n",
      "epoch 7, batch 96, loss = 0.01956317014992237\n",
      "epoch 7, batch 97, loss = 0.009820915758609772\n",
      "epoch 7, batch 98, loss = 0.000817715423181653\n",
      "epoch 7, batch 99, loss = 0.01702892780303955\n",
      "epoch 7, batch 100, loss = 0.014777356758713722\n",
      "epoch 7, batch 101, loss = 0.05507311224937439\n",
      "epoch 7, batch 102, loss = 0.004180066753178835\n",
      "epoch 7, batch 103, loss = 0.008313029073178768\n",
      "epoch 7, batch 104, loss = 0.010255349799990654\n",
      "epoch 7, batch 105, loss = 0.011670701205730438\n",
      "epoch 7, batch 106, loss = 0.014683102257549763\n",
      "epoch 7, batch 107, loss = 0.0020649635698646307\n",
      "epoch 7, batch 108, loss = 0.05775522068142891\n",
      "epoch 7, batch 109, loss = 0.005630165804177523\n",
      "epoch 7, batch 110, loss = 0.015010688453912735\n",
      "epoch 7, batch 111, loss = 0.0025877230800688267\n",
      "epoch 7, batch 112, loss = 0.012291821651160717\n",
      "epoch 7, batch 113, loss = 0.030928106978535652\n",
      "epoch 7, batch 114, loss = 0.004326615482568741\n",
      "epoch 7, batch 115, loss = 0.013019910082221031\n",
      "epoch 7, batch 116, loss = 0.014778795652091503\n",
      "epoch 7, batch 117, loss = 0.0019022506894543767\n",
      "epoch 7, batch 118, loss = 0.0046848817728459835\n",
      "epoch 7, batch 119, loss = 0.004723230842500925\n",
      "epoch 7, batch 120, loss = 0.008498822338879108\n",
      "epoch 7, batch 121, loss = 0.03984032943844795\n",
      "epoch 7, batch 122, loss = 0.011515054851770401\n",
      "epoch 7, batch 123, loss = 0.005082873627543449\n",
      "epoch 7, batch 124, loss = 0.004575430415570736\n",
      "epoch 7, batch 125, loss = 0.015238209627568722\n",
      "epoch 7, batch 126, loss = 0.009950312785804272\n",
      "epoch 7, batch 127, loss = 0.004241807386279106\n",
      "epoch 7, batch 128, loss = 0.019588300958275795\n",
      "epoch 7, batch 129, loss = 0.023903967812657356\n",
      "epoch 7, batch 130, loss = 0.018245810642838478\n",
      "epoch 7, batch 131, loss = 0.01053656730800867\n",
      "epoch 7, batch 132, loss = 0.0017172477673739195\n",
      "epoch 7, batch 133, loss = 0.024364329874515533\n",
      "epoch 7, batch 134, loss = 0.0026728513184934855\n",
      "epoch 7, batch 135, loss = 0.004205057397484779\n",
      "epoch 7, batch 136, loss = 0.0039782277308404446\n",
      "epoch 7, batch 137, loss = 0.0028927528765052557\n",
      "epoch 7, batch 138, loss = 0.0029095865320414305\n",
      "epoch 7, batch 139, loss = 0.016893861815333366\n",
      "epoch 7, batch 140, loss = 0.0043209209106862545\n",
      "epoch 7, batch 141, loss = 0.010749181732535362\n",
      "epoch 7, batch 142, loss = 0.009324710816144943\n",
      "epoch 7, batch 143, loss = 0.0034925201907753944\n",
      "epoch 7, batch 144, loss = 0.017475223168730736\n",
      "epoch 7, batch 145, loss = 0.012978869490325451\n",
      "epoch 7, batch 146, loss = 0.10094466805458069\n",
      "epoch 7, batch 147, loss = 0.011449432000517845\n",
      "epoch 7, batch 148, loss = 0.012257546186447144\n",
      "epoch 7, batch 149, loss = 0.001564821694046259\n",
      "epoch 7, batch 150, loss = 0.0038202712312340736\n",
      "epoch 7, batch 151, loss = 0.04285215213894844\n",
      "epoch 7, batch 152, loss = 0.003688942175358534\n",
      "epoch 7, batch 153, loss = 0.003764876164495945\n",
      "epoch 7, batch 154, loss = 0.008732059970498085\n",
      "epoch 7, batch 155, loss = 0.0022010058164596558\n",
      "epoch 7, batch 156, loss = 0.001526026288047433\n",
      "epoch 7, batch 157, loss = 0.026069335639476776\n",
      "epoch 7, batch 158, loss = 0.018130779266357422\n",
      "epoch 7, batch 159, loss = 0.027643311768770218\n",
      "epoch 7, batch 160, loss = 0.012636899016797543\n",
      "epoch 7, batch 161, loss = 0.006636868230998516\n",
      "epoch 7, batch 162, loss = 0.006884204223752022\n",
      "epoch 7, batch 163, loss = 0.026450490579009056\n",
      "epoch 7, batch 164, loss = 0.027970638126134872\n",
      "epoch 7, batch 165, loss = 0.007374567445367575\n",
      "epoch 7, batch 166, loss = 0.020353160798549652\n",
      "epoch 7, batch 167, loss = 0.010650279931724072\n",
      "epoch 7, batch 168, loss = 0.0017538825049996376\n",
      "epoch 7, batch 169, loss = 0.0023736923467367887\n",
      "epoch 7, batch 170, loss = 0.03453904017806053\n",
      "epoch 7, batch 171, loss = 0.010203398764133453\n",
      "epoch 7, batch 172, loss = 0.012561189942061901\n",
      "epoch 7, batch 173, loss = 0.013827375136315823\n",
      "epoch 7, batch 174, loss = 0.00576787581667304\n",
      "epoch 7, batch 175, loss = 0.0034445873461663723\n",
      "epoch 7, batch 176, loss = 0.031675562262535095\n",
      "epoch 7, batch 177, loss = 0.004316599573940039\n",
      "epoch 7, batch 178, loss = 0.0028440835885703564\n",
      "epoch 7, batch 179, loss = 0.006900560110807419\n",
      "epoch 7, batch 180, loss = 0.006968640256673098\n",
      "epoch 7, batch 181, loss = 0.0012751721078529954\n",
      "epoch 7, batch 182, loss = 0.004059336613863707\n",
      "epoch 7, batch 183, loss = 0.011995775625109673\n",
      "epoch 7, batch 184, loss = 0.008638749830424786\n",
      "epoch 7, batch 185, loss = 0.027467573061585426\n",
      "epoch 7, batch 186, loss = 0.0010487984400242567\n",
      "epoch 7, batch 187, loss = 0.04682714119553566\n",
      "epoch 7, batch 188, loss = 0.004065727349370718\n",
      "epoch 7, batch 189, loss = 0.016188181936740875\n",
      "epoch 7, batch 190, loss = 0.002784223295748234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, batch 191, loss = 0.00551975704729557\n",
      "epoch 7, batch 192, loss = 0.009967400692403316\n",
      "epoch 7, batch 193, loss = 0.03917261213064194\n",
      "epoch 7, batch 194, loss = 0.010558944195508957\n",
      "epoch 7, batch 195, loss = 0.019697707146406174\n",
      "epoch 7, batch 196, loss = 0.01424079667776823\n",
      "epoch 7, batch 197, loss = 0.003653082065284252\n",
      "epoch 7, batch 198, loss = 0.02560737170279026\n",
      "epoch 7, batch 199, loss = 0.017873162403702736\n",
      "epoch 8, batch 1, loss = 0.00946273747831583\n",
      "epoch 8, batch 2, loss = 0.0055802310816943645\n",
      "epoch 8, batch 3, loss = 0.0071947770193219185\n",
      "epoch 8, batch 4, loss = 0.002527782227844\n",
      "epoch 8, batch 5, loss = 0.008624290116131306\n",
      "epoch 8, batch 6, loss = 0.003703777212649584\n",
      "epoch 8, batch 7, loss = 0.04587503522634506\n",
      "epoch 8, batch 8, loss = 0.016120806336402893\n",
      "epoch 8, batch 9, loss = 0.021120207384228706\n",
      "epoch 8, batch 10, loss = 0.004247056320309639\n",
      "epoch 8, batch 11, loss = 0.0011723182396963239\n",
      "epoch 8, batch 12, loss = 0.012121684849262238\n",
      "epoch 8, batch 13, loss = 0.0007964085671119392\n",
      "epoch 8, batch 14, loss = 0.004961587954312563\n",
      "epoch 8, batch 15, loss = 0.019618147984147072\n",
      "epoch 8, batch 16, loss = 0.04583123326301575\n",
      "epoch 8, batch 17, loss = 0.001415313920006156\n",
      "epoch 8, batch 18, loss = 0.00925436057150364\n",
      "epoch 8, batch 19, loss = 0.003767535788938403\n",
      "epoch 8, batch 20, loss = 0.005375072360038757\n",
      "epoch 8, batch 21, loss = 0.0035830005072057247\n",
      "epoch 8, batch 22, loss = 0.022271931171417236\n",
      "epoch 8, batch 23, loss = 0.00270292186178267\n",
      "epoch 8, batch 24, loss = 0.005833957344293594\n",
      "epoch 8, batch 25, loss = 0.0011722627095878124\n",
      "epoch 8, batch 26, loss = 0.01627417653799057\n",
      "epoch 8, batch 27, loss = 0.019454140216112137\n",
      "epoch 8, batch 28, loss = 0.015883516520261765\n",
      "epoch 8, batch 29, loss = 0.00609904108569026\n",
      "epoch 8, batch 30, loss = 0.008829193189740181\n",
      "epoch 8, batch 31, loss = 0.004959882237017155\n",
      "epoch 8, batch 32, loss = 0.0012680583167821169\n",
      "epoch 8, batch 33, loss = 0.0010952720185741782\n",
      "epoch 8, batch 34, loss = 0.008752749301493168\n",
      "epoch 8, batch 35, loss = 0.0013621674152091146\n",
      "epoch 8, batch 36, loss = 0.00414319010451436\n",
      "epoch 8, batch 37, loss = 0.004014867823570967\n",
      "epoch 8, batch 38, loss = 0.021622365340590477\n",
      "epoch 8, batch 39, loss = 0.011623868718743324\n",
      "epoch 8, batch 40, loss = 0.016880586743354797\n",
      "epoch 8, batch 41, loss = 0.0010020032059401274\n",
      "epoch 8, batch 42, loss = 0.0016789104556664824\n",
      "epoch 8, batch 43, loss = 0.007274629548192024\n",
      "epoch 8, batch 44, loss = 0.02623971737921238\n",
      "epoch 8, batch 45, loss = 0.006605298724025488\n",
      "epoch 8, batch 46, loss = 0.0064980941824615\n",
      "epoch 8, batch 47, loss = 0.004953353200107813\n",
      "epoch 8, batch 48, loss = 0.01740383543074131\n",
      "epoch 8, batch 49, loss = 0.013128685764968395\n",
      "epoch 8, batch 50, loss = 0.002199241193011403\n",
      "epoch 8, batch 51, loss = 0.005582497920840979\n",
      "epoch 8, batch 52, loss = 0.022133493795990944\n",
      "epoch 8, batch 53, loss = 0.011040592566132545\n",
      "epoch 8, batch 54, loss = 0.025123411789536476\n",
      "epoch 8, batch 55, loss = 0.0019648843444883823\n",
      "epoch 8, batch 56, loss = 0.0018977380823343992\n",
      "epoch 8, batch 57, loss = 0.015044916421175003\n",
      "epoch 8, batch 58, loss = 0.011153523810207844\n",
      "epoch 8, batch 59, loss = 0.006243591196835041\n",
      "epoch 8, batch 60, loss = 0.0045075323432683945\n",
      "epoch 8, batch 61, loss = 0.0009708255529403687\n",
      "epoch 8, batch 62, loss = 0.005203378852456808\n",
      "epoch 8, batch 63, loss = 0.0034838251303881407\n",
      "epoch 8, batch 64, loss = 0.009739329107105732\n",
      "epoch 8, batch 65, loss = 0.0021972942631691694\n",
      "epoch 8, batch 66, loss = 0.0035627412144094706\n",
      "epoch 8, batch 67, loss = 0.004579817410558462\n",
      "epoch 8, batch 68, loss = 0.027274684980511665\n",
      "epoch 8, batch 69, loss = 0.004996108822524548\n",
      "epoch 8, batch 70, loss = 0.01049474161118269\n",
      "epoch 8, batch 71, loss = 0.0035255602560937405\n",
      "epoch 8, batch 72, loss = 0.0070213996805250645\n",
      "epoch 8, batch 73, loss = 0.005804122891277075\n",
      "epoch 8, batch 74, loss = 0.014486143365502357\n",
      "epoch 8, batch 75, loss = 0.01722361333668232\n",
      "epoch 8, batch 76, loss = 0.0077096461318433285\n",
      "epoch 8, batch 77, loss = 0.0049506137147545815\n",
      "epoch 8, batch 78, loss = 0.013377835974097252\n",
      "epoch 8, batch 79, loss = 0.004211336839944124\n",
      "epoch 8, batch 80, loss = 0.00782514363527298\n",
      "epoch 8, batch 81, loss = 0.012735367752611637\n",
      "epoch 8, batch 82, loss = 0.008585128001868725\n",
      "epoch 8, batch 83, loss = 0.01147452276200056\n",
      "epoch 8, batch 84, loss = 0.0035843325313180685\n",
      "epoch 8, batch 85, loss = 0.008919987827539444\n",
      "epoch 8, batch 86, loss = 0.0069161029532551765\n",
      "epoch 8, batch 87, loss = 0.012598945759236813\n",
      "epoch 8, batch 88, loss = 0.0019059113692492247\n",
      "epoch 8, batch 89, loss = 0.006274152547121048\n",
      "epoch 8, batch 90, loss = 0.028317905962467194\n",
      "epoch 8, batch 91, loss = 0.018568620085716248\n",
      "epoch 8, batch 92, loss = 0.0008591979276388884\n",
      "epoch 8, batch 93, loss = 0.01830078288912773\n",
      "epoch 8, batch 94, loss = 0.005958092864602804\n",
      "epoch 8, batch 95, loss = 0.010343069210648537\n",
      "epoch 8, batch 96, loss = 0.011048109270632267\n",
      "epoch 8, batch 97, loss = 0.009250184521079063\n",
      "epoch 8, batch 98, loss = 0.019661974161863327\n",
      "epoch 8, batch 99, loss = 0.001253490336239338\n",
      "epoch 8, batch 100, loss = 0.007305092643946409\n",
      "epoch 8, batch 101, loss = 0.000818218570202589\n",
      "epoch 8, batch 102, loss = 0.01963849365711212\n",
      "epoch 8, batch 103, loss = 0.007735017687082291\n",
      "epoch 8, batch 104, loss = 0.011998637579381466\n",
      "epoch 8, batch 105, loss = 0.009357855655252934\n",
      "epoch 8, batch 106, loss = 0.009067476727068424\n",
      "epoch 8, batch 107, loss = 0.005535998847335577\n",
      "epoch 8, batch 108, loss = 0.00503932312130928\n",
      "epoch 8, batch 109, loss = 0.0023545967414975166\n",
      "epoch 8, batch 110, loss = 0.0045469412580132484\n",
      "epoch 8, batch 111, loss = 0.010766253806650639\n",
      "epoch 8, batch 112, loss = 0.06171926483511925\n",
      "epoch 8, batch 113, loss = 0.007410096935927868\n",
      "epoch 8, batch 114, loss = 0.005912857595831156\n",
      "epoch 8, batch 115, loss = 0.0016456524608656764\n",
      "epoch 8, batch 116, loss = 0.0017941485857591033\n",
      "epoch 8, batch 117, loss = 0.01268067117780447\n",
      "epoch 8, batch 118, loss = 0.0023406853433698416\n",
      "epoch 8, batch 119, loss = 0.0012529677478596568\n",
      "epoch 8, batch 120, loss = 0.00429990328848362\n",
      "epoch 8, batch 121, loss = 0.016029205173254013\n",
      "epoch 8, batch 122, loss = 0.0013437530724331737\n",
      "epoch 8, batch 123, loss = 0.006291279569268227\n",
      "epoch 8, batch 124, loss = 0.013937033712863922\n",
      "epoch 8, batch 125, loss = 0.06300150603055954\n",
      "epoch 8, batch 126, loss = 0.01600496843457222\n",
      "epoch 8, batch 127, loss = 0.005764912813901901\n",
      "epoch 8, batch 128, loss = 0.004991262219846249\n",
      "epoch 8, batch 129, loss = 0.002211285289376974\n",
      "epoch 8, batch 130, loss = 0.010180097073316574\n",
      "epoch 8, batch 131, loss = 0.006067685317248106\n",
      "epoch 8, batch 132, loss = 0.03932882845401764\n",
      "epoch 8, batch 133, loss = 0.03805242106318474\n",
      "epoch 8, batch 134, loss = 0.0024994141422212124\n",
      "epoch 8, batch 135, loss = 0.007944468408823013\n",
      "epoch 8, batch 136, loss = 0.02295793779194355\n",
      "epoch 8, batch 137, loss = 0.007040790747851133\n",
      "epoch 8, batch 138, loss = 0.009315866976976395\n",
      "epoch 8, batch 139, loss = 0.03580162674188614\n",
      "epoch 8, batch 140, loss = 0.009628456085920334\n",
      "epoch 8, batch 141, loss = 0.008019034750759602\n",
      "epoch 8, batch 142, loss = 0.004941256251186132\n",
      "epoch 8, batch 143, loss = 0.010032253339886665\n",
      "epoch 8, batch 144, loss = 0.0015371250919997692\n",
      "epoch 8, batch 145, loss = 0.031907323747873306\n",
      "epoch 8, batch 146, loss = 0.021492796018719673\n",
      "epoch 8, batch 147, loss = 0.01660914346575737\n",
      "epoch 8, batch 148, loss = 0.010369127616286278\n",
      "epoch 8, batch 149, loss = 0.0213624257594347\n",
      "epoch 8, batch 150, loss = 0.009433174505829811\n",
      "epoch 8, batch 151, loss = 0.05424022302031517\n",
      "epoch 8, batch 152, loss = 0.02215094491839409\n",
      "epoch 8, batch 153, loss = 0.0012600499903783202\n",
      "epoch 8, batch 154, loss = 0.07423578947782516\n",
      "epoch 8, batch 155, loss = 0.005112484097480774\n",
      "epoch 8, batch 156, loss = 0.0038330843672156334\n",
      "epoch 8, batch 157, loss = 0.005456309299916029\n",
      "epoch 8, batch 158, loss = 0.012472710572183132\n",
      "epoch 8, batch 159, loss = 0.0031748125329613686\n",
      "epoch 8, batch 160, loss = 0.037406012415885925\n",
      "epoch 8, batch 161, loss = 0.0027869439218193293\n",
      "epoch 8, batch 162, loss = 0.0031657994259148836\n",
      "epoch 8, batch 163, loss = 0.0017042052932083607\n",
      "epoch 8, batch 164, loss = 0.047940388321876526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, batch 165, loss = 0.01542289461940527\n",
      "epoch 8, batch 166, loss = 0.004074546508491039\n",
      "epoch 8, batch 167, loss = 0.014107500202953815\n",
      "epoch 8, batch 168, loss = 0.00880275759845972\n",
      "epoch 8, batch 169, loss = 0.00836636871099472\n",
      "epoch 8, batch 170, loss = 0.031448207795619965\n",
      "epoch 8, batch 171, loss = 0.0031369617208838463\n",
      "epoch 8, batch 172, loss = 0.0017653168179094791\n",
      "epoch 8, batch 173, loss = 0.025235213339328766\n",
      "epoch 8, batch 174, loss = 0.00469308253377676\n",
      "epoch 8, batch 175, loss = 0.016915028914809227\n",
      "epoch 8, batch 176, loss = 0.015501029789447784\n",
      "epoch 8, batch 177, loss = 0.0034339982084929943\n",
      "epoch 8, batch 178, loss = 0.0026272835675626993\n",
      "epoch 8, batch 179, loss = 0.0012284711701795459\n",
      "epoch 8, batch 180, loss = 0.003773720934987068\n",
      "epoch 8, batch 181, loss = 0.0012126616202294827\n",
      "epoch 8, batch 182, loss = 0.014913086779415607\n",
      "epoch 8, batch 183, loss = 0.01408237311989069\n",
      "epoch 8, batch 184, loss = 0.009899918921291828\n",
      "epoch 8, batch 185, loss = 0.01129833236336708\n",
      "epoch 8, batch 186, loss = 0.06928539276123047\n",
      "epoch 8, batch 187, loss = 0.002139841904863715\n",
      "epoch 8, batch 188, loss = 0.009301075711846352\n",
      "epoch 8, batch 189, loss = 0.0012850031489506364\n",
      "epoch 8, batch 190, loss = 0.01105834636837244\n",
      "epoch 8, batch 191, loss = 0.004227328579872847\n",
      "epoch 8, batch 192, loss = 0.024519629776477814\n",
      "epoch 8, batch 193, loss = 0.005632725544273853\n",
      "epoch 8, batch 194, loss = 0.004877160768955946\n",
      "epoch 8, batch 195, loss = 0.006180033553391695\n",
      "epoch 8, batch 196, loss = 0.003362500574439764\n",
      "epoch 8, batch 197, loss = 0.009897042997181416\n",
      "epoch 8, batch 198, loss = 0.0059379399754107\n",
      "epoch 8, batch 199, loss = 0.02052418887615204\n",
      "epoch 9, batch 1, loss = 0.0005784594686701894\n",
      "epoch 9, batch 2, loss = 0.002100752666592598\n",
      "epoch 9, batch 3, loss = 0.0073557705618441105\n",
      "epoch 9, batch 4, loss = 0.016323409974575043\n",
      "epoch 9, batch 5, loss = 0.02600114792585373\n",
      "epoch 9, batch 6, loss = 0.0038811350241303444\n",
      "epoch 9, batch 7, loss = 0.0033326626289635897\n",
      "epoch 9, batch 8, loss = 0.0017805055249482393\n",
      "epoch 9, batch 9, loss = 0.0011137040564790368\n",
      "epoch 9, batch 10, loss = 0.011108324863016605\n",
      "epoch 9, batch 11, loss = 0.002064961241558194\n",
      "epoch 9, batch 12, loss = 0.003253028029575944\n",
      "epoch 9, batch 13, loss = 0.001959143904969096\n",
      "epoch 9, batch 14, loss = 0.005293985828757286\n",
      "epoch 9, batch 15, loss = 0.005176194943487644\n",
      "epoch 9, batch 16, loss = 0.006099258549511433\n",
      "epoch 9, batch 17, loss = 0.008312090300023556\n",
      "epoch 9, batch 18, loss = 0.000593458884395659\n",
      "epoch 9, batch 19, loss = 0.0033636607695370913\n",
      "epoch 9, batch 20, loss = 0.005543647333979607\n",
      "epoch 9, batch 21, loss = 0.005729621276259422\n",
      "epoch 9, batch 22, loss = 0.0009077323484234512\n",
      "epoch 9, batch 23, loss = 0.01325921155512333\n",
      "epoch 9, batch 24, loss = 0.0015358827076852322\n",
      "epoch 9, batch 25, loss = 0.00949777290225029\n",
      "epoch 9, batch 26, loss = 0.003856788855046034\n",
      "epoch 9, batch 27, loss = 0.006601123139262199\n",
      "epoch 9, batch 28, loss = 0.004818500950932503\n",
      "epoch 9, batch 29, loss = 0.0035431520082056522\n",
      "epoch 9, batch 30, loss = 0.00832496676594019\n",
      "epoch 9, batch 31, loss = 0.005804532207548618\n",
      "epoch 9, batch 32, loss = 0.004510119091719389\n",
      "epoch 9, batch 33, loss = 0.0002808714343700558\n",
      "epoch 9, batch 34, loss = 0.0017908589215949178\n",
      "epoch 9, batch 35, loss = 0.0023126788437366486\n",
      "epoch 9, batch 36, loss = 0.002291698707267642\n",
      "epoch 9, batch 37, loss = 0.002613913733512163\n",
      "epoch 9, batch 38, loss = 0.02420957386493683\n",
      "epoch 9, batch 39, loss = 0.0038290051743388176\n",
      "epoch 9, batch 40, loss = 0.0029690098017454147\n",
      "epoch 9, batch 41, loss = 0.0050806086510419846\n",
      "epoch 9, batch 42, loss = 0.004573088139295578\n",
      "epoch 9, batch 43, loss = 0.000482177798403427\n",
      "epoch 9, batch 44, loss = 0.006719483528286219\n",
      "epoch 9, batch 45, loss = 0.025783341377973557\n",
      "epoch 9, batch 46, loss = 0.0007318945717997849\n",
      "epoch 9, batch 47, loss = 0.02321820706129074\n",
      "epoch 9, batch 48, loss = 0.003843184793367982\n",
      "epoch 9, batch 49, loss = 0.012542967684566975\n",
      "epoch 9, batch 50, loss = 0.015923088416457176\n",
      "epoch 9, batch 51, loss = 0.002685596700757742\n",
      "epoch 9, batch 52, loss = 0.0008317199535667896\n",
      "epoch 9, batch 53, loss = 0.013348703272640705\n",
      "epoch 9, batch 54, loss = 0.0011937590315937996\n",
      "epoch 9, batch 55, loss = 0.00026208299095742404\n",
      "epoch 9, batch 56, loss = 0.006144234910607338\n",
      "epoch 9, batch 57, loss = 0.00449443468824029\n",
      "epoch 9, batch 58, loss = 0.00282043544575572\n",
      "epoch 9, batch 59, loss = 0.00296270614489913\n",
      "epoch 9, batch 60, loss = 0.011512283235788345\n",
      "epoch 9, batch 61, loss = 0.007570063695311546\n",
      "epoch 9, batch 62, loss = 0.0023521732073277235\n",
      "epoch 9, batch 63, loss = 0.0034563574008643627\n",
      "epoch 9, batch 64, loss = 0.023036515340209007\n",
      "epoch 9, batch 65, loss = 0.0020547627937048674\n",
      "epoch 9, batch 66, loss = 0.015976926311850548\n",
      "epoch 9, batch 67, loss = 0.004089269321411848\n",
      "epoch 9, batch 68, loss = 0.0023101388942450285\n",
      "epoch 9, batch 69, loss = 0.003616147907450795\n",
      "epoch 9, batch 70, loss = 0.0015453584492206573\n",
      "epoch 9, batch 71, loss = 0.01098846085369587\n",
      "epoch 9, batch 72, loss = 0.0011031756876036525\n",
      "epoch 9, batch 73, loss = 0.016892120242118835\n",
      "epoch 9, batch 74, loss = 0.00035248848143965006\n",
      "epoch 9, batch 75, loss = 0.0009902874007821083\n",
      "epoch 9, batch 76, loss = 0.007663697004318237\n",
      "epoch 9, batch 77, loss = 0.005067659541964531\n",
      "epoch 9, batch 78, loss = 0.0014774900628253818\n",
      "epoch 9, batch 79, loss = 0.0020960518158972263\n",
      "epoch 9, batch 80, loss = 0.004620173014700413\n",
      "epoch 9, batch 81, loss = 0.024752290919423103\n",
      "epoch 9, batch 82, loss = 0.009611135348677635\n",
      "epoch 9, batch 83, loss = 0.012319456785917282\n",
      "epoch 9, batch 84, loss = 0.009100638329982758\n",
      "epoch 9, batch 85, loss = 0.01768336445093155\n",
      "epoch 9, batch 86, loss = 0.006381219252943993\n",
      "epoch 9, batch 87, loss = 0.006665138527750969\n",
      "epoch 9, batch 88, loss = 0.006772887893021107\n",
      "epoch 9, batch 89, loss = 0.0013401658507063985\n",
      "epoch 9, batch 90, loss = 0.004393904935568571\n",
      "epoch 9, batch 91, loss = 0.00234051700681448\n",
      "epoch 9, batch 92, loss = 0.02107349783182144\n",
      "epoch 9, batch 93, loss = 0.0013544471003115177\n",
      "epoch 9, batch 94, loss = 0.04738481342792511\n",
      "epoch 9, batch 95, loss = 0.022970622405409813\n",
      "epoch 9, batch 96, loss = 0.01660912297666073\n",
      "epoch 9, batch 97, loss = 0.0027202977798879147\n",
      "epoch 9, batch 98, loss = 0.04718654975295067\n",
      "epoch 9, batch 99, loss = 0.0063093495555222034\n",
      "epoch 9, batch 100, loss = 0.04998102784156799\n",
      "epoch 9, batch 101, loss = 0.005738610401749611\n",
      "epoch 9, batch 102, loss = 0.011683560907840729\n",
      "epoch 9, batch 103, loss = 0.004926803521811962\n",
      "epoch 9, batch 104, loss = 0.013474148698151112\n",
      "epoch 9, batch 105, loss = 0.004222340881824493\n",
      "epoch 9, batch 106, loss = 0.004170835018157959\n",
      "epoch 9, batch 107, loss = 0.02204097993671894\n",
      "epoch 9, batch 108, loss = 0.0021192855201661587\n",
      "epoch 9, batch 109, loss = 0.00875052809715271\n",
      "epoch 9, batch 110, loss = 0.01322249136865139\n",
      "epoch 9, batch 111, loss = 0.01300803106278181\n",
      "epoch 9, batch 112, loss = 0.005147494841367006\n",
      "epoch 9, batch 113, loss = 0.011713550426065922\n",
      "epoch 9, batch 114, loss = 0.0115513876080513\n",
      "epoch 9, batch 115, loss = 0.0019582307431846857\n",
      "epoch 9, batch 116, loss = 0.004923906642943621\n",
      "epoch 9, batch 117, loss = 0.006258692126721144\n",
      "epoch 9, batch 118, loss = 0.004725588485598564\n",
      "epoch 9, batch 119, loss = 0.021364519372582436\n",
      "epoch 9, batch 120, loss = 0.00584074854850769\n",
      "epoch 9, batch 121, loss = 0.0009414811502210796\n",
      "epoch 9, batch 122, loss = 0.003273538313806057\n",
      "epoch 9, batch 123, loss = 0.014957567676901817\n",
      "epoch 9, batch 124, loss = 0.021069860085844994\n",
      "epoch 9, batch 125, loss = 0.006507007405161858\n",
      "epoch 9, batch 126, loss = 0.0042540221475064754\n",
      "epoch 9, batch 127, loss = 0.0012230472639203072\n",
      "epoch 9, batch 128, loss = 0.007175685837864876\n",
      "epoch 9, batch 129, loss = 0.031772810965776443\n",
      "epoch 9, batch 130, loss = 0.042779263108968735\n",
      "epoch 9, batch 131, loss = 0.04767860099673271\n",
      "epoch 9, batch 132, loss = 0.007433807477355003\n",
      "epoch 9, batch 133, loss = 0.0012224146630614996\n",
      "epoch 9, batch 134, loss = 0.003000490367412567\n",
      "epoch 9, batch 135, loss = 0.010953612625598907\n",
      "epoch 9, batch 136, loss = 0.007366834674030542\n",
      "epoch 9, batch 137, loss = 0.006333594210445881\n",
      "epoch 9, batch 138, loss = 0.0031896959990262985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, batch 139, loss = 0.00876080896705389\n",
      "epoch 9, batch 140, loss = 0.03380126133561134\n",
      "epoch 9, batch 141, loss = 0.005965843331068754\n",
      "epoch 9, batch 142, loss = 0.03684099391102791\n",
      "epoch 9, batch 143, loss = 0.00867688748985529\n",
      "epoch 9, batch 144, loss = 0.020176200196146965\n",
      "epoch 9, batch 145, loss = 0.007272625342011452\n",
      "epoch 9, batch 146, loss = 0.021984029561281204\n",
      "epoch 9, batch 147, loss = 0.002843281254172325\n",
      "epoch 9, batch 148, loss = 0.0055556707084178925\n",
      "epoch 9, batch 149, loss = 0.0050965058617293835\n",
      "epoch 9, batch 150, loss = 0.0016503736842423677\n",
      "epoch 9, batch 151, loss = 0.006448377855122089\n",
      "epoch 9, batch 152, loss = 0.0063003432005643845\n",
      "epoch 9, batch 153, loss = 0.011766002513468266\n",
      "epoch 9, batch 154, loss = 0.02219957299530506\n",
      "epoch 9, batch 155, loss = 0.0025639499071985483\n",
      "epoch 9, batch 156, loss = 0.009619399905204773\n",
      "epoch 9, batch 157, loss = 0.007016408257186413\n",
      "epoch 9, batch 158, loss = 0.001873694476671517\n",
      "epoch 9, batch 159, loss = 0.0015640768688172102\n",
      "epoch 9, batch 160, loss = 0.002946163062006235\n",
      "epoch 9, batch 161, loss = 0.010069935582578182\n",
      "epoch 9, batch 162, loss = 0.0115329185500741\n",
      "epoch 9, batch 163, loss = 0.024438561871647835\n",
      "epoch 9, batch 164, loss = 0.007897789590060711\n",
      "epoch 9, batch 165, loss = 0.03998163715004921\n",
      "epoch 9, batch 166, loss = 0.0015091601526364684\n",
      "epoch 9, batch 167, loss = 0.01893078163266182\n",
      "epoch 9, batch 168, loss = 0.01775229722261429\n",
      "epoch 9, batch 169, loss = 0.0020259907469153404\n",
      "epoch 9, batch 170, loss = 0.005754159297794104\n",
      "epoch 9, batch 171, loss = 0.042823582887649536\n",
      "epoch 9, batch 172, loss = 0.0038366697262972593\n",
      "epoch 9, batch 173, loss = 0.005590474233031273\n",
      "epoch 9, batch 174, loss = 0.01830347813665867\n",
      "epoch 9, batch 175, loss = 0.007878548465669155\n",
      "epoch 9, batch 176, loss = 0.024645913392305374\n",
      "epoch 9, batch 177, loss = 0.004735500086098909\n",
      "epoch 9, batch 178, loss = 0.002165040001273155\n",
      "epoch 9, batch 179, loss = 0.018076356500387192\n",
      "epoch 9, batch 180, loss = 0.004272324498742819\n",
      "epoch 9, batch 181, loss = 0.002995668910443783\n",
      "epoch 9, batch 182, loss = 0.001356522087007761\n",
      "epoch 9, batch 183, loss = 0.003266775980591774\n",
      "epoch 9, batch 184, loss = 0.0014505167491734028\n",
      "epoch 9, batch 185, loss = 0.012252523563802242\n",
      "epoch 9, batch 186, loss = 0.01155287865549326\n",
      "epoch 9, batch 187, loss = 0.01546715758740902\n",
      "epoch 9, batch 188, loss = 0.0035835301969200373\n",
      "epoch 9, batch 189, loss = 0.016686270013451576\n",
      "epoch 9, batch 190, loss = 0.0028675857465714216\n",
      "epoch 9, batch 191, loss = 0.0029253046959638596\n",
      "epoch 9, batch 192, loss = 0.010896529071033001\n",
      "epoch 9, batch 193, loss = 0.006479440722614527\n",
      "epoch 9, batch 194, loss = 0.02268499881029129\n",
      "epoch 9, batch 195, loss = 0.0012353453785181046\n",
      "epoch 9, batch 196, loss = 0.0015848043840378523\n",
      "epoch 9, batch 197, loss = 0.008367118425667286\n",
      "epoch 9, batch 198, loss = 0.0024164877831935883\n",
      "epoch 9, batch 199, loss = 0.00325126014649868\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "EPOCH      = 10\n",
    "LR         = 0.001\n",
    "\n",
    "model  = MyModel()\n",
    "loader = DataLoader(mnist, batch_size=BATCH_SIZE, shuffle=True)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "for i in range(EPOCH):\n",
    "    j = 0\n",
    "    for x, y in loader:\n",
    "        pred = model(x)\n",
    "        loss = lossfn(torch.log(pred), y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        j += 1\n",
    "        if (j // BATCH_SIZE) % 10 == 0:\n",
    "            print(f\"epoch {i+1}, batch {j+1}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证模型的效果，我们对前10个观测（即之前生成的 `smallx` 和 `smally`）进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.999 0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.    0.    0.   ]]\n",
      "tensor([7, 4, 5, 4, 0, 7, 1, 0, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "ypred = model(smallx)\n",
    "print(np.round(ypred.detach().cpu().numpy(), 3))\n",
    "print(smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果模型搭建和训练都正常，那么每一行中概率最大的取值所在的位置应该正好对应真实的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们用模型对一些真实的手写数字图片进行预测。请你利用绘图软件（如 Windows 自带的绘图，或 Photoshop 等）准备10张正方形黑色底色的图片，每张用鼠标绘制一个数字（请使用较粗的笔划），从0到9，然后以0.png，1.png等文件名存储下来，放到当前目录一个名为 digits 的文件夹中。以下是几个例子：\n",
    "![](digits/sample0.png) ![](digits/sample5.png) ![](digits/sample8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来利用 Pillow 软件包读取图片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=261x261 at 0x1F3A1C9C580>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(\"digits/sample0.png\")\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时如果直接将其转为 Numpy 数组会得到三个通道："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 261, 3)\n"
     ]
    }
   ],
   "source": [
    "im_arr = np.array(im)\n",
    "print(im_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们先强制转换为灰度图片（单通道），再缩放至模型的图片大小 28 x 28："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABQElEQVR4nGNgoAlgROaIiop/5vny8cO7vxiSksEuwiyf+SQZ7+yd+4KBgYGBgQUmJRcQp3D/z8ebWmLvNIzlK94h6WQKyNf7+HbT9s+fBf4w2iRKxx5Csi3s9rvjkfJQDs+6l1FIxspksM9acAOmkomP7Q+EwcDAwMDgafxmDlyOge33z1cISSY1hhV3EXboqL5+jSQpwPjtP0JSUODeY4QkI9N/ToQckx7Lva9IdnJ8eIiQ1Av6dP4vQvL3nf/JEjA5sSyFuweQfMlgdO2NL5QpNOfVFXsGFND5Zo0SAwMDAwN74YuH/qhyDDpnPnQwMTAwMCTcu5fPjCbJGHn3ZhQDA3P4rQ/d3AzogL317a32oP4nbxeLYsgxMMivfvL9+ZdX85SQzYOzRAJtxa6d3vgNi0YGBgYGRjYmHDKDBQAAHZJkUSWMm9UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1F39F5BCC70>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = im.convert(\"L\")\n",
    "im.thumbnail((28, 28))\n",
    "im_arr = np.array(im)\n",
    "print(im_arr.shape)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了传递给模型对象，还需要先将数值归一化到 [0,1] 区间，转换为 PyTorch 的 Tensor 类型，并增加一个批次和一个通道的维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "test0 = torch.tensor(im_arr / 255.0, dtype=torch.float32).view(1, 1, 28, 28)\n",
    "print(test0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后对图片标签进行预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.048 0.    0.    0.    0.    0.    0.    0.    0.006 0.945]]\n"
     ]
    }
   ],
   "source": [
    "pred0 = model(test0)\n",
    "print(np.round(pred0.detach().cpu().numpy(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测结果是否符合真实情形？请对你自己绘制出的10张图片进行类似的预测操作，并评价其效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample5.png\")\n",
    "im_arr = np.array(im)\n",
    "im = im.convert(\"L\")\n",
    "im.thumbnail((28, 28))\n",
    "im_arr = np.array(im)\n",
    "test5 = torch.tensor(im_arr / 255.0, dtype=torch.float32).view(1, 1, 28, 28)\n",
    "pred5 = model(test5)\n",
    "print(np.round(pred5.detach().cpu().numpy(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"digits/sample8.png\")\n",
    "im_arr = np.array(im)\n",
    "im = im.convert(\"L\")\n",
    "im.thumbnail((28, 28))\n",
    "im_arr = np.array(im)\n",
    "test8 = torch.tensor(im_arr / 255.0, dtype=torch.float32).view(1, 1, 28, 28)\n",
    "pred8 = model(test8)\n",
    "print(np.round(pred8.detach().cpu().numpy(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 目标：通过对英文名数据进行训练，构建一个 RNN 模型，实现英文名的自动生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 主要步骤：获取和整理数据，对字符串进行 one-hot 编码，创建模型结构，定义损失函数，编写训练循环，最后生成人名字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取和整理数据。数据文件已存为 `data/names.txt`，先将其读取为字符串列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668\n",
      "['Abbas', 'Abbey', 'Abbott', 'Abdi', 'Abel']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "lines = io.open(\"data/names.txt\").read().strip().split('\\n')\n",
    "print(len(lines))\n",
    "print(lines[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，共读取了3668个名字。为了简单起见，我们将所有的大写字母转换为小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbas', 'abbey', 'abbott', 'abdi', 'abel']\n"
     ]
    }
   ],
   "source": [
    "names = [s.lower() for s in lines]\n",
    "print(names[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要构建一个字符的字典。对于英文名来说很简单，即26个字母。我们可以通过下面的代码直接得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "char_dict = string.ascii_lowercase\n",
    "char_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 下面准备好 one-hot 编码所需的函数。编写函数 `char2index(char)`，将一个字母转换为其所在字典的位置。例如 `char2index(\"a\")` 要返回0，`char2index(\"z\")` 要返回25，等等。提示：使用字符串的 `.find()` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def char2index(char):\n",
    "    if char in char_dict:\n",
    "        return char_dict.find(char)\n",
    "    else:\n",
    "        return 26\n",
    "\n",
    "print(char2index(\"z\") == 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写 `char2tensor(char)` 函数，将一个字母转换为 one-hot 向量，即该向量中第 i 个元素为1，其余为0，其中 i 表示该字母在字典中的位置。\n",
    "\n",
    "**注意，该向量的长度应为27，因为我们要预留终止符，用 `[0.0, 0.0, ..., 1.0]` 表示**。\n",
    "\n",
    "`char2tensor(\"a\")` 应返回 `torch.tensor([1.0, 0.0, ...])`，`char2tensor(\"z\")` 应返回 `torch.tensor([0.0, ..., 1.0, 0.0])`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.zeros(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def char2tensor(char):\n",
    "    coding = torch.zeros(27)\n",
    "    coding[char2index(char)] = torch.tensor(1.0)\n",
    "    return coding\n",
    "\n",
    "print(char2tensor(\"a\"))\n",
    "print(char2tensor(\"z\"))\n",
    "print(char2tensor(\"z\").shape[0] == 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 构建模型。我们使用最简单的 RNN 结构，即隐藏单元是输入和上一期隐藏单元的线性变换加上 Tanh 激活函数，输出单元是隐藏单元的线性变换加上 Softmax 激活函数。输出的结果代表下一个字符的概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = torch.nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.h2o = torch.nn.Linear(hidden_size, 27)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = torch.nn.functional.softmax(self.h2o(hidden), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做一个简单的测试。请在下面的代码中加入适当的语句，使得每次运行的结果不变。根据其输出结果，请问当前模型预测字符a的下一个字符是什么？为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0304, 0.0364, 0.0491, 0.0442, 0.0371, 0.0395, 0.0339, 0.0299, 0.0321,\n",
      "         0.0324, 0.0487, 0.0523, 0.0299, 0.0342, 0.0407, 0.0314, 0.0277, 0.0337,\n",
      "         0.0446, 0.0299, 0.0310, 0.0293, 0.0383, 0.0450, 0.0426, 0.0398, 0.0361]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=27, hidden_size=10)\n",
    "input = char2tensor(\"a\")\n",
    "hidden = rnn.init_hidden()\n",
    "output, hidden = rnn(input.view(1, 27), hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 接下来我们定义好损失函数。与第1题中类似，预测值是一个概率分布，而真实的标签是0到26中的一个整数，代表真实的下一个字符在字典中的位置。假设当前处理的名字为\"abel\"，那么字符a的输出结果对应的标签是什么？请完成下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9882, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "target = char2index('b')\n",
    "\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "loss = lossfn(torch.log(output), torch.tensor([target]))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 明确单个字符的损失函数的计算方法后，请在下面计算出\"abel\"这个观测整体的损失函数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3341, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "name = 'abel'\n",
    "rnn = RNN(input_size=27, hidden_size=10)\n",
    "hidden = rnn.init_hidden()\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "loss = 0\n",
    "\n",
    "for i in range(len(name)):\n",
    "    input = char2tensor(name[i])\n",
    "    output, hidden = rnn(input.view(1,27), hidden)\n",
    "\n",
    "    if i == len(name) - 1:\n",
    "        target = 26\n",
    "    else:\n",
    "        target = char2index(name[i + 1])\n",
    "\n",
    "    loss = loss + lossfn(torch.log(output), torch.tensor([target]))\n",
    "\n",
    "loss = loss / len(name)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 将上述过程在数据上进行反复迭代，训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, obs 0, loss = 3.3991856575012207\n",
      "epoch 0, obs 1000, loss = 2.918344020843506\n",
      "epoch 0, obs 2000, loss = 2.6398532390594482\n",
      "epoch 0, obs 3000, loss = 3.1096529960632324\n",
      "epoch 1, obs 0, loss = 2.776276111602783\n",
      "epoch 1, obs 1000, loss = 2.5617995262145996\n",
      "epoch 1, obs 2000, loss = 2.415987014770508\n",
      "epoch 1, obs 3000, loss = 2.4527029991149902\n",
      "epoch 2, obs 0, loss = 2.5975751876831055\n",
      "epoch 2, obs 1000, loss = 2.585355520248413\n",
      "epoch 2, obs 2000, loss = 2.5821340084075928\n",
      "epoch 2, obs 3000, loss = 2.7673685550689697\n",
      "epoch 3, obs 0, loss = 2.584155559539795\n",
      "epoch 3, obs 1000, loss = 2.1915831565856934\n",
      "epoch 3, obs 2000, loss = 2.4389891624450684\n",
      "epoch 3, obs 3000, loss = 2.0150928497314453\n",
      "epoch 4, obs 0, loss = 2.4576306343078613\n",
      "epoch 4, obs 1000, loss = 2.9122250080108643\n",
      "epoch 4, obs 2000, loss = 2.5470449924468994\n",
      "epoch 4, obs 3000, loss = 2.1251466274261475\n",
      "epoch 5, obs 0, loss = 2.283302068710327\n",
      "epoch 5, obs 1000, loss = 2.564319133758545\n",
      "epoch 5, obs 2000, loss = 2.1136810779571533\n",
      "epoch 5, obs 3000, loss = 2.19895601272583\n",
      "epoch 6, obs 0, loss = 2.385223388671875\n",
      "epoch 6, obs 1000, loss = 2.7023868560791016\n",
      "epoch 6, obs 2000, loss = 2.240619659423828\n",
      "epoch 6, obs 3000, loss = 2.557904005050659\n",
      "epoch 7, obs 0, loss = 1.9776948690414429\n",
      "epoch 7, obs 1000, loss = 1.7167218923568726\n",
      "epoch 7, obs 2000, loss = 1.6471465826034546\n",
      "epoch 7, obs 3000, loss = 1.972745418548584\n",
      "epoch 8, obs 0, loss = 2.3119893074035645\n",
      "epoch 8, obs 1000, loss = 2.0465922355651855\n",
      "epoch 8, obs 2000, loss = 2.7650790214538574\n",
      "epoch 8, obs 3000, loss = 1.8902024030685425\n",
      "epoch 9, obs 0, loss = 2.1596932411193848\n",
      "epoch 9, obs 1000, loss = 2.3838236331939697\n",
      "epoch 9, obs 2000, loss = 2.1008870601654053\n",
      "epoch 9, obs 3000, loss = 3.150839328765869\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.random.manual_seed(123)\n",
    "\n",
    "n = len(names)\n",
    "n_hidden = 16\n",
    "n_input = 27\n",
    "nepoch = 10\n",
    "\n",
    "rnn = RNN(n_input, n_hidden)\n",
    "opt = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "train_ind = np.arange(n)\n",
    "losses = []\n",
    "\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "\n",
    "# Loop over epochs\n",
    "for k in range(nepoch):\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(train_ind)\n",
    "    # Loop over observations. Each observation is a name\n",
    "    for i in range(n):\n",
    "        name = names[train_ind[i]]\n",
    "        nchar = len(name)\n",
    "        # Loop over the characters in the name\n",
    "        # Each input character has a target, which is the index of the next character in the dictionary\n",
    "        # For the last character in the name, the target is the end-of-sequence symbol, which has index 26\n",
    "        loss = 0.0\n",
    "        hidden = rnn.init_hidden()\n",
    "        for j in range(nchar):\n",
    "            input = char2tensor(name[j])\n",
    "            output, hidden = rnn(input.view(1, n_input), hidden)\n",
    "\n",
    "            if j == nchar - 1:\n",
    "                target = 26\n",
    "            else:\n",
    "                target = char2index(name[j + 1])\n",
    "\n",
    "            loss = loss + lossfn(torch.log(output), torch.tensor([target]))\n",
    "    \n",
    "        loss = loss / nchar\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"epoch {k}, obs {i}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ee540edb80>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 编写一个函数 `random_first_letter()`，它随机返回字典中的一个字符，我们将利用它来随机生成第一个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n",
      "c\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "def random_first_letter():\n",
    "    idx = np.random.randint(0, 25)\n",
    "    return char_dict[idx]\n",
    "\n",
    "print(random_first_letter())\n",
    "print(random_first_letter())\n",
    "print(random_first_letter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请简要说明如下的代码的含义（可以在代码中加入注释），然后利用它随机生成10个名字。评价生成的结果，并简要说明可以如何改进模型的效果？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_name(max_len=20):\n",
    "    rnn.eval()\n",
    "    first_letter = random_first_letter()\n",
    "    char_ind = [char2index(first_letter)]\n",
    "    input = char2tensor(first_letter)\n",
    "    hidden = rnn.init_hidden()\n",
    "    for i in range(max_len - 1):\n",
    "        output, hidden = rnn(input.view(1, n_input), hidden)\n",
    "        ind = torch.argmax(output).item()\n",
    "        if ind == 26:\n",
    "            break\n",
    "        char_ind.append(ind)\n",
    "        input.zero_()\n",
    "        input[ind] = 1.0\n",
    "    return \"\".join([char_dict[i] for i in char_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hololololo\n",
      "alolololol\n",
      "sololololo\n",
      "rololololo\n",
      "idpppppppp\n",
      "bdolololol\n",
      "mololololo\n",
      "tdpppppppp\n",
      "jlolololol\n",
      "hololololo\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(random_name(max_len=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用卷积函数实现任意大整数的乘法。给定两个整数，如 183612 和 23333，用两个列表表达它们的序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [1, 8, 3, 6, 1, 2]\n",
    "n2 = [2, 3, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请编写一个函数 `integer_mult(n1, n2)`，返回 `n1 * n2` 对应的整数序列。注意不要直接调用乘法表达式（设想有两个非常大的整数，直接相乘可能会导致数值溢出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 8, 4, 2, 1, 8, 7, 9, 6]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def integer_mult(n1, n2):\n",
    "    poly_multi_result = np.convolve(n1, n2)\n",
    "    to_next = 0\n",
    "    params = []\n",
    "    for j in poly_multi_result[::-1]:\n",
    "        params.append((j + to_next) % 10)\n",
    "        to_next = (j + to_next) // 10\n",
    "    while to_next:\n",
    "        params.append(to_next % 10)\n",
    "        to_next = to_next // 10  \n",
    "    return params[::-1]\n",
    "\n",
    "res = integer_mult(n1, n2)\n",
    "print(res)\n",
    "print(res == [4, 2, 8, 4, 2, 1, 8, 7, 9, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 先实现多项式的乘法。例如，给定 $p(x)=1+2x+x^4$ 和 $q(x)=x+3x^2+5x^3$，计算 $r(x)=p(x)q(x)$。我们将 $p(x)$ 编码为 `p = [1, 2, 0, 0, 1]`，$q(x)$ 编码为 `q = [0, 1, 3, 5]`，请编写函数 `poly_mult(p, q)`，使得 `poly_mult(p, q) == [0, 1, 5, 11, 10, 1, 3, 5]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "p = [1, 2, 0, 0, 1]\n",
    "q = [0, 1, 3, 5]\n",
    "\n",
    "def poly_mult(p, q):\n",
    "    return np.convolve(p, q)\n",
    "\n",
    "print(poly_mult(p, q) == [0, 1, 5, 11, 10,  1, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对于任意的一个整数，将其看成是某个多项式在 $x=10$ 处的取值，如 $123 = p_1(10)$，$p_1(x)=3+2x+x^2$，$5310 = p_2(10)$，$p_2(x)=x+3x^2+5x^3$，注意需要适当将序列反序。因此，要计算 $123\\times 5310$，相当于计算 $r(10)$ 的值，但为了避免直接进行乘法运算（防止溢出），可以先计算 $r(x)$ 的表达式（等价于其系数向量），然后建立起 $r(x)$ 的系数与 $r(10)$ 之间的联系（见如下第3点）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如果一个多项式 $r(x)$ 所有的系数都是0到9之间的整数，那么 $r(x)$ 和 $r(10)$ 的关系非常直接，比如若 $r(x)=1+2x+5x^2+3x^3$，则 $r(10)=3521$。但如果有系数超过10，就需要考虑进位的影响，比如 $r(x)=1+11x+2x^2$，$r(10)=311$。此时可以从 $r(x)$ 的第一项开始逐项进位，构造一个新的多项式 $r'(x)=1+x+3x^2$，满足 $r'(10)=r(10)$，且 $r'(x)$ 所有的系数都不超过10。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 综合利用以上信息，完成本题的算法编写。并测试 23742389754298365 * 809723950 的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_mult(n1, n2):\n",
    "    poly_multi_result = np.convolve(n1, n2)\n",
    "    to_next = 0\n",
    "    params = []\n",
    "    for j in poly_multi_result[::-1]:\n",
    "        params.append((j + to_next) % 10)\n",
    "        to_next = (j + to_next) // 10\n",
    "    while to_next:\n",
    "        params.append(to_next % 10)\n",
    "        to_next = to_next // 10  \n",
    "    return params[::-1]\n",
    "\n",
    "def big_number_multi(params):\n",
    "    result = 0\n",
    "    for i, j in enumerate(params[::-1]):\n",
    "        result = result + (10**i)*j\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将两个数字表示成列表表达形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [2,3,7,4,2,3,8,9,7,5,4,2,9,8,3,6,5]\n",
    "n2 = [8,0,9,7,2,3,9,5,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算乘积多项式系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9, 2, 2, 4, 7, 8, 1, 6, 1, 4, 2, 9, 0, 0, 0, 1, 5, 8, 6, 3, 4, 1, 7, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "poly_params = integer_mult(n1, n2)\n",
    "print(poly_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将$x=10$带入多项式中进行计算，得到乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9224781614290002e+25"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_number_multi(poly_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
